[
  {
    "arxiv_id": "2512.04072v1",
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "abstract": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.",
    "authors": [
      "Zayne Sprague",
      "Jack Lu",
      "Manya Wadhwa",
      "Sedrick Keh",
      "Mengye Ren",
      "Greg Durrett"
    ],
    "published": "2025-12-03T18:54:53+00:00",
    "url": "https://arxiv.org/pdf/2512.04072v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.04069v1",
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "abstract": "Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.",
    "authors": [
      "Siyi Chen",
      "Mikaela Angelina Uy",
      "Chan Hee Song",
      "Faisal Ladhak",
      "Adithyavairavan Murali",
      "Qing Qu",
      "Stan Birchfield",
      "Valts Blukis",
      "Jonathan Tremblay"
    ],
    "published": "2025-12-03T18:50:04+00:00",
    "url": "https://arxiv.org/pdf/2512.04069v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.04065v1",
    "title": "Fare Comparison App of Uber, Ola and Rapido",
    "abstract": "In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.",
    "authors": [
      "Ashlesha Gopinath Sawant",
      "Sahil S. Jadhav",
      "Vidhan R. Jain",
      "Shriraj S. Jagtap",
      "Prachi Jadhav",
      "Soham Jadhav",
      "Ichha Raina"
    ],
    "published": "2025-12-03T18:48:33+00:00",
    "url": "https://arxiv.org/pdf/2512.04065v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.04048v1",
    "title": "Stable Signer: Hierarchical Sign Language Generative Model",
    "abstract": "Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.",
    "authors": [
      "Sen Fang",
      "Yalin Feng",
      "Hongbin Zhong",
      "Yanxin Zhang",
      "Dimitris N. Metaxas"
    ],
    "published": "2025-12-03T18:33:40+00:00",
    "url": "https://arxiv.org/pdf/2512.04048v1",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.04047v1",
    "title": "Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs",
    "abstract": "In democracies, major policy decisions typically require some form of majority or consensus, so elites must secure mass support to govern. Historically, elites could shape support only through limited instruments like schooling and mass media; advances in AI-driven persuasion sharply reduce the cost and increase the precision of shaping public opinion, making the distribution of preferences itself an object of deliberate design. We develop a dynamic model in which elites choose how much to reshape the distribution of policy preferences, subject to persuasion costs and a majority rule constraint. With a single elite, any optimal intervention tends to push society toward more polarized opinion profiles - a ``polarization pull'' - and improvements in persuasion technology accelerate this drift. When two opposed elites alternate in power, the same technology also creates incentives to park society in ``semi-lock'' regions where opinions are more cohesive and harder for a rival to overturn, so advances in persuasion can either heighten or dampen polarization depending on the environment. Taken together, cheaper persuasion technologies recast polarization as a strategic instrument of governance rather than a purely emergent social byproduct, with important implications for democratic stability as AI capabilities advance.",
    "authors": [
      "Nadav Kunievsky"
    ],
    "published": "2025-12-03T18:33:26+00:00",
    "url": "https://arxiv.org/pdf/2512.04047v1",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.04044v1",
    "title": "MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking",
    "abstract": "Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.",
    "authors": [
      "Yizhou Zhao",
      "Zhiwei Steven Wu",
      "Adam Block"
    ],
    "published": "2025-12-03T18:32:19+00:00",
    "url": "https://arxiv.org/pdf/2512.04044v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "arxiv_id": "2512.04040v1",
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "abstract": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
    "authors": [
      "Yicong Hong",
      "Yiqun Mei",
      "Chongjian Ge",
      "Yiran Xu",
      "Yang Zhou",
      "Sai Bi",
      "Yannick Hold-Geoffroy",
      "Mike Roberts",
      "Matthew Fisher",
      "Eli Shechtman",
      "Kalyan Sunkavalli",
      "Feng Liu",
      "Zhengqi Li",
      "Hao Tan"
    ],
    "published": "2025-12-03T18:29:20+00:00",
    "url": "https://arxiv.org/pdf/2512.04040v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.04039v1",
    "title": "Fast & Efficient Normalizing Flows and Applications of Image Generative Models",
    "abstract": "This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance.   The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.",
    "authors": [
      "Sandeep Nagar"
    ],
    "published": "2025-12-03T18:29:03+00:00",
    "url": "https://arxiv.org/pdf/2512.04039v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.04032v1",
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "abstract": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.",
    "authors": [
      "Andreas Koukounas",
      "Georgios Mastrapas",
      "Florian H\u00f6nicke",
      "Sedigheh Eslami",
      "Guillaume Roncari",
      "Scott Martens",
      "Han Xiao"
    ],
    "published": "2025-12-03T18:13:41+00:00",
    "url": "https://arxiv.org/pdf/2512.04032v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.04031v1",
    "title": "Large Language Models for Limited Noisy Data: A Gravitational Wave Identification Study",
    "abstract": "This work investigates whether large language models (LLMs) offer advantages over traditional neural networks for astronomical data processing, in regimes with non-Gaussian, non-stationary noise and limited labeled samples. Gravitational wave observations provide an suitable test case, using only 90 LIGO events, finetuned LLMs achieve 97.4\\% accuracy for identifying signals. Further experiments show that, in contrast to traditional networks that rely on large simulated datasets, additional simulated samples do not improve LLM performance, while scaling studies reveal predictable gains with increasing model size and dataset size. These results indicate that LLMs can extract discriminative structure directly from observational data and provide an efficient assessment for gravitational wave identification. The same strategy may extend to other astronomical domains with similar noise properties, such as radio or pulsar observations.",
    "authors": [
      "Yixuan Li",
      "Yuhao Lu",
      "Yang Liu",
      "Liang Li",
      "R. Ruffini",
      "Di Li",
      "Rong-Gen Cai",
      "Xiaoyan Zhu",
      "Wenbin Lin",
      "Yu Wang"
    ],
    "published": "2025-12-03T18:13:01+00:00",
    "url": "https://arxiv.org/pdf/2512.04031v1",
    "categories": [
      "astro-ph.IM",
      "astro-ph.HE",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.04025v1",
    "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
    "abstract": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA",
    "authors": [
      "Xiaolong Li",
      "Youping Gu",
      "Xi Lin",
      "Weijie Wang",
      "Bohan Zhuang"
    ],
    "published": "2025-12-03T18:02:11+00:00",
    "url": "https://arxiv.org/pdf/2512.04025v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.04021v1",
    "title": "C3G: Learning Compact 3D Representations with 2K Gaussians",
    "abstract": "Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.",
    "authors": [
      "Honggyu An",
      "Jaewoo Jung",
      "Mungyeom Kim",
      "Sunghwan Hong",
      "Chaehyun Kim",
      "Kazumi Fukuda",
      "Minkyeong Jeon",
      "Jisang Han",
      "Takuya Narihira",
      "Hyuna Ko",
      "Junsu Kim",
      "Yuki Mitsufuji",
      "Seungryong Kim"
    ],
    "published": "2025-12-03T17:59:05+00:00",
    "url": "https://arxiv.org/pdf/2512.04021v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.04019v1",
    "title": "Ultra-lightweight Neural Video Representation Compression",
    "abstract": "Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.",
    "authors": [
      "Ho Man Kwan",
      "Tianhao Peng",
      "Ge Gao",
      "Fan Zhang",
      "Mike Nilsson",
      "Andrew Gower",
      "David Bull"
    ],
    "published": "2025-12-03T17:56:44+00:00",
    "url": "https://arxiv.org/pdf/2512.04019v1",
    "categories": [
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "arxiv_id": "2512.04016v1",
    "title": "TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees",
    "abstract": "Quantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches. TARA k, based on Kolmogorov Smirnov calibration against local hidden variable (LHV) null distributions, achieving ROC AUC = 0.96 for quantum-classical discrimination. And TARA-m, employing betting martingales for streaming detection with anytime valid type I error control that enables real time monitoring of quantum channels. We establish theoretical guarantees proving that under (context conditional) exchangeability, conformal p-values remain uniformly distributed even for strongly contextual quantum data, confirming that quantum contextuality does not break conformal prediction validity a result with implications beyond quantum certification to any application of distribution-free methods to nonclassical data. Extensive validation on both IBM Torino (superconducting, CHSH = 2.725) and IonQ Forte Enterprise (trapped ion, CHSH = 2.716) quantum processors demonstrates cross-platform robustness, achieving 36% security margins above the classical CHSH bound of 2. Critically, our framework reveals a methodological concern affecting quantum certification more broadly: same-distribution calibration can inflate detection performance by up to 44 percentage points compared to proper cross-distribution calibration, suggesting that prior quantum certification studies using standard train test splits may have systematically overestimated adversarial robustness.",
    "authors": [
      "Davut Emre Tasar",
      "Ceren Ocal Tasar"
    ],
    "published": "2025-12-03T17:53:38+00:00",
    "url": "https://arxiv.org/pdf/2512.04016v1",
    "categories": [
      "quant-ph",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.04015v1",
    "title": "Learning Group Actions In Disentangled Latent Image Representations",
    "abstract": "Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .",
    "authors": [
      "Farhana Hossain Swarnali",
      "Miaomiao Zhang",
      "Tonmoy Hossain"
    ],
    "published": "2025-12-03T17:52:24+00:00",
    "url": "https://arxiv.org/pdf/2512.04015v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.04013v1",
    "title": "AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving",
    "abstract": "As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.   This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.",
    "authors": [
      "Ying Wang",
      "Zhen Jin",
      "Jiexiong Xu",
      "Wenhai Lin",
      "Yiquan Chen",
      "Wenzhi Chen"
    ],
    "published": "2025-12-03T17:49:38+00:00",
    "url": "https://arxiv.org/pdf/2512.04013v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.04012v1",
    "title": "Emergent Outlier View Rejection in Visual Geometry Grounded Transformers",
    "abstract": "Reliable 3D reconstruction from in-the-wild image collections is often hindered by \"noisy\" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.",
    "authors": [
      "Jisang Han",
      "Sunghwan Hong",
      "Jaewoo Jung",
      "Wooseok Jang",
      "Honggyu An",
      "Qianqian Wang",
      "Seungryong Kim",
      "Chen Feng"
    ],
    "published": "2025-12-03T17:48:25+00:00",
    "url": "https://arxiv.org/pdf/2512.04012v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.04007v1",
    "title": "On the Temporality for Sketch Representation Learning",
    "abstract": "Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.",
    "authors": [
      "Marcelo Isaias de Moraes Junior",
      "Moacir Antonelli Ponti"
    ],
    "published": "2025-12-03T17:46:05+00:00",
    "url": "https://arxiv.org/pdf/2512.04007v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.04000v1",
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "abstract": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
    "authors": [
      "Jialuo Li",
      "Bin Li",
      "Jiahao Li",
      "Yan Lu"
    ],
    "published": "2025-12-03T17:36:06+00:00",
    "url": "https://arxiv.org/pdf/2512.04000v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03996v1",
    "title": "Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation",
    "abstract": "Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \\href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.",
    "authors": [
      "Hang Xu",
      "Linjiang Huang",
      "Feng Zhao"
    ],
    "published": "2025-12-03T17:27:53+00:00",
    "url": "https://arxiv.org/pdf/2512.03996v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03995v1",
    "title": "Artificial Microsaccade Compensation: Stable Vision for an Ornithopter",
    "abstract": "Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for \"Artificial Microsaccade Compensation\". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.",
    "authors": [
      "Levi Burner",
      "Guido de Croon",
      "Yiannis Aloimonos"
    ],
    "published": "2025-12-03T17:24:02+00:00",
    "url": "https://arxiv.org/pdf/2512.03995v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03992v1",
    "title": "DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation",
    "abstract": "Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.",
    "authors": [
      "Zexin Lin",
      "Hawen Wan",
      "Yebin Zhong",
      "Xiaoqiang"
    ],
    "published": "2025-12-03T17:22:29+00:00",
    "url": "https://arxiv.org/pdf/2512.03992v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03989v1",
    "title": "Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models",
    "abstract": "Tokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package.",
    "authors": [
      "Taido Purason",
      "Pavel Chizhov",
      "Ivan P. Yamshchikov",
      "Mark Fishel"
    ],
    "published": "2025-12-03T17:20:16+00:00",
    "url": "https://arxiv.org/pdf/2512.03989v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03981v1",
    "title": "DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment",
    "abstract": "Drag-based image editing using generative models provides intuitive control over image structures. However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision. Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts. To address these limitations, we propose DirectDrag, a novel mask- and prompt-free editing framework. DirectDrag enables precise and efficient manipulation with minimal user input while maintaining high image fidelity and accurate point alignment. DirectDrag introduces two key innovations. First, we design an Auto Soft Mask Generation module that intelligently infers editable regions from point displacement, automatically localizing deformation along movement paths while preserving contextual integrity through the generative model's inherent capacity. Second, we develop a Readout-Guided Feature Alignment mechanism that leverages intermediate diffusion activations to maintain structural consistency during point-based edits, substantially improving visual fidelity. Despite operating without manual mask or prompt, DirectDrag achieves superior image quality compared to existing methods while maintaining competitive drag accuracy. Extensive experiments on DragBench and real-world scenarios demonstrate the effectiveness and practicality of DirectDrag for high-quality, interactive image manipulation. Project Page: https://frakw.github.io/DirectDrag/. Code is available at: https://github.com/frakw/DirectDrag.",
    "authors": [
      "Sheng-Hao Liao",
      "Shang-Fu Chen",
      "Tai-Ming Huang",
      "Wen-Huang Cheng",
      "Kai-Lung Hua"
    ],
    "published": "2025-12-03T17:12:00+00:00",
    "url": "https://arxiv.org/pdf/2512.03981v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03979v1",
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "abstract": "Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.",
    "authors": [
      "Jin-Ting He",
      "Fu-Jen Tsai",
      "Yan-Tsung Peng",
      "Min-Hung Chen",
      "Chia-Wen Lin",
      "Yen-Yu Lin"
    ],
    "published": "2025-12-03T17:10:44+00:00",
    "url": "https://arxiv.org/pdf/2512.03979v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03976v1",
    "title": "Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study",
    "abstract": "Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\\rightarrow$ 1.54) and substantial improvements in Chinese$\\rightarrow$Tibetan translation quality (BLEU: 0.046 $\\rightarrow$ 0.261; chrF: 2.2 $\\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.",
    "authors": [
      "Lifeng Chen",
      "Ryan Lai",
      "Tianming Liu"
    ],
    "published": "2025-12-03T17:06:51+00:00",
    "url": "https://arxiv.org/pdf/2512.03976v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03975v1",
    "title": "Sponsored Questions and How to Auction Them",
    "abstract": "Online platforms connect users with relevant products and services using ads. A key challenge is that a user's search query often leaves their true intent ambiguous. Typically, platforms passively predict relevance based on available signals and in some cases offer query refinements. The shift from traditional search to conversational AI provides a new approach. When a user's query is ambiguous, a Large Language Model (LLM) can proactively offer several clarifying follow-up prompts. In this paper we consider the following: what if some of these follow-up prompts can be ``sponsored,'' i.e., selected for their advertising potential. How should these ``suggestion slots'' be allocated? And, how does this new mechanism interact with the traditional ad auction that might follow?   This paper introduces a formal model for designing and analyzing these interactive platforms. We use this model to investigate a critical engineering choice: whether it is better to build an end-to-end pipeline that jointly optimizes the user interaction and the final ad auction, or to decouple them into separate mechanisms for the suggestion slots and another for the subsequent ad slot. We show that the VCG mechanism can be adopted to jointly optimize the sponsored suggestion and the ads that follow; while this mechanism is more complex, it achieves outcomes that are efficient and truthful. On the other hand, we prove that the simple-to-implement modular approach suffers from strategic inefficiency: its Price of Anarchy is unbounded.",
    "authors": [
      "Kshipra Bhawalkar",
      "Alexandros Psomas",
      "Di Wang"
    ],
    "published": "2025-12-03T17:06:27+00:00",
    "url": "https://arxiv.org/pdf/2512.03975v1",
    "categories": [
      "cs.GT",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03973v1",
    "title": "Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning",
    "abstract": "Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/",
    "authors": [
      "Franki Nguimatsia Tiofack",
      "Th\u00e9otime Le Hellard",
      "Fabian Schramm",
      "Nicolas Perrin-Gilbert",
      "Justin Carpentier"
    ],
    "published": "2025-12-03T17:05:58+00:00",
    "url": "https://arxiv.org/pdf/2512.03973v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03964v1",
    "title": "Training for Identity, Inference for Controllability: A Unified Approach to Tuning-Free Face Personalization",
    "abstract": "Tuning-free face personalization methods have developed along two distinct paradigms: text embedding approaches that map facial features into the text embedding space, and adapter-based methods that inject features through auxiliary cross-attention layers. While both paradigms have shown promise, existing methods struggle to simultaneously achieve high identity fidelity and flexible text controllability. We introduce UniID, a unified tuning-free framework that synergistically integrates both paradigms. Our key insight is that when merging these approaches, they should mutually reinforce only identity-relevant information while preserving the original diffusion prior for non-identity attributes. We realize this through a principled training-inference strategy: during training, we employ an identity-focused learning scheme that guides both branches to capture identity features exclusively; at inference, we introduce a normalized rescaling mechanism that recovers the text controllability of the base diffusion model while enabling complementary identity signals to enhance each other. This principled design enables UniID to achieve high-fidelity face personalization with flexible text controllability. Extensive experiments against six state-of-the-art methods demonstrate that UniID achieves superior performance in both identity preservation and text controllability. Code will be available at https://github.com/lyuPang/UniID",
    "authors": [
      "Lianyu Pang",
      "Ji Zhou",
      "Qiping Wang",
      "Baoquan Zhao",
      "Zhenguo Yang",
      "Qing Li",
      "Xudong Mao"
    ],
    "published": "2025-12-03T16:57:50+00:00",
    "url": "https://arxiv.org/pdf/2512.03964v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03963v1",
    "title": "TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning",
    "abstract": "Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.",
    "authors": [
      "Tao Wu",
      "Li Yang",
      "Gen Zhan",
      "Yiting Liao",
      "Junlin Li",
      "Deliang Fu",
      "Li Zhang",
      "Limin Wang"
    ],
    "published": "2025-12-03T16:57:00+00:00",
    "url": "https://arxiv.org/pdf/2512.03963v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03962v1",
    "title": "Tada-DIP: Input-adaptive Deep Image Prior for One-shot 3D Image Reconstruction",
    "abstract": "Deep Image Prior (DIP) has recently emerged as a promising one-shot neural-network based image reconstruction method. However, DIP has seen limited application to 3D image reconstruction problems. In this work, we introduce Tada-DIP, a highly effective and fully 3D DIP method for solving 3D inverse problems. By combining input-adaptation and denoising regularization, Tada-DIP produces high-quality 3D reconstructions while avoiding the overfitting phenomenon that is common in DIP. Experiments on sparse-view X-ray computed tomography reconstruction validate the effectiveness of the proposed method, demonstrating that Tada-DIP produces much better reconstructions than training-data-free baselines and achieves reconstruction performance on par with a supervised network trained using a large dataset with fully-sampled volumes.",
    "authors": [
      "Evan Bell",
      "Shijun Liang",
      "Ismail Alkhouri",
      "Saiprasad Ravishankar"
    ],
    "published": "2025-12-03T16:56:38+00:00",
    "url": "https://arxiv.org/pdf/2512.03962v1",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03955v1",
    "title": "Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol",
    "abstract": "Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.",
    "authors": [
      "Niklas Jobs",
      "Luis Miguel Vieira da Silva",
      "Jayanth Somashekaraiah",
      "Maximilian Weigand",
      "David Kube",
      "Felix Gehlhoff"
    ],
    "published": "2025-12-03T16:49:14+00:00",
    "url": "https://arxiv.org/pdf/2512.03955v1",
    "categories": [
      "cs.AI",
      "cs.ET"
    ]
  },
  {
    "arxiv_id": "2512.03943v1",
    "title": "Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions",
    "abstract": "While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.",
    "authors": [
      "Kazi Abrab Hossain",
      "Jannatul Somiya Mahmud",
      "Maria Hossain Tuli",
      "Anik Mitra",
      "S. M. Taiabul Haque",
      "Farig Y. Sadeque"
    ],
    "published": "2025-12-03T16:38:41+00:00",
    "url": "https://arxiv.org/pdf/2512.03943v1",
    "categories": [
      "cs.CL",
      "cs.HC"
    ]
  },
  {
    "arxiv_id": "2512.03939v1",
    "title": "MUT3R: Motion-aware Updating Transformer for Dynamic 3D Reconstruction",
    "abstract": "Recent stateful recurrent neural networks have achieved remarkable progress on static 3D reconstruction but remain vulnerable to motion-induced artifacts, where non-rigid regions corrupt attention propagation between the spatial memory and image feature. By analyzing the internal behaviors of the state and image token updating mechanism, we find that aggregating self-attention maps across layers reveals a consistent pattern: dynamic regions are naturally down-weighted, exposing an implicit motion cue that the pretrained transformer already encodes but never explicitly uses. Motivated by this observation, we introduce MUT3R, a training-free framework that applies the attention-derived motion cue to suppress dynamic content in the early layers of the transformer during inference. Our attention-level gating module suppresses the influence of dynamic regions before their artifacts propagate through the feature hierarchy. Notably, we do not retrain or fine-tune the model; we let the pretrained transformer diagnose its own motion cues and correct itself. This early regulation stabilizes geometric reasoning in streaming scenarios and leads to improvements in temporal consistency and camera pose robustness across multiple dynamic benchmarks, offering a simple and training-free pathway toward motion-aware streaming reconstruction.",
    "authors": [
      "Guole Shen",
      "Tianchen Deng",
      "Xingrui Qin",
      "Nailin Wang",
      "Jianyu Wang",
      "Yanbo Wang",
      "Yongtao Chen",
      "Hesheng Wang",
      "Jingchuan Wang"
    ],
    "published": "2025-12-03T16:36:53+00:00",
    "url": "https://arxiv.org/pdf/2512.03939v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.03932v1",
    "title": "Beyond the Ground Truth: Enhanced Supervision for Image Restoration",
    "abstract": "Deep learning-based image restoration has achieved significant success. However, when addressing real-world degradations, model performance is limited by the quality of ground-truth images in datasets due to practical constraints in data acquisition. To address this limitation, we propose a novel framework that enhances existing ground truth images to provide higher-quality supervision for real-world restoration. Our framework generates perceptually enhanced ground truth images using super-resolution by incorporating adaptive frequency masks, which are learned by a conditional frequency mask generator. These masks guide the optimal fusion of frequency components from the original ground truth and its super-resolved variants, yielding enhanced ground truth images. This frequency-domain mixup preserves the semantic consistency of the original content while selectively enriching perceptual details, preventing hallucinated artifacts that could compromise fidelity. The enhanced ground truth images are used to train a lightweight output refinement network that can be seamlessly integrated with existing restoration models. Extensive experiments demonstrate that our approach consistently improves the quality of restored images. We further validate the effectiveness of both supervision enhancement and output refinement through user studies. Code is available at https://github.com/dhryougit/Beyond-the-Ground-Truth.",
    "authors": [
      "Donghun Ryou",
      "Inju Ha",
      "Sanghyeok Chu",
      "Bohyung Han"
    ],
    "published": "2025-12-03T16:30:32+00:00",
    "url": "https://arxiv.org/pdf/2512.03932v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03931v1",
    "title": "Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties",
    "abstract": "This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).",
    "authors": [
      "Vineel Tummala",
      "Daniela Inclezan"
    ],
    "published": "2025-12-03T16:29:09+00:00",
    "url": "https://arxiv.org/pdf/2512.03931v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03918v1",
    "title": "UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework",
    "abstract": "We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.",
    "authors": [
      "Youxin Pang",
      "Yong Zhang",
      "Ruizhi Shao",
      "Xiang Deng",
      "Feng Gao",
      "Xu Xiaoming",
      "Xiaoming Wei",
      "Yebin Liu"
    ],
    "published": "2025-12-03T16:03:18+00:00",
    "url": "https://arxiv.org/pdf/2512.03918v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03915v1",
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
    "authors": [
      "X. Y. Han",
      "Yuan Zhong"
    ],
    "published": "2025-12-03T16:00:02+00:00",
    "url": "https://arxiv.org/pdf/2512.03915v1",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03913v1",
    "title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations",
    "abstract": "Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.",
    "authors": [
      "Jeongeun Park",
      "Jihwan Yoon",
      "Byungwoo Jeon",
      "Juhan Park",
      "Jinwoo Shin",
      "Namhoon Cho",
      "Kyungjae Lee",
      "Sangdoo Yun",
      "Sungjoon Choi"
    ],
    "published": "2025-12-03T15:58:38+00:00",
    "url": "https://arxiv.org/pdf/2512.03913v1",
    "categories": [
      "cs.RO",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03911v1",
    "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware",
    "abstract": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.",
    "authors": [
      "Kenneth Stewart",
      "Roxana Leontie",
      "Samantha Chapin",
      "Joe Hays",
      "Sumit Bam Shrestha",
      "Carl Glen Henshaw"
    ],
    "published": "2025-12-03T15:56:39+00:00",
    "url": "https://arxiv.org/pdf/2512.03911v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03905v1",
    "title": "Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence",
    "abstract": "The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.",
    "authors": [
      "Shuai Yang",
      "Junxin Lin",
      "Yifan Zhou",
      "Ziwei Liu",
      "Chen Change Loy"
    ],
    "published": "2025-12-03T15:51:11+00:00",
    "url": "https://arxiv.org/pdf/2512.03905v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03903v1",
    "title": "BERnaT: Basque Encoders for Representing Natural Textual Diversity",
    "abstract": "Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.",
    "authors": [
      "Ekhi Azurmendi",
      "Joseba Fernandez de Landa",
      "Jaione Bengoetxea",
      "Maite Heredia",
      "Julen Etxaniz",
      "Mikel Zubillaga",
      "Ander Soraluze",
      "Aitor Soroa"
    ],
    "published": "2025-12-03T15:50:42+00:00",
    "url": "https://arxiv.org/pdf/2512.03903v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03887v1",
    "title": "A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)",
    "abstract": "The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.   The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.   We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/",
    "authors": [
      "Saurav Prateek"
    ],
    "published": "2025-12-03T15:37:13+00:00",
    "url": "https://arxiv.org/pdf/2512.03887v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03883v1",
    "title": "Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy",
    "abstract": "Increasing evidence supports watch-and-wait (WW) surveillance for patients with rectal cancer who show clinical complete response (cCR) at restaging following total neoadjuvant treatment (TNT). However, objectively accurate methods to early detect local regrowth (LR) from follow-up endoscopy images during WW are essential to manage care and prevent distant metastases. Hence, we developed a Siamese Swin Transformer with Dual Cross-Attention (SSDCA) to combine longitudinal endoscopic images at restaging and follow-up and distinguish cCR from LR. SSDCA leverages pretrained Swin transformers to extract domain agnostic features and enhance robustness to imaging variations. Dual cross attention is implemented to emphasize features from the two scans without requiring any spatial alignment of images to predict response. SSDCA as well as Swin-based baselines were trained using image pairs from 135 patients and evaluated on a held-out set of image pairs from 62 patients. SSDCA produced the best balanced accuracy (81.76\\% $\\pm$ 0.04), sensitivity (90.07\\% $\\pm$ 0.08), and specificity (72.86\\% $\\pm$ 0.05). Robustness analysis showed stable performance irrespective of artifacts including blood, stool, telangiectasia, and poor image quality. UMAP clustering of extracted features showed maximal inter-cluster separation (1.45 $\\pm$ 0.18) and minimal intra-cluster dispersion (1.07 $\\pm$ 0.19) with SSDCA, confirming discriminative representation learning.",
    "authors": [
      "Jorge Tapias Gomez",
      "Despoina Kanata",
      "Aneesh Rangnekar",
      "Christina Lee",
      "Julio Garcia-Aguilar",
      "Joshua Jesse Smith",
      "Harini Veeraraghavan"
    ],
    "published": "2025-12-03T15:34:29+00:00",
    "url": "https://arxiv.org/pdf/2512.03883v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03870v1",
    "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
    "abstract": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.",
    "authors": [
      "Hongzhan Lin",
      "Zhiqi Bai",
      "Xinmiao Zhang",
      "Sen Yang",
      "Xiang Li",
      "Siran Yang",
      "Yunlong Xu",
      "Jiaheng Liu",
      "Yongchi Zhao",
      "Jiamang Wang",
      "Yuchi Xu",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "published": "2025-12-03T15:22:00+00:00",
    "url": "https://arxiv.org/pdf/2512.03870v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03869v1",
    "title": "An Automated Framework for Large-Scale Graph-Based Cerebrovascular Analysis",
    "abstract": "We present CaravelMetrics, a computational framework for automated cerebrovascular analysis that models vessel morphology through skeletonization-derived graph representations. The framework integrates atlas-based regional parcellation, centerline extraction, and graph construction to compute fifteen morphometric, topological, fractal, and geometric features. The features can be estimated globally from the complete vascular network or regionally within arterial territories, enabling multiscale characterization of cerebrovascular organization. Applied to 570 3D TOF-MRA scans from the IXI dataset (ages 20-86), CaravelMetrics yields reproducible vessel graphs capturing age- and sex-related variations and education-associated increases in vascular complexity, consistent with findings reported in the literature. The framework provides a scalable and fully automated approach for quantitative cerebrovascular feature extraction, supporting normative modeling and population-level studies of vascular health and aging.",
    "authors": [
      "Daniele Falcetta",
      "Liane S. Canas",
      "Lorenzo Suppa",
      "Matteo Pentassuglia",
      "Jon Cleary",
      "Marc Modat",
      "S\u00e9bastien Ourselin",
      "Maria A. Zuluaga"
    ],
    "published": "2025-12-03T15:21:51+00:00",
    "url": "https://arxiv.org/pdf/2512.03869v1",
    "categories": [
      "cs.CV",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.03864v1",
    "title": "Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment",
    "abstract": "Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\\times$ for training and 175 to 1000$\\times$ for inference. Furthermore, HDC reduces training times by 200$\\times$ and inference times by 300 to 600$\\times$, showcasing its potential for energy-efficient smart manufacturing.",
    "authors": [
      "Danny Hoang",
      "Anandkumar Patel",
      "Ruimen Chen",
      "Rajiv Malhotra",
      "Farhad Imani"
    ],
    "published": "2025-12-03T15:14:34+00:00",
    "url": "https://arxiv.org/pdf/2512.03864v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF",
      "cs.SC"
    ]
  },
  {
    "arxiv_id": "2512.03862v1",
    "title": "Diminishing Returns in Self-Supervised Learning",
    "abstract": "While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.",
    "authors": [
      "Oli Bridge",
      "Huey Sun",
      "Botond Branyicskai-Nagy",
      "Charles D'Ornano",
      "Shomit Basu"
    ],
    "published": "2025-12-03T15:11:44+00:00",
    "url": "https://arxiv.org/pdf/2512.03862v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03861v1",
    "title": "Scalable Decision Focused Learning via Online Trainable Surrogates",
    "abstract": "Decision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques.",
    "authors": [
      "Gaetano Signorelli",
      "Michele Lombardi"
    ],
    "published": "2025-12-03T15:09:21+00:00",
    "url": "https://arxiv.org/pdf/2512.03861v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03854v1",
    "title": "Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population",
    "abstract": "Artificial intelligence (AI) is increasingly used in digital pathology. Publicly available histopathology datasets remain scarce, and those that do exist predominantly represent Western populations. Consequently, the generalizability of AI models to populations from less digitized regions, such as the Middle East, is largely unknown. This motivates the public release of our dataset to support the development and validation of pathology AI models across globally diverse populations. We present 339 whole-slide images of prostate core needle biopsies from a consecutive series of 185 patients collected in Erbil, Iraq. The slides are associated with Gleason scores and International Society of Urological Pathology grades assigned independently by three pathologists. Scanning was performed using two high-throughput scanners (Leica and Hamamatsu) and one compact scanner (Grundium). All slides were de-identified and are provided in their native formats without further conversion. The dataset enables grading concordance analyses, color normalization, and cross-scanner robustness evaluations. Data will be deposited in the Bioimage Archive (BIA) under accession code: to be announced (TBA), and released under a CC BY 4.0 license.",
    "authors": [
      "Peshawa J. Muhammad Ali",
      "Navin Vincent",
      "Saman S. Abdulla",
      "Han N. Mohammed Fadhl",
      "Anders Blilie",
      "Kelvin Szolnoky",
      "Julia Anna Mielcarz",
      "Xiaoyi Ji",
      "Kimmo Kartasalo",
      "Abdulbasit K. Al-Talabani",
      "Nita Mulliqi"
    ],
    "published": "2025-12-03T14:54:14+00:00",
    "url": "https://arxiv.org/pdf/2512.03854v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03852v1",
    "title": "Traffic Image Restoration under Adverse Weather via Frequency-Aware Mamba",
    "abstract": "Traffic image restoration under adverse weather conditions remains a critical challenge for intelligent transportation systems. Existing methods primarily focus on spatial-domain modeling but neglect frequency-domain priors. Although the emerging Mamba architecture excels at long-range dependency modeling through patch-wise correlation analysis, its potential for frequency-domain feature extraction remains unexplored. To address this, we propose Frequency-Aware Mamba (FAMamba), a novel framework that integrates frequency guidance with sequence modeling for efficient image restoration. Our architecture consists of two key components: (1) a Dual-Branch Feature Extraction Block (DFEB) that enhances local-global interaction via bidirectional 2D frequency-adaptive scanning, dynamically adjusting traversal paths based on sub-band texture distributions; and (2) a Prior-Guided Block (PGB) that refines texture details through wavelet-based high-frequency residual learning, enabling high-quality image reconstruction with precise details. Meanwhile, we design a novel Adaptive Frequency Scanning Mechanism (AFSM) for the Mamba architecture, which enables the Mamba to achieve frequency-domain scanning across distinct subgraphs, thereby fully leveraging the texture distribution characteristics inherent in subgraph structures. Extensive experiments demonstrate the efficiency and effectiveness of FAMamba.",
    "authors": [
      "Liwen Pan",
      "Longguang Wang",
      "Guangwei Gao",
      "Jun Wang",
      "Jun Shi",
      "Juncheng Li"
    ],
    "published": "2025-12-03T14:50:20+00:00",
    "url": "https://arxiv.org/pdf/2512.03852v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03848v1",
    "title": "PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation",
    "abstract": "Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.",
    "authors": [
      "Hania Ghouse",
      "Maryam Alsharqi",
      "Farhad R. Nezami",
      "Muzammil Behzad"
    ],
    "published": "2025-12-03T14:49:01+00:00",
    "url": "https://arxiv.org/pdf/2512.03848v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03847v1",
    "title": "DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training",
    "abstract": "Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.",
    "authors": [
      "Dingwei Zhu",
      "Zhiheng Xi",
      "Shihan Dou",
      "Yuhui Wang",
      "Sixian Li",
      "Junjie Ye",
      "Honglin Guo",
      "Shichun Liu",
      "Chenhao Huang",
      "Yajie Yang",
      "Junlin Shang",
      "Senjie Jin",
      "Ming Zhang",
      "Jiazheng Zhang",
      "Caishuang Huang",
      "Yunke Zhang",
      "Demei Yan",
      "Yuran Wang",
      "Tao Gui"
    ],
    "published": "2025-12-03T14:48:38+00:00",
    "url": "https://arxiv.org/pdf/2512.03847v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03844v1",
    "title": "CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation",
    "abstract": "Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the \"intrinsic core distribution\" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA",
    "authors": [
      "Letian Zhou",
      "Songhua Liu",
      "Xinchao Wang"
    ],
    "published": "2025-12-03T14:45:57+00:00",
    "url": "https://arxiv.org/pdf/2512.03844v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03838v1",
    "title": "Training and Evaluation of Guideline-Based Medical Reasoning in LLMs",
    "abstract": "Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.",
    "authors": [
      "Michael Staniek",
      "Artem Sokolov",
      "Stefan Riezler"
    ],
    "published": "2025-12-03T14:39:02+00:00",
    "url": "https://arxiv.org/pdf/2512.03838v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03837v1",
    "title": "Heatmap Pooling Network for Action Recognition from RGB Videos",
    "abstract": "Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.",
    "authors": [
      "Mengyuan Liu",
      "Jinfu Liu",
      "Yongkang Jiang",
      "Bin He"
    ],
    "published": "2025-12-03T14:36:59+00:00",
    "url": "https://arxiv.org/pdf/2512.03837v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03834v1",
    "title": "Lean Unet: A Compact Model for Image Segmentation",
    "abstract": "Unet and its variations have been standard in semantic image segmentation, especially for computer assisted radiology. Current Unet architectures iteratively downsample spatial resolution while increasing channel dimensions to preserve information content. Such a structure demands a large memory footprint, limiting training batch sizes and increasing inference latency. Channel pruning compresses Unet architecture without accuracy loss, but requires lengthy optimization and may not generalize across tasks and datasets. By investigating Unet pruning, we hypothesize that the final structure is the crucial factor, not the channel selection strategy of pruning. Based on our observations, we propose a lean Unet architecture (LUnet) with a compact, flat hierarchy where channels are not doubled as resolution is halved. We evaluate on a public MRI dataset allowing comparable reporting, as well as on two internal CT datasets. We show that a state-of-the-art pruning solution (STAMP) mainly prunes from the layers with the highest number of channels. Comparatively, simply eliminating a random channel at the pruning-identified layer or at the largest layer achieves similar or better performance. Our proposed LUnet with fixed architectures and over 30 times fewer parameters achieves performance comparable to both conventional Unet counterparts and data-adaptively pruned networks. The proposed lean Unet with constant channel count across layers requires far fewer parameters while achieving performance superior to standard Unet for the same total number of parameters. Skip connections allow Unet bottleneck channels to be largely reduced, unlike standard encoder-decoder architectures requiring increased bottleneck channels for information propagation.",
    "authors": [
      "Ture Hassler",
      "Ida \u00c5kerholm",
      "Marcus Nordstr\u00f6m",
      "Gabriele Balletti",
      "Orcun Goksel"
    ],
    "published": "2025-12-03T14:35:21+00:00",
    "url": "https://arxiv.org/pdf/2512.03834v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03827v1",
    "title": "A Robust Camera-based Method for Breath Rate Measurement",
    "abstract": "Proliferation of cheap and accessible cameras makes it possible to measure a subject's breath rate from video footage alone. Recent works on this topic have proposed a variety of approaches for accurately measuring human breath rate, however they are either tested in near-ideal conditions, or produce results that are not sufficiently accurate. The present study proposes a more robust method to measure breath rate in humans with minimal hardware requirements using a combination of mathematical transforms with a relative deviation from the ground truth of less than 5%. The method was tested on videos taken from 14 volunteers with a total duration of over 2 hours 30 minutes. The obtained results were compared to reference data and the average mean absolute error was found to be at 0.57 respirations per minute, which is noticeably better than the results from previous works. The breath rate measurement method proposed in the present article is more resistant to distortions caused by subject movement and thus allows one to remotely measure the subject's breath rate without any significant limitations on the subject's behavior.",
    "authors": [
      "Alexey Protopopov"
    ],
    "published": "2025-12-03T14:19:21+00:00",
    "url": "https://arxiv.org/pdf/2512.03827v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03818v1",
    "title": "Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology",
    "abstract": "Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.",
    "authors": [
      "Kylie L. Anglin",
      "Stephanie Milan",
      "Brittney Hernandez",
      "Claudia Ventura"
    ],
    "published": "2025-12-03T14:07:42+00:00",
    "url": "https://arxiv.org/pdf/2512.03818v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03817v1",
    "title": "HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English",
    "abstract": "Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.",
    "authors": [
      "Ahmed Nasser",
      "Marwan Mohamed",
      "Alaa Sherif",
      "Basmala Mahmoud",
      "Shereen Yehia",
      "Asmaa Saad",
      "Mariam S. El-Rahmany",
      "Ensaf H. Mohamed"
    ],
    "published": "2025-12-03T14:05:18+00:00",
    "url": "https://arxiv.org/pdf/2512.03817v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03803v1",
    "title": "Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5",
    "abstract": "Contrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities.",
    "authors": [
      "Huey Sun",
      "Anabel Yong",
      "Lorenzo Gilly",
      "Felipe Jin"
    ],
    "published": "2025-12-03T13:54:11+00:00",
    "url": "https://arxiv.org/pdf/2512.03803v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03796v1",
    "title": "LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling",
    "abstract": "Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.",
    "authors": [
      "Hong-Kai Zheng",
      "Piji Li"
    ],
    "published": "2025-12-03T13:44:30+00:00",
    "url": "https://arxiv.org/pdf/2512.03796v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03795v1",
    "title": "MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving",
    "abstract": "Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.",
    "authors": [
      "Jia Hu",
      "Zhexi Lian",
      "Xuerun Yan",
      "Ruiang Bi",
      "Dou Shen",
      "Yu Ruan",
      "Haoran Wang"
    ],
    "published": "2025-12-03T13:43:33+00:00",
    "url": "https://arxiv.org/pdf/2512.03795v1",
    "categories": [
      "cs.RO",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03794v1",
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "abstract": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.",
    "authors": [
      "Zichuan Lin",
      "Yicheng Liu",
      "Yang Yang",
      "Lvfang Tao",
      "Deheng Ye"
    ],
    "published": "2025-12-03T13:43:30+00:00",
    "url": "https://arxiv.org/pdf/2512.03794v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03783v1",
    "title": "Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning",
    "abstract": "Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.",
    "authors": [
      "Dongchao Yang",
      "Songxiang Liu",
      "Disong Wang",
      "Yuanyuan Wang",
      "Guanglu Wan",
      "Helen Meng"
    ],
    "published": "2025-12-03T13:33:28+00:00",
    "url": "https://arxiv.org/pdf/2512.03783v1",
    "categories": [
      "cs.AI",
      "cs.SD"
    ]
  },
  {
    "arxiv_id": "2512.03772v1",
    "title": "Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control",
    "abstract": "This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.",
    "authors": [
      "Gabriele Fadini",
      "Deepak Ingole",
      "Tong Duy Son",
      "Alisa Rupenyan"
    ],
    "published": "2025-12-03T13:19:42+00:00",
    "url": "https://arxiv.org/pdf/2512.03772v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ]
  },
  {
    "arxiv_id": "2512.03771v1",
    "title": "In-Context Representation Hijacking",
    "abstract": "We introduce \\textbf{Doublespeak}, a simple \\emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \\textit{bomb}) with a benign token (e.g., \\textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
    "authors": [
      "Itay Yona",
      "Amir Sarid",
      "Michael Karasik",
      "Yossi Gandelsman"
    ],
    "published": "2025-12-03T13:19:34+00:00",
    "url": "https://arxiv.org/pdf/2512.03771v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03762v1",
    "title": "RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design",
    "abstract": "Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.",
    "authors": [
      "Jiawei Xu",
      "Fengfeng Wei",
      "Weineng Chen"
    ],
    "published": "2025-12-03T13:09:34+00:00",
    "url": "https://arxiv.org/pdf/2512.03762v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03759v1",
    "title": "Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective",
    "abstract": "Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.",
    "authors": [
      "Jingyang Ou",
      "Jiaqi Han",
      "Minkai Xu",
      "Shaoxuan Xu",
      "Jianwen Xie",
      "Stefano Ermon",
      "Yi Wu",
      "Chongxuan Li"
    ],
    "published": "2025-12-03T13:05:32+00:00",
    "url": "https://arxiv.org/pdf/2512.03759v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03751v1",
    "title": "Research on Brain Tumor Classification Method Based on Improved ResNet34 Network",
    "abstract": "Previously, image interpretation in radiology relied heavily on manual methods. However, manual classification of brain tumor medical images is time-consuming and labor-intensive. Even with shallow convolutional neural network models, the accuracy is not ideal. To improve the efficiency and accuracy of brain tumor image classification, this paper proposes a brain tumor classification model based on an improved ResNet34 network. This model uses the ResNet34 residual network as the backbone network and incorporates multi-scale feature extraction. It uses a multi-scale input module as the first layer of the ResNet34 network and an Inception v2 module as the residual downsampling layer. Furthermore, a channel attention mechanism module assigns different weights to different channels of the image from a channel domain perspective, obtaining more important feature information. The results after a five-fold crossover experiment show that the average classification accuracy of the improved network model is approximately 98.8%, which is not only 1% higher than ResNet34, but also only 80% of the number of parameters of the original model. Therefore, the improved network model not only improves accuracy but also reduces clutter, achieving a classification effect with fewer parameters and higher accuracy.",
    "authors": [
      "Yufeng Li",
      "Wenchao Zhao",
      "Bo Dang",
      "Weimin Wang"
    ],
    "published": "2025-12-03T12:47:23+00:00",
    "url": "https://arxiv.org/pdf/2512.03751v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03749v1",
    "title": "Fully Unsupervised Self-debiasing of Text-to-Image Diffusion Models",
    "abstract": "Text-to-image (T2I) diffusion models have achieved widespread success due to their ability to generate high-resolution, photorealistic images. These models are trained on large-scale datasets, like LAION-5B, often scraped from the internet. However, since this data contains numerous biases, the models inherently learn and reproduce them, resulting in stereotypical outputs. We introduce SelfDebias, a fully unsupervised test-time debiasing method applicable to any diffusion model that uses a UNet as its noise predictor. SelfDebias identifies semantic clusters in an image encoder's embedding space and uses these clusters to guide the diffusion process during inference, minimizing the KL divergence between the output distribution and the uniform distribution. Unlike supervised approaches, SelfDebias does not require human-annotated datasets or external classifiers trained for each generated concept. Instead, it is designed to automatically identify semantic modes. Extensive experiments show that SelfDebias generalizes across prompts and diffusion model architectures, including both conditional and unconditional models. It not only effectively debiases images along key demographic dimensions while maintaining the visual fidelity of the generated images, but also more abstract concepts for which identifying biases is also challenging.",
    "authors": [
      "Korada Sri Vardhana",
      "Shrikrishna Lolla",
      "Soma Biswas"
    ],
    "published": "2025-12-03T12:46:42+00:00",
    "url": "https://arxiv.org/pdf/2512.03749v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03746v1",
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "abstract": "Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.",
    "authors": [
      "Zirun Guo",
      "Minjie Hong",
      "Feng Zhang",
      "Kai Jia",
      "Tao Jin"
    ],
    "published": "2025-12-03T12:44:15+00:00",
    "url": "https://arxiv.org/pdf/2512.03746v1",
    "categories": [
      "cs.CV",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03745v1",
    "title": "Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification",
    "abstract": "Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.",
    "authors": [
      "Jiaze Li",
      "Yan Lu",
      "Bin Liu",
      "Guojun Yin",
      "Mang Ye"
    ],
    "published": "2025-12-03T12:43:16+00:00",
    "url": "https://arxiv.org/pdf/2512.03745v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03737v1",
    "title": "AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation",
    "abstract": "Accurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce \\textbf{AR-Med}, a novel framework for \\textbf{A}utomated \\textbf{R}elevance assessment for \\textbf{Med}ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\\%, a 24\\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications.",
    "authors": [
      "Chuyue Wang",
      "Jie Feng",
      "Yuxi Wu",
      "Hang Zhang",
      "Zhiguo Fan",
      "Bing Cheng",
      "Wei Lin"
    ],
    "published": "2025-12-03T12:34:47+00:00",
    "url": "https://arxiv.org/pdf/2512.03737v1",
    "categories": [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "arxiv_id": "2512.03730v1",
    "title": "Out-of-the-box: Black-box Causal Attacks on Object Detectors",
    "abstract": "Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.",
    "authors": [
      "Melane Navaratnarajah",
      "David A. Kelly",
      "Hana Chockler"
    ],
    "published": "2025-12-03T12:17:35+00:00",
    "url": "https://arxiv.org/pdf/2512.03730v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03728v1",
    "title": "AI/ML in 3GPP 5G Advanced - Services and Architecture",
    "abstract": "The 3rd Generation Partnership Project (3GPP), the standards body for mobile networks, is in the final phase of Release 19 standardization and is beginning Release 20. Artificial Intelligence/ Machine Learning (AI/ML) has brought about a paradigm shift in technology and it is being adopted across industries and verticals. 3GPP has been integrating AI/ML into the 5G advanced system since Release 18. This paper focuses on the AI/ML related technological advancements and features introduced in Release 19 within the Service and System Aspects (SA) Technical specifications group of 3GPP. The advancements relate to two paradigms: (i) enhancements that AI/ML brought to the 5G advanced system (AI for network), e.g. resource optimization, and (ii) enhancements that were made to the 5G system to support AI/ML applications (Network for AI), e.g. image recognition.",
    "authors": [
      "Pradnya Taksande",
      "Shwetha Kiran",
      "Pranav Jha",
      "Prasanna Chaporkar"
    ],
    "published": "2025-12-03T12:16:08+00:00",
    "url": "https://arxiv.org/pdf/2512.03728v1",
    "categories": [
      "cs.ET",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03724v1",
    "title": "PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention",
    "abstract": "The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.",
    "authors": [
      "Ziwen Li",
      "Xin Wang",
      "Hanlue Zhang",
      "Runnan Chen",
      "Runqi Lin",
      "Xiao He",
      "Han Huang",
      "Yandong Guo",
      "Fakhri Karray",
      "Tongliang Liu",
      "Mingming Gong"
    ],
    "published": "2025-12-03T12:14:29+00:00",
    "url": "https://arxiv.org/pdf/2512.03724v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.03720v1",
    "title": "Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs",
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically balances semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.",
    "authors": [
      "Tengyun Ma",
      "Jiaqi Yao",
      "Daojing He",
      "Shihao Peng",
      "Yu Li",
      "Shaohui Liu",
      "Zhuotao Tian"
    ],
    "published": "2025-12-03T12:10:21+00:00",
    "url": "https://arxiv.org/pdf/2512.03720v1",
    "categories": [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03719v1",
    "title": "Over-the-Air Federated Learning: Rethinking Edge AI Through Signal Processing",
    "abstract": "Over-the-Air Federated Learning (AirFL) is an emerging paradigm that tightly integrates wireless signal processing and distributed machine learning to enable scalable AI at the network edge. By leveraging the superposition property of wireless signals, AirFL performs communication and model aggregation of the learning process simultaneously, significantly reducing latency, bandwidth, and energy consumption. This article offers a tutorial treatment of AirFL, presenting a novel classification into three design approaches: CSIT-aware, blind, and weighted AirFL. We provide a comprehensive guide to theoretical foundations, performance analysis, complexity considerations, practical limitations, and prospective research directions.",
    "authors": [
      "Seyed Mohammad Azimi-Abarghouyi",
      "Carlo Fischione",
      "Kaibin Huang"
    ],
    "published": "2025-12-03T12:10:15+00:00",
    "url": "https://arxiv.org/pdf/2512.03719v1",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03718v1",
    "title": "Matrix Editing Meets Fair Clustering: Parameterized Algorithms and Complexity",
    "abstract": "We study the computational problem of computing a fair means clustering of discrete vectors, which admits an equivalent formulation as editing a colored matrix into one with few distinct color-balanced rows by changing at most $k$ values. While NP-hard in both the fairness-oblivious and the fair settings, the problem is well-known to admit a fixed-parameter algorithm in the former ``vanilla'' setting. As our first contribution, we exclude an analogous algorithm even for highly restricted fair means clustering instances. We then proceed to obtain a full complexity landscape of the problem, and establish tractability results which capture three means of circumventing our obtained lower bound: placing additional constraints on the problem instances, fixed-parameter approximation, or using an alternative parameterization targeting tree-like matrices.",
    "authors": [
      "Robert Ganian",
      "Hung P. Hoang",
      "Simon Wietheger"
    ],
    "published": "2025-12-03T12:07:24+00:00",
    "url": "https://arxiv.org/pdf/2512.03718v1",
    "categories": [
      "cs.DS",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03715v1",
    "title": "DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction",
    "abstract": "This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images. The   method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and   matching. DINO is employed to retrieve semantically relevant image pairs in large collections, while   rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue. Experiments on the Kaggle Image Matching Challenge 2025 demonstrate consistent improve ments in mean Average Accuracy (mAA), achieving a Silver Award (47th of 943 teams). The results   confirm that combining self-supervised global descriptors with rotation-enhanced local matching offers   a robust and scalable solution for large-scale 3D reconstruction.",
    "authors": [
      "Kaichen Zhang",
      "Tianxiang Sheng",
      "Xuanming Shi"
    ],
    "published": "2025-12-03T12:05:49+00:00",
    "url": "https://arxiv.org/pdf/2512.03715v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03704v1",
    "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
    "abstract": "Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a \"Capacity-Stability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO",
    "authors": [
      "Yijun Liao"
    ],
    "published": "2025-12-03T11:56:53+00:00",
    "url": "https://arxiv.org/pdf/2512.03704v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03701v1",
    "title": "Structured Uncertainty Similarity Score (SUSS): Learning a Probabilistic, Interpretable, Perceptual Metric Between Images",
    "abstract": "Perceptual similarity scores that align with human vision are critical for both training and evaluating computer vision models. Deep perceptual losses, such as LPIPS, achieve good alignment but rely on complex, highly non-linear discriminative features with unknown invariances, while hand-crafted measures like SSIM are interpretable but miss key perceptual properties.   We introduce the Structured Uncertainty Similarity Score (SUSS); it models each image through a set of perceptual components, each represented by a structured multivariate Normal distribution. These are trained in a generative, self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets. Unlike feature-based methods, SUSS learns image-specific linear transformations of residuals in pixel space, enabling transparent inspection through decorrelated residuals and sampling.   SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, and provides localized, interpretable explanations of its similarity assessments. We further demonstrate stable optimization behavior and competitive performance when using SUSS as a perceptual loss for downstream imaging tasks.",
    "authors": [
      "Paula Seidler",
      "Neill D. F. Campbell",
      "Ivor J A Simpson"
    ],
    "published": "2025-12-03T11:48:59+00:00",
    "url": "https://arxiv.org/pdf/2512.03701v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03696v1",
    "title": "Quantum Topological Graph Neural Networks for Detecting Complex Fraud Patterns",
    "abstract": "We propose a novel QTGNN framework for detecting fraudulent transactions in large-scale financial networks. By integrating quantum embedding, variational graph convolutions, and topological data analysis, QTGNN captures complex transaction dynamics and structural anomalies indicative of fraud. The methodology includes quantum data embedding with entanglement enhancement, variational quantum graph convolutions with non-linear dynamics, extraction of higher-order topological invariants, hybrid quantum-classical anomaly learning with adaptive optimization, and interpretable decision-making via topological attribution. Rigorous convergence guarantees ensure stable training on noisy intermediate-scale quantum (NISQ) devices, while stability of topological signatures provides robust fraud detection. Optimized for NISQ hardware with circuit simplifications and graph sampling, the framework scales to large transaction networks. Simulations on financial datasets, such as PaySim and Elliptic, benchmark QTGNN against classical and quantum baselines, using metrics like ROC-AUC, precision, and false positive rate. An ablation study evaluates the contributions of quantum embeddings, topological features, non-linear channels, and hybrid learning. QTGNN offers a theoretically sound, interpretable, and practical solution for financial fraud detection, bridging quantum machine learning, graph theory, and topological analysis.",
    "authors": [
      "Mohammad Doost",
      "Mohammad Manthouri"
    ],
    "published": "2025-12-03T11:38:21+00:00",
    "url": "https://arxiv.org/pdf/2512.03696v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03688v1",
    "title": "AITutor-EvalKit: Exploring the Capabilities of AI Tutors",
    "abstract": "We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations.",
    "authors": [
      "Numaan Naeem",
      "Kaushal Kumar Maurya",
      "Kseniia Petukhova",
      "Ekaterina Kochmar"
    ],
    "published": "2025-12-03T11:27:50+00:00",
    "url": "https://arxiv.org/pdf/2512.03688v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03687v1",
    "title": "Active Visual Perception: Opportunities and Challenges",
    "abstract": "Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.",
    "authors": [
      "Yian Li",
      "Xiaoyu Guo",
      "Hao Zhang",
      "Shuiwang Li",
      "Xiaowei Dai"
    ],
    "published": "2025-12-03T11:27:37+00:00",
    "url": "https://arxiv.org/pdf/2512.03687v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03683v1",
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "abstract": "3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.",
    "authors": [
      "Melis Ocal",
      "Xiaoyan Xing",
      "Yue Li",
      "Ngo Anh Vien",
      "Sezer Karaoglu",
      "Theo Gevers"
    ],
    "published": "2025-12-03T11:23:07+00:00",
    "url": "https://arxiv.org/pdf/2512.03683v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03676v1",
    "title": "Different types of syntactic agreement recruit the same units within large language models",
    "abstract": "Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.",
    "authors": [
      "Daria Kryvosheieva",
      "Andrea de Varda",
      "Evelina Fedorenko",
      "Greta Tuckute"
    ],
    "published": "2025-12-03T11:07:50+00:00",
    "url": "https://arxiv.org/pdf/2512.03676v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03673v1",
    "title": "ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers",
    "abstract": "Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\\times$ speedup and 4.05$\\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.",
    "authors": [
      "Feice Huang",
      "Zuliang Han",
      "Xing Zhou",
      "Yihuang Chen",
      "Lifei Zhu",
      "Haoqian Wang"
    ],
    "published": "2025-12-03T11:02:16+00:00",
    "url": "https://arxiv.org/pdf/2512.03673v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03672v1",
    "title": "Evaluating Hydro-Science and Engineering Knowledge of Large Language Models",
    "abstract": "Hydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs.",
    "authors": [
      "Shiruo Hu",
      "Wenbo Shan",
      "Yingjia Li",
      "Zhiqi Wan",
      "Xinpeng Yu",
      "Yunjia Qi",
      "Haotian Xia",
      "Yang Xiao",
      "Dingxiao Liu",
      "Jiaru Wang",
      "Chenxu Gong",
      "Ruixi Zhang",
      "Shuyue Wu",
      "Shibo Cui",
      "Chee Hui Lai",
      "Wei Luo",
      "Yubin He",
      "Bin Xu",
      "Jianshi Zhao"
    ],
    "published": "2025-12-03T11:01:40+00:00",
    "url": "https://arxiv.org/pdf/2512.03672v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03671v1",
    "title": "Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context",
    "abstract": "The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.",
    "authors": [
      "Beatrice Savoldi",
      "Giuseppe Attanasio",
      "Olga Gorodetskaya",
      "Marta Marchiori Manerba",
      "Elisa Bassignana",
      "Silvia Casola",
      "Matteo Negri",
      "Tommaso Caselli",
      "Luisa Bentivogli",
      "Alan Ramponi",
      "Arianna Muti",
      "Nicoletta Balbo",
      "Debora Nozza"
    ],
    "published": "2025-12-03T11:01:28+00:00",
    "url": "https://arxiv.org/pdf/2512.03671v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03667v1",
    "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
    "abstract": "In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.",
    "authors": [
      "Ge-Peng Ji",
      "Jingyi Liu",
      "Deng-Ping Fan",
      "Nick Barnes"
    ],
    "published": "2025-12-03T10:55:07+00:00",
    "url": "https://arxiv.org/pdf/2512.03667v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03666v1",
    "title": "ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos",
    "abstract": "A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \\textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \\textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \\textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \\textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \\href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..",
    "authors": [
      "Qi'ao Xu",
      "Tianwen Qian",
      "Yuqian Fu",
      "Kailing Li",
      "Yang Jiao",
      "Jiacheng Zhang",
      "Xiaoling Wang",
      "Liang He"
    ],
    "published": "2025-12-03T10:54:44+00:00",
    "url": "https://arxiv.org/pdf/2512.03666v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03663v1",
    "title": "Multi-Scale Visual Prompting for Lightweight Small-Image Classification",
    "abstract": "Visual prompting has recently emerged as an efficient strategy to adapt vision models using lightweight, learnable parameters injected into the input space. However, prior work mainly targets large Vision Transformers and high-resolution datasets such as ImageNet. In contrast, small-image benchmarks like MNIST, Fashion-MNIST, and CIFAR-10 remain widely used in education, prototyping, and research, yet have received little attention in the context of prompting. In this paper, we introduce \\textbf{Multi-Scale Visual Prompting (MSVP)}, a simple and generic module that learns a set of global, mid-scale, and local prompt maps fused with the input image via a lightweight $1 \\times 1$ convolution. MSVP is backbone-agnostic, adds less than $0.02\\%$ parameters, and significantly improves performance across CNN and Vision Transformer backbones.   We provide a unified benchmark on MNIST, Fashion-MNIST, and CIFAR-10 using a simple CNN, ResNet-18, and a small Vision Transformer. Our method yields consistent improvements with negligible computational overhead. We further include ablations on prompt scales, fusion strategies, and backbone architectures, along with qualitative analyzes using prompt visualizations and Grad-CAM. Our results demonstrate that multi-scale prompting provides an effective inductive bias even on low-resolution images.",
    "authors": [
      "Salim Khazem"
    ],
    "published": "2025-12-03T10:51:18+00:00",
    "url": "https://arxiv.org/pdf/2512.03663v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03661v1",
    "title": "Dynamically Scaled Activation Steering",
    "abstract": "Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.",
    "authors": [
      "Alex Ferrando",
      "Xavier Suau",
      "Jordi Gonz\u00e0lez",
      "Pau Rodriguez"
    ],
    "published": "2025-12-03T10:50:15+00:00",
    "url": "https://arxiv.org/pdf/2512.03661v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03656v1",
    "title": "Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting",
    "abstract": "Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.",
    "authors": [
      "Salim Khazem",
      "Houssam Kanso"
    ],
    "published": "2025-12-03T10:46:02+00:00",
    "url": "https://arxiv.org/pdf/2512.03656v1",
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03643v1",
    "title": "Optical Context Compression Is Just (Bad) Autoencoding",
    "abstract": "DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding",
    "authors": [
      "Ivan Yee Lee",
      "Cheng Yang",
      "Taylor Berg-Kirkpatrick"
    ],
    "published": "2025-12-03T10:27:27+00:00",
    "url": "https://arxiv.org/pdf/2512.03643v1",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03640v1",
    "title": "MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms",
    "abstract": "Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.",
    "authors": [
      "Jiahao Zhang",
      "Xiao Zhao",
      "Guangyu Gao"
    ],
    "published": "2025-12-03T10:22:27+00:00",
    "url": "https://arxiv.org/pdf/2512.03640v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03634v1",
    "title": "AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment",
    "abstract": "Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.",
    "authors": [
      "Ahmad Aghaebrahimian"
    ],
    "published": "2025-12-03T10:14:31+00:00",
    "url": "https://arxiv.org/pdf/2512.03634v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03627v1",
    "title": "MemVerse: Multimodal Memory for Lifelong Learning Agents",
    "abstract": "Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.",
    "authors": [
      "Junming Liu",
      "Yifei Sun",
      "Weihua Cheng",
      "Haodong Lei",
      "Yirong Chen",
      "Licheng Wen",
      "Xuemeng Yang",
      "Daocheng Fu",
      "Pinlong Cai",
      "Nianchen Deng",
      "Yi Yu",
      "Shuyue Hu",
      "Botian Shi",
      "Ding Wang"
    ],
    "published": "2025-12-03T10:06:14+00:00",
    "url": "https://arxiv.org/pdf/2512.03627v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03625v1",
    "title": "FeatureLens: A Highly Generalizable and Interpretable Framework for Detecting Adversarial Examples Based on Image Features",
    "abstract": "Although the remarkable performance of deep neural networks (DNNs) in image classification, their vulnerability to adversarial attacks remains a critical challenge. Most existing detection methods rely on complex and poorly interpretable architectures, which compromise interpretability and generalization. To address this, we propose FeatureLens, a lightweight framework that acts as a lens to scrutinize anomalies in image features. Comprising an Image Feature Extractor (IFE) and shallow classifiers (e.g., SVM, MLP, or XGBoost) with model sizes ranging from 1,000 to 30,000 parameters, FeatureLens achieves high detection accuracy ranging from 97.8% to 99.75% in closed-set evaluation and 86.17% to 99.6% in generalization evaluation across FGSM, PGD, CW, and DAmageNet attacks, using only 51 dimensional features. By combining strong detection performance with excellent generalization, interpretability, and computational efficiency, FeatureLens offers a practical pathway toward transparent and effective adversarial defense.",
    "authors": [
      "Zhigang Yang",
      "Yuan Liu",
      "Jiawei Zhang",
      "Puning Zhang",
      "Xinqiang Ma"
    ],
    "published": "2025-12-03T10:02:34+00:00",
    "url": "https://arxiv.org/pdf/2512.03625v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03623v1",
    "title": "The promising potential of vision language models for the generation of textual weather forecasts",
    "abstract": "Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.",
    "authors": [
      "Edward C. C. Steele",
      "Dinesh Mane",
      "Emilio Monti",
      "Luis Orus",
      "Rebecca Chantrill-Cheyette",
      "Matthew Couch",
      "Kirstine I. Dale",
      "Simon Eaton",
      "Govindarajan Rangarajan",
      "Amir Majlesi",
      "Steven Ramsdale",
      "Michael Sharpe",
      "Craig Smith",
      "Jonathan Smith",
      "Rebecca Yates",
      "Holly Ellis",
      "Charles Ewen"
    ],
    "published": "2025-12-03T10:00:15+00:00",
    "url": "https://arxiv.org/pdf/2512.03623v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ]
  },
  {
    "arxiv_id": "2512.03621v1",
    "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
    "abstract": "We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.",
    "authors": [
      "Yaokun Li",
      "Shuaixian Wang",
      "Mantang Guo",
      "Jiehui Huang",
      "Taojun Ding",
      "Mu Hu",
      "Kaixuan Wang",
      "Shaojie Shen",
      "Guang Tan"
    ],
    "published": "2025-12-03T09:55:25+00:00",
    "url": "https://arxiv.org/pdf/2512.03621v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03620v1",
    "title": "SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting",
    "abstract": "The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.",
    "authors": [
      "Hanxiu Zhang",
      "Yue Zheng"
    ],
    "published": "2025-12-03T09:53:47+00:00",
    "url": "https://arxiv.org/pdf/2512.03620v1",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03619v1",
    "title": "LAMP: Language-Assisted Motion Planning for Controllable Video Generation",
    "abstract": "Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications.",
    "authors": [
      "Muhammed Burak Kizil",
      "Enes Sanli",
      "Niloy J. Mitra",
      "Erkut Erdem",
      "Aykut Erdem",
      "Duygu Ceylan"
    ],
    "published": "2025-12-03T09:51:13+00:00",
    "url": "https://arxiv.org/pdf/2512.03619v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03608v1",
    "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing",
    "abstract": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.   We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.",
    "authors": [
      "Lishuo Deng",
      "Shaojie Xu",
      "Jinwu Chen",
      "Changwei Yan",
      "Jiajie Wang",
      "Zhe Jiang",
      "Weiwei Shan"
    ],
    "published": "2025-12-03T09:41:03+00:00",
    "url": "https://arxiv.org/pdf/2512.03608v1",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.ET"
    ]
  },
  {
    "arxiv_id": "2512.03607v1",
    "title": "DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization",
    "abstract": "This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.   Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.",
    "authors": [
      "Yusen Wu",
      "Xiaotie Deng"
    ],
    "published": "2025-12-03T09:40:33+00:00",
    "url": "https://arxiv.org/pdf/2512.03607v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03601v1",
    "title": "Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding",
    "abstract": "Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.",
    "authors": [
      "Haoran Zhou",
      "Gim Hee Lee"
    ],
    "published": "2025-12-03T09:32:56+00:00",
    "url": "https://arxiv.org/pdf/2512.03601v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03598v1",
    "title": "Memory-Guided Point Cloud Completion for Dental Reconstruction",
    "abstract": "Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.",
    "authors": [
      "Jianan Sun",
      "Yukang Huang",
      "Dongzhihan Wang",
      "Mingyu Fan"
    ],
    "published": "2025-12-03T09:31:07+00:00",
    "url": "https://arxiv.org/pdf/2512.03598v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03597v1",
    "title": "HBFormer: A Hybrid-Bridge Transformer for Microtumor and Miniature Organ Segmentation",
    "abstract": "Medical image segmentation is a cornerstone of modern clinical diagnostics. While Vision Transformers that leverage shifted window-based self-attention have established new benchmarks in this field, they are often hampered by a critical limitation: their localized attention mechanism struggles to effectively fuse local details with global context. This deficiency is particularly detrimental to challenging tasks such as the segmentation of microtumors and miniature organs, where both fine-grained boundary definition and broad contextual understanding are paramount. To address this gap, we propose HBFormer, a novel Hybrid-Bridge Transformer architecture. The 'Hybrid' design of HBFormer synergizes a classic U-shaped encoder-decoder framework with a powerful Swin Transformer backbone for robust hierarchical feature extraction. The core innovation lies in its 'Bridge' mechanism, a sophisticated nexus for multi-scale feature integration. This bridge is architecturally embodied by our novel Multi-Scale Feature Fusion (MFF) decoder. Departing from conventional symmetric designs, the MFF decoder is engineered to fuse multi-scale features from the encoder with global contextual information. It achieves this through a synergistic combination of channel and spatial attention modules, which are constructed from a series of dilated and depth-wise convolutions. These components work in concert to create a powerful feature bridge that explicitly captures long-range dependencies and refines object boundaries with exceptional precision. Comprehensive experiments on challenging medical image segmentation datasets, including multi-organ, liver tumor, and bladder tumor benchmarks, demonstrate that HBFormer achieves state-of-the-art results, showcasing its outstanding capabilities in microtumor and miniature organ segmentation. Code and models are available at: https://github.com/lzeeorno/HBFormer.",
    "authors": [
      "Fuchen Zheng",
      "Xinyi Chen",
      "Weixuan Li",
      "Quanjun Li",
      "Junhua Zhou",
      "Xiaojiao Guo",
      "Xuhang Chen",
      "Chi-Man Pun",
      "Shoujun Zhou"
    ],
    "published": "2025-12-03T09:30:39+00:00",
    "url": "https://arxiv.org/pdf/2512.03597v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03593v1",
    "title": "CloseUpAvatar: High-Fidelity Animatable Full-Body Avatars with Mixture of Multi-Scale Textures",
    "abstract": "We present a CloseUpAvatar - a novel approach for articulated human avatar representation dealing with more general camera motions, while preserving rendering quality for close-up views. CloseUpAvatar represents an avatar as a set of textured planes with two sets of learnable textures for low and high-frequency detail. The method automatically switches to high-frequency textures only for cameras positioned close to the avatar's surface and gradually reduces their impact as the camera moves farther away. Such parametrization of the avatar enables CloseUpAvatar to adjust rendering quality based on camera distance ensuring realistic rendering across a wider range of camera orientations than previous approaches. We provide experiments using the ActorsHQ dataset with high-resolution input images. CloseUpAvatar demonstrates both qualitative and quantitative improvements over existing methods in rendering from novel wide range camera positions, while maintaining high FPS by limiting the number of required primitives.",
    "authors": [
      "David Svitov",
      "Pietro Morerio",
      "Lourdes Agapito",
      "Alessio Del Bue"
    ],
    "published": "2025-12-03T09:25:01+00:00",
    "url": "https://arxiv.org/pdf/2512.03593v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03592v1",
    "title": "Harnessing Hypergraphs in Geometric Deep Learning for 3D RNA Inverse Folding",
    "abstract": "The RNA inverse folding problem, a key challenge in RNA design, involves identifying nucleotide sequences that can fold into desired secondary structures, which are critical for ensuring molecular stability and function. The inherent complexity of this task stems from the intricate relationship between sequence and structure, making it particularly challenging. In this paper, we propose a framework, named HyperRNA, a generative model with an encoder-decoder architecture that leverages hypergraphs to design RNA sequences. Specifically, our HyperRNA model consists of three main components: preprocessing, encoding and decoding.   In the preprocessing stage, graph structures are constructed by extracting the atom coordinates of RNA backbone based on 3-bead coarse-grained representation. The encoding stage processes these graphs, capturing higher order dependencies and complex biomolecular interactions using an attention embedding module and a hypergraph-based encoder. Finally, the decoding stage generates the RNA sequence in an autoregressive manner. We conducted quantitative and qualitative experiments on the PDBBind and RNAsolo datasets to evaluate the inverse folding task for RNA sequence generation and RNA-protein complex sequence generation. The experimental results demonstrate that HyperRNA not only outperforms existing RNA design methods but also highlights the potential of leveraging hypergraphs in RNA engineering.",
    "authors": [
      "Guang Yang",
      "Lei Fan"
    ],
    "published": "2025-12-03T09:23:59+00:00",
    "url": "https://arxiv.org/pdf/2512.03592v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03590v1",
    "title": "Beyond Boundary Frames: Audio-Visual Semantic Guidance for Context-Aware Video Interpolation",
    "abstract": "Handling fast, complex, and highly non-linear motion patterns has long posed challenges for video frame interpolation. Although recent diffusion-based approaches improve upon traditional optical-flow-based methods, they still struggle to cover diverse application scenarios and often fail to produce sharp, temporally consistent frames in fine-grained motion tasks such as audio-visual synchronized interpolation. To address these limitations, we introduce BBF (Beyond Boundary Frames), a context-aware video frame interpolation framework, which could be guided by audio/visual semantics. First, we enhance the input design of the interpolation model so that it can flexibly handle multiple conditional modalities, including text, audio, images, and video. Second, we propose a decoupled multimodal fusion mechanism that sequentially injects different conditional signals into a DiT backbone. Finally, to maintain the generation abilities of the foundation model, we adopt a progressive multi-stage training paradigm, where the start-end frame difference embedding is used to dynamically adjust both the data sampling and the loss weighting. Extensive experimental results demonstrate that BBF outperforms specialized state-of-the-art methods on both generic interpolation and audio-visual synchronized interpolation tasks, establishing a unified framework for video frame interpolation under coordinated multi-channel conditioning.",
    "authors": [
      "Yuchen Deng",
      "Xiuyang Wu",
      "Hai-Tao Zheng",
      "Jie Wang",
      "Feidiao Yang",
      "Yuxing Han"
    ],
    "published": "2025-12-03T09:22:13+00:00",
    "url": "https://arxiv.org/pdf/2512.03590v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03582v1",
    "title": "Fine-grained Narrative Classification in Biased News Articles",
    "abstract": "Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.",
    "authors": [
      "Zeba Afroz",
      "Harsh Vardhan",
      "Pawan Bhakuni",
      "Aanchal Punia",
      "Rajdeep Kumar",
      "Md. Shad Akhtar"
    ],
    "published": "2025-12-03T09:07:52+00:00",
    "url": "https://arxiv.org/pdf/2512.03582v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03580v1",
    "title": "Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes",
    "abstract": "We propose the Dynamic Optical Test for Bot Identification (DOT-BI): a quick and easy method that uses human perception of motion to differentiate between human respondents and automated systems in surveys and online processes. In DOT-BI, a 'hidden' number is displayed with the same random black-and-white pixel texture as its background. Only the difference in motion and scale between the number and the background makes the number perceptible to humans across frames, while frame-by-frame algorithmic processing yields no meaningful signal. We conducted two preliminary assessments. Firstly, state-of-the-art, video-capable, multimodal models (GPT-5-Thinking and Gemini 2.5 Pro) fail to extract the correct value, even when given explicit instructions about the mechanism. Secondly, in an online survey (n=182), 99.5% (181/182) of participants solved the task, with an average end-to-end completion time of 10.7 seconds; a supervised lab study (n=39) found no negative effects on perceived ease-of-use or completion time relative to a control. We release code to generate tests and 100+ pre-rendered variants to facilitate adoption in surveys and online processes.",
    "authors": [
      "Malte Bleeker",
      "Mauro Gotsch"
    ],
    "published": "2025-12-03T09:03:35+00:00",
    "url": "https://arxiv.org/pdf/2512.03580v1",
    "categories": [
      "cs.CV",
      "cs.CR"
    ]
  },
  {
    "arxiv_id": "2512.03578v1",
    "title": "When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate",
    "abstract": "Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.   To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.",
    "authors": [
      "Florent Forest",
      "Amaury Wei",
      "Olga Fink"
    ],
    "published": "2025-12-03T09:01:41+00:00",
    "url": "https://arxiv.org/pdf/2512.03578v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03577v1",
    "title": "Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning",
    "abstract": "Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&E enriches H&E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&E, HER2, KI67, ER, PGR) to enable paired H&E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.",
    "authors": [
      "Yizhi Zhang",
      "Lei Fan",
      "Zhulin Tao",
      "Donglin Di",
      "Yang Song",
      "Sidong Liu",
      "Cong Cong"
    ],
    "published": "2025-12-03T09:00:27+00:00",
    "url": "https://arxiv.org/pdf/2512.03577v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03575v1",
    "title": "UniComp: Rethinking Video Compression Through Informational Uniqueness",
    "abstract": "Distinct from attention-based compression methods, this paper presents an information uniqueness driven video compression framework, termed UniComp, which aims to maximize the information fidelity of video representations under constrained computational budgets. Starting from the information-theoretic perspective, we formulate the vision compression as an optimization problem that minimizes conditional entropy (reconstruction error) between retained and full tokens. To achieve this, we introduce the notion of information uniqueness to measure intrinsic redundancy among tokens to link with reconstruction error. Based on uniqueness, we design three modules-Frame Group Fusion, Token Allocation, and Spatial Dynamic Compression-that progressively perform semantic frame grouping, adaptive resource allocation, and fine-grained spatial compression. Extensive experiments demonstrate that UniComp consistently outperforms existing compression methods in preserving essential visual tokens under limited computational budgets, highlighting the pivotal role of information uniqueness in token compression efficacy.",
    "authors": [
      "Chao Yuan",
      "Shimin Chen",
      "Minliang Lin",
      "Limeng Qiao",
      "Guanglu Wan",
      "Lin Ma"
    ],
    "published": "2025-12-03T08:56:23+00:00",
    "url": "https://arxiv.org/pdf/2512.03575v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03574v1",
    "title": "Global-Local Aware Scene Text Editing",
    "abstract": "Scene Text Editing (STE) involves replacing text in a scene image with new target text while preserving both the original text style and background texture. Existing methods suffer from two major challenges: inconsistency and length-insensitivity. They often fail to maintain coherence between the edited local patch and the surrounding area, and they struggle to handle significant differences in text length before and after editing. To tackle these challenges, we propose an end-to-end framework called Global-Local Aware Scene Text Editing (GLASTE), which simultaneously incorporates high-level global contextual information along with delicate local features. Specifically, we design a global-local combination structure, joint global and local losses, and enhance text image features to ensure consistency in text style within local patches while maintaining harmony between local and global areas. Additionally, we express the text style as a vector independent of the image size, which can be transferred to target text images of various sizes. We use an affine fusion to fill target text images into the editing patch while maintaining their aspect ratio unchanged. Extensive experiments on real-world datasets validate that our GLASTE model outperforms previous methods in both quantitative metrics and qualitative results and effectively mitigates the two challenges.",
    "authors": [
      "Fuxiang Yang",
      "Tonghua Su",
      "Donglin Di",
      "Yin Chen",
      "Xiangqian Wu",
      "Zhongjie Wang",
      "Lei Fan"
    ],
    "published": "2025-12-03T08:56:01+00:00",
    "url": "https://arxiv.org/pdf/2512.03574v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03571v1",
    "title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths",
    "abstract": "We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce \"probabilistic angelic nondeterminism\" (\"PAN\"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.",
    "authors": [
      "Zhening Li",
      "Armando Solar-Lezama",
      "Yisong Yue",
      "Stephan Zheng"
    ],
    "published": "2025-12-03T08:50:16+00:00",
    "url": "https://arxiv.org/pdf/2512.03571v1",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ]
  },
  {
    "arxiv_id": "2512.03570v1",
    "title": "Machine Learning to Predict Slot Usage in TSCH Wireless Sensor Networks",
    "abstract": "Wireless sensor networks (WSNs) are employed across a wide range of industrial applications where ultra-low power consumption is a critical prerequisite. At the same time, these systems must maintain a certain level of determinism to ensure reliable and predictable operation. In this view, time slotted channel hopping (TSCH) is a communication technology that meets both conditions, making it an attractive option for its usage in industrial WSNs. This work proposes the use of machine learning to learn the traffic pattern generated in networks based on the TSCH protocol, in order to turn nodes into a deep sleep state when no transmission is planned and thus to improve the energy efficiency of the WSN. The ability of machine learning models to make good predictions at different network levels in a typical tree network topology was analyzed in depth, showing how their capabilities degrade while approaching the root of the tree. The application of these models on simulated data based on an accurate modeling of wireless sensor nodes indicates that the investigated algorithms can be suitably used to further and substantially reduce the power consumption of a TSCH network.",
    "authors": [
      "Stefano Scanzio",
      "Gabriele Formis",
      "Tullio Facchinetti",
      "Gianluca Cena"
    ],
    "published": "2025-12-03T08:50:02+00:00",
    "url": "https://arxiv.org/pdf/2512.03570v1",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03566v1",
    "title": "GAOT: Generating Articulated Objects Through Text-Guided Diffusion Models",
    "abstract": "Articulated object generation has seen increasing advancements, yet existing models often lack the ability to be conditioned on text prompts. To address the significant gap between textual descriptions and 3D articulated object representations, we propose GAOT, a three-phase framework that generates articulated objects from text prompts, leveraging diffusion models and hypergraph learning in a three-step process. First, we fine-tune a point cloud generation model to produce a coarse representation of objects from text prompts. Given the inherent connection between articulated objects and graph structures, we design a hypergraph-based learning method to refine these coarse representations, representing object parts as graph vertices. Finally, leveraging a diffusion model, the joints of articulated objects-represented as graph edges-are generated based on the object parts. Extensive qualitative and quantitative experiments on the PartNet-Mobility dataset demonstrate the effectiveness of our approach, achieving superior performance over previous methods.",
    "authors": [
      "Hao Sun",
      "Lei Fan",
      "Donglin Di",
      "Shaohui Liu"
    ],
    "published": "2025-12-03T08:44:17+00:00",
    "url": "https://arxiv.org/pdf/2512.03566v1",
    "categories": [
      "cs.CV",
      "cs.MM"
    ]
  },
  {
    "arxiv_id": "2512.03563v1",
    "title": "State Space Models for Bioacoustics: A comparative Evaluation with Transformers",
    "abstract": "In this study, we evaluate the efficacy of the Mamba model in the field of bioacoustics. We first pretrain a Mamba-based audio large language model (LLM) on a large corpus of audio data using self-supervised learning. We fine-tune and evaluate BioMamba on the BEANS benchmark, a collection of diverse bioacoustic tasks including classification and detection, and compare its performance and efficiency with multiple baseline models, including AVES, a state-of-the-art Transformer-based model. The results show that BioMamba achieves comparable performance with AVES while consumption significantly less VRAM, demonstrating its potential in this domain.",
    "authors": [
      "Chengyu Tang",
      "Sanjeev Baskiyar"
    ],
    "published": "2025-12-03T08:37:09+00:00",
    "url": "https://arxiv.org/pdf/2512.03563v1",
    "categories": [
      "cs.SD",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03560v1",
    "title": "Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks",
    "abstract": "Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.",
    "authors": [
      "Gianni Molinari",
      "Fabio Ciravegna"
    ],
    "published": "2025-12-03T08:28:40+00:00",
    "url": "https://arxiv.org/pdf/2512.03560v1",
    "categories": [
      "cs.AI",
      "cs.MA"
    ]
  },
  {
    "arxiv_id": "2512.03558v1",
    "title": "CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding",
    "abstract": "The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git",
    "authors": [
      "Huy Quang Ung",
      "Guillaume Habault",
      "Yasutaka Nishimura",
      "Hao Niu",
      "Roberto Legaspi",
      "Tomoki Oya",
      "Ryoichi Kojima",
      "Masato Taya",
      "Chihiro Ono",
      "Atsunori Minamikawa",
      "Yan Liu"
    ],
    "published": "2025-12-03T08:25:22+00:00",
    "url": "https://arxiv.org/pdf/2512.03558v1",
    "categories": [
      "cs.CV",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03556v1",
    "title": "RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL",
    "abstract": "Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.",
    "authors": [
      "Yinzhou Tang",
      "Yu Shang",
      "Yinuo Chen",
      "Bingwen Wei",
      "Xin Zhang",
      "Shu'ang Yu",
      "Liangzhi Shi",
      "Chao Yu",
      "Chen Gao",
      "Wei Wu",
      "Yong Li"
    ],
    "published": "2025-12-03T08:24:16+00:00",
    "url": "https://arxiv.org/pdf/2512.03556v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03553v1",
    "title": "Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching",
    "abstract": "Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.",
    "authors": [
      "Wei Chee Yew",
      "Hailun Xu",
      "Sanjay Saha",
      "Xiaotian Fan",
      "Hiok Hian Ong",
      "David Yuchen Wang",
      "Kanchan Sarkar",
      "Zhenheng Yang",
      "Danhui Guan"
    ],
    "published": "2025-12-03T08:20:58+00:00",
    "url": "https://arxiv.org/pdf/2512.03553v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03549v1",
    "title": "PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks",
    "abstract": "We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.",
    "authors": [
      "Yuki Orimo",
      "Iori Kurata",
      "Hodaka Mori",
      "Ryuhei Okuno",
      "Ryohto Sawada",
      "Daisuke Okanohara"
    ],
    "published": "2025-12-03T08:15:10+00:00",
    "url": "https://arxiv.org/pdf/2512.03549v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03548v1",
    "title": "A Learning-based Control Methodology for Transitioning VTOL UAVs",
    "abstract": "Transition control poses a critical challenge in Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL UAV) development due to the tilting rotor mechanism, which shifts the center of gravity and thrust direction during transitions. Current control methods' decoupled control of altitude and position leads to significant vibration, and limits interaction consideration and adaptability. In this study, we propose a novel coupled transition control methodology based on reinforcement learning (RL) driven controller. Besides, contrasting to the conventional phase-transition approach, the ST3M method demonstrates a new perspective by treating cruise mode as a special case of hover. We validate the feasibility of applying our method in simulation and real-world environments, demonstrating efficient controller development and migration while accurately controlling UAV position and attitude, exhibiting outstanding trajectory tracking and reduced vibrations during the transition process.",
    "authors": [
      "Zexin Lin",
      "Yebin Zhong",
      "Hanwen Wan",
      "Jiu Cheng",
      "Zhenglong Sun",
      "Xiaoqiang Ji"
    ],
    "published": "2025-12-03T08:13:50+00:00",
    "url": "https://arxiv.org/pdf/2512.03548v1",
    "categories": [
      "cs.RO",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03542v1",
    "title": "V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention",
    "abstract": "Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on \"how to intervene\" but overlooking the prerequisite \"when to intervene\", which leads to the \"over-intervention\" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.",
    "authors": [
      "Nan Sun",
      "Zhenyu Zhang",
      "Xixun Lin",
      "Kun Wang",
      "Yanmin Shang",
      "Naibin Gu",
      "Shuohuan Wang",
      "Yu Sun",
      "Hua Wu",
      "Haifeng Wang",
      "Yanan Cao"
    ],
    "published": "2025-12-03T08:03:54+00:00",
    "url": "https://arxiv.org/pdf/2512.03542v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03540v1",
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "abstract": "Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.",
    "authors": [
      "Ruoxuan Zhang",
      "Bin Wen",
      "Hongxia Xie",
      "Yi Yao",
      "Songhan Zuo",
      "Jian-Yu Jiang-Lin",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ],
    "published": "2025-12-03T08:01:48+00:00",
    "url": "https://arxiv.org/pdf/2512.03540v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03534v1",
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "abstract": "Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.",
    "authors": [
      "Subin Kim",
      "Sangwoo Mo",
      "Mamshad Nayeem Rizve",
      "Yiran Xu",
      "Difan Liu",
      "Jinwoo Shin",
      "Tobias Hinz"
    ],
    "published": "2025-12-03T07:54:05+00:00",
    "url": "https://arxiv.org/pdf/2512.03534v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03532v1",
    "title": "OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation",
    "abstract": "Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.",
    "authors": [
      "Zhishan Zhou",
      "Siyuan Wei",
      "Zengran Wang",
      "Chunjie Wang",
      "Xiaosheng Yan",
      "Xiao Liu"
    ],
    "published": "2025-12-03T07:51:03+00:00",
    "url": "https://arxiv.org/pdf/2512.03532v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03528v1",
    "title": "Multi-Agent Reinforcement Learning with Communication-Constrained Priors",
    "abstract": "Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.",
    "authors": [
      "Guang Yang",
      "Tianpei Yang",
      "Jingwen Qiao",
      "Yanqing Wu",
      "Jing Huo",
      "Xingguo Chen",
      "Yang Gao"
    ],
    "published": "2025-12-03T07:35:07+00:00",
    "url": "https://arxiv.org/pdf/2512.03528v1",
    "categories": [
      "cs.AI",
      "cs.MA"
    ]
  },
  {
    "arxiv_id": "2512.03522v1",
    "title": "MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization",
    "abstract": "Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.",
    "authors": [
      "Gihyeon Lee",
      "Jungwoo Lee",
      "Juwon Kim",
      "Young-Sik Shin",
      "Younggun Cho"
    ],
    "published": "2025-12-03T07:28:01+00:00",
    "url": "https://arxiv.org/pdf/2512.03522v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03520v1",
    "title": "FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation",
    "abstract": "We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/",
    "authors": [
      "Yiyi Cai",
      "Yuhan Wu",
      "Kunhang Li",
      "You Zhou",
      "Bo Zheng",
      "Haiyang Liu"
    ],
    "published": "2025-12-03T07:23:47+00:00",
    "url": "https://arxiv.org/pdf/2512.03520v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03514v1",
    "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
    "abstract": "Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.",
    "authors": [
      "Adithya S Kolavi",
      "Vyoman Jain"
    ],
    "published": "2025-12-03T07:17:59+00:00",
    "url": "https://arxiv.org/pdf/2512.03514v1",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03512v1",
    "title": "Physics-Driven Learning Framework for Tomographic Tactile Sensing",
    "abstract": "Electrical impedance tomography (EIT) provides an attractive solution for large-area tactile sensing due to its minimal wiring and shape flexibility, but its nonlinear inverse problem often leads to severe artifacts and inaccurate contact reconstruction. This work presents PhyDNN, a physics-driven deep reconstruction framework that embeds the EIT forward model directly into the learning objective. By jointly minimizing the discrepancy between predicted and ground-truth conductivity maps and enforcing consistency with the forward PDE, PhyDNN reduces the black-box nature of deep networks and improves both physical plausibility and generalization. To enable efficient backpropagation, we design a differentiable forward-operator network that accurately approximates the nonlinear EIT response, allowing fast physics-guided training. Extensive simulations and real tactile experiments on a 16-electrode soft sensor show that PhyDNN consistently outperforms NOSER, TV, and standard DNNs in reconstructing contact shape, location, and pressure distribution. PhyDNN yields fewer artifacts, sharper boundaries, and higher metric scores, demonstrating its effectiveness for high-quality tomographic tactile sensing.",
    "authors": [
      "Xuanxuan Yang",
      "Xiuyang Zhang",
      "Haofeng Chen",
      "Gang Ma",
      "Xiaojie Wang"
    ],
    "published": "2025-12-03T07:11:01+00:00",
    "url": "https://arxiv.org/pdf/2512.03512v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03510v1",
    "title": "CSMapping: Scalable Crowdsourced Semantic Mapping and Topology Inference for Autonomous Driving",
    "abstract": "Crowdsourcing enables scalable autonomous driving map construction, but low-cost sensor noise hinders quality from improving with data volume. We propose CSMapping, a system that produces accurate semantic maps and topological road centerlines whose quality consistently increases with more crowdsourced data. For semantic mapping, we train a latent diffusion model on HD maps (optionally conditioned on SD maps) to learn a generative prior of real-world map structure, without requiring paired crowdsourced/HD-map supervision. This prior is incorporated via constrained MAP optimization in latent space, ensuring robustness to severe noise and plausible completion in unobserved areas. Initialization uses a robust vectorized mapping module followed by diffusion inversion; optimization employs efficient Gaussian-basis reparameterization, projected gradient descent zobracket multi-start, and latent-space factor-graph for global consistency. For topological mapping, we apply confidence-weighted k-medoids clustering and kinematic refinement to trajectories, yielding smooth, human-like centerlines robust to trajectory variation. Experiments on nuScenes, Argoverse 2, and a large proprietary dataset achieve state-of-the-art semantic and topological mapping performance, with thorough ablation and scalability studies.",
    "authors": [
      "Zhijian Qiao",
      "Zehuan Yu",
      "Tong Li",
      "Chih-Chung Chou",
      "Wenchao Ding",
      "Shaojie Shen"
    ],
    "published": "2025-12-03T07:06:18+00:00",
    "url": "https://arxiv.org/pdf/2512.03510v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.03509v1",
    "title": "AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model",
    "abstract": "This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.",
    "authors": [
      "Kwaku Opoku-Ware",
      "Gideon Opoku"
    ],
    "published": "2025-12-03T07:06:06+00:00",
    "url": "https://arxiv.org/pdf/2512.03509v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03508v1",
    "title": "Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation",
    "abstract": "Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.",
    "authors": [
      "Seogkyu Jeon",
      "Kibeom Hong",
      "Hyeran Byun"
    ],
    "published": "2025-12-03T06:58:38+00:00",
    "url": "https://arxiv.org/pdf/2512.03508v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03503v1",
    "title": "Understanding LLM Reasoning for Abstractive Summarization",
    "abstract": "While the reasoning capabilities of Large Language Models (LLMs) excel in analytical tasks such as mathematics and code generation, their utility for abstractive summarization remains widely assumed but largely unverified. To bridge this gap, we first tailor general reasoning strategies to the summarization domain. We then conduct a systematic, large scale comparative study of 8 reasoning strategies and 3 Large Reasoning Models (LRMs) across 8 diverse datasets, assessing both summary quality and faithfulness. Our findings show that reasoning is not a universal solution and its effectiveness is highly dependent on the specific strategy and context. Specifically, we observe a trade-off between summary quality and factual faithfulness: explicit reasoning strategies tend to improve fluency at the expense of factual grounding, while implicit reasoning in LRMs exhibits the inverse pattern. Furthermore, increasing an LRM's internal reasoning budget does not improve, and can even hurt, factual consistency, suggesting that effective summarization demands faithful compression rather than creative over-thinking.",
    "authors": [
      "Haohan Yuan",
      "Siu Cheung Hui",
      "Haopeng Zhang"
    ],
    "published": "2025-12-03T06:52:44+00:00",
    "url": "https://arxiv.org/pdf/2512.03503v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03500v1",
    "title": "EEA: Exploration-Exploitation Agent for Long Video Understanding",
    "abstract": "Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.",
    "authors": [
      "Te Yang",
      "Xiangyu Zhu",
      "Bo Wang",
      "Quan Chen",
      "Peng Jiang",
      "Zhen Lei"
    ],
    "published": "2025-12-03T06:48:36+00:00",
    "url": "https://arxiv.org/pdf/2512.03500v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03499v1",
    "title": "NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation",
    "abstract": "The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.",
    "authors": [
      "Renqi Chen",
      "Haoyang Su",
      "Shixiang Tang"
    ],
    "published": "2025-12-03T06:47:56+00:00",
    "url": "https://arxiv.org/pdf/2512.03499v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03497v1",
    "title": "Cell-cell communication inference and analysis: biological mechanisms, computational approaches, and future opportunities",
    "abstract": "In multicellular organisms, cells coordinate their activities through cell-cell communication (CCC), which are crucial for development, tissue homeostasis, and disease progression. Recent advances in single-cell and spatial omics technologies provide unprecedented opportunities to systematically infer and analyze CCC from these omics data, either by integrating prior knowledge of ligand-receptor interactions (LRIs) or through de novo approaches. A variety of computational methods have been developed, focusing on methodological innovations, accurate modeling of complex signaling mechanisms, and investigation of broader biological questions. These advances have greatly enhanced our ability to analyze CCC and generate biological hypotheses. Here, we introduce the biological mechanisms and modeling strategies of CCC, and provide a focused overview of more than 140 computational methods for inferring CCC from single-cell and spatial transcriptomic data, emphasizing the diversity in methodological frameworks and biological questions. Finally, we discuss the current challenges and future opportunities in this rapidly evolving field.",
    "authors": [
      "Xiangzheng Cheng",
      "Haili Huang",
      "Ye Su",
      "Qing Nie",
      "Xiufen Zou",
      "Suoqin Jin"
    ],
    "published": "2025-12-03T06:45:35+00:00",
    "url": "https://arxiv.org/pdf/2512.03497v1",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "q-bio.CB"
    ]
  },
  {
    "arxiv_id": "2512.03494v1",
    "title": "A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention",
    "abstract": "Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.",
    "authors": [
      "Di Xiu",
      "Hongyin Tang",
      "Bolin Rong",
      "Lizhi Yan",
      "Jingang Wang",
      "Yifan Lu",
      "Xunliang Cai"
    ],
    "published": "2025-12-03T06:44:02+00:00",
    "url": "https://arxiv.org/pdf/2512.03494v1",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03479v1",
    "title": "Towards Object-centric Understanding for Instructional Videos",
    "abstract": "Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks. Existing action-centric methods struggle with the flexibility of real procedures, where step order varies depending on object states. In this work, we propose to shift the focus to an object-centric paradigm by regarding actions as mechanisms that drive state transitions. To advance this direction, we introduce Object-IVQA, a long-form instructional video benchmark with 107 videos and 514 open-ended question-answer pairs annotated with temporally grounded evidence. The benchmark evaluates four dimensions of object-centric reasoning, including state evolution, precondition verification, counterfactual reasoning and mistake recognition. We further propose an agent framework that orchestrates object-centric planning, perception, analysis and generation tools, enabling explicit evidence retrieval and multi-hop reasoning across disjoint segments. Experiments show that existing large vision-language models struggle in object-level recognition and reasoning, whereas our framework achieves substantially improvement.",
    "authors": [
      "Wenliang Guo",
      "Yu Kong"
    ],
    "published": "2025-12-03T06:14:26+00:00",
    "url": "https://arxiv.org/pdf/2512.03479v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03477v1",
    "title": "Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis",
    "abstract": "Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.",
    "authors": [
      "Zijian Gu",
      "Yuxi Liu",
      "Zhenhao Zhang",
      "Song Wang"
    ],
    "published": "2025-12-03T06:09:14+00:00",
    "url": "https://arxiv.org/pdf/2512.03477v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03476v1",
    "title": "ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms",
    "abstract": "Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' ($A_n$) from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code ($S_n$) to generate scientific rewards ($R_n$). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of $10^{-14}$. Furthermore, collaborative ``human-in-the-loop\" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.",
    "authors": [
      "Juan Diego Toscano",
      "Daniel T. Chen",
      "George Em Karniadakis"
    ],
    "published": "2025-12-03T06:05:27+00:00",
    "url": "https://arxiv.org/pdf/2512.03476v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "math.NA",
      "physics.comp-ph"
    ]
  },
  {
    "arxiv_id": "2512.03474v1",
    "title": "Procedural Mistake Detection via Action Effect Modeling",
    "abstract": "Mistake detection in procedural tasks is essential for building intelligent systems that support learning and task execution. Existing approaches primarily analyze how an action is performed, while overlooking what it produces, i.e., the \\textbf{action effect}. Yet many errors manifest not in the execution itself but in the resulting outcome, such as an unintended object state or incorrect spatial arrangement. To address this gap, we propose Action Effect Modeling (AEM), a unified framework that jointly captures action execution and its outcomes through a probabilistic formulation. AEM first identifies the outcome of an action by selecting the most informative effect frame based on semantic relevance and visual quality. It then extracts complementary cues from visual grounding and symbolic scene graphs, aligning them in a shared latent space to form robust effect-aware representations. To detect mistakes, we further design a prompt-based detector that incorporates task-specific prompts and aligns each action segment with its intended execution semantics. Our approach achieves state-of-the-art performance on the EgoPER and CaptainCook4D benchmarks under the challenging one-class classification (OCC) setting. These results demonstrate that modeling both execution and outcome yields more reliable mistake detection, and highlight the potential of effect-aware representations to benefit a broader range of downstream applications.",
    "authors": [
      "Wenliang Guo",
      "Yujiang Pu",
      "Yu Kong"
    ],
    "published": "2025-12-03T05:56:17+00:00",
    "url": "https://arxiv.org/pdf/2512.03474v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03470v1",
    "title": "Difference Decomposition Networks for Infrared Small Target Detection",
    "abstract": "Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\\mathrm{2}$Net integrates SD$^\\mathrm{2}$M and SD$^\\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\\mathrm{2}$M to introduce motion information, which transforms SD$^\\mathrm{2}$Net into STD$^\\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\\mathrm{2}$Net achieves a mIoU of 87.68\\%, outperforming SD$^\\mathrm{2}$Net, which achieves a mIoU of 64.97\\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.",
    "authors": [
      "Chen Hu",
      "Mingyu Zhou",
      "Shuai Yuan",
      "Hongbo Hu",
      "Xiangyu Qiu",
      "Junhai Luo",
      "Tian Pu",
      "Xiyin Li"
    ],
    "published": "2025-12-03T05:52:06+00:00",
    "url": "https://arxiv.org/pdf/2512.03470v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03466v1",
    "title": "AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation",
    "abstract": "Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.",
    "authors": [
      "Xavier Cadet",
      "Edward Koh",
      "Peter Chin"
    ],
    "published": "2025-12-03T05:42:01+00:00",
    "url": "https://arxiv.org/pdf/2512.03466v1",
    "categories": [
      "cs.MA",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03465v1",
    "title": "Tuning for TraceTarnish: Techniques, Trends, and Testing Tangible Traits",
    "abstract": "In this study, we more rigorously evaluated our attack script $\\textit{TraceTarnish}$, which leverages adversarial stylometry principles to anonymize the authorship of text-based messages. To ensure the efficacy and utility of our attack, we sourced, processed, and analyzed Reddit comments--comments that were later alchemized into $\\textit{TraceTarnish}$ data--to gain valuable insights. The transformed $\\textit{TraceTarnish}$ data was then further augmented by $\\textit{StyloMetrix}$ to manufacture stylometric features--features that were culled using the Information Gain criterion, leaving only the most informative, predictive, and discriminative ones. Our results found that function words and function word types ($L\\_FUNC\\_A$ $\\&$ $L\\_FUNC\\_T$); content words and content word types ($L\\_CONT\\_A$ $\\&$ $L\\_CONT\\_T$); and the Type-Token Ratio ($ST\\_TYPE\\_TOKEN\\_RATIO\\_LEMMAS$) yielded significant Information-Gain readings. The identified stylometric cues--function-word frequencies, content-word distributions, and the Type-Token Ratio--serve as reliable indicators of compromise (IoCs), revealing when a text has been deliberately altered to mask its true author. Similarly, these features could function as forensic beacons, alerting defenders to the presence of an adversarial stylometry attack; granted, in the absence of the original message, this signal may go largely unnoticed, as it appears to depend on a pre- and post-transformation comparison. \"In trying to erase a trace, you often imprint a larger one.\" Armed with this understanding, we framed $\\textit{TraceTarnish}$'s operations and outputs around these five isolated features, using them to conceptualize and implement enhancements that further strengthen the attack.",
    "authors": [
      "Robert Dilworth"
    ],
    "published": "2025-12-03T05:39:40+00:00",
    "url": "https://arxiv.org/pdf/2512.03465v1",
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "arxiv_id": "2512.03463v1",
    "title": "Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models",
    "abstract": "Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.",
    "authors": [
      "Shojiro Yamabe",
      "Futa Waseda",
      "Daiki Shiono",
      "Tsubasa Takahashi"
    ],
    "published": "2025-12-03T05:36:46+00:00",
    "url": "https://arxiv.org/pdf/2512.03463v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03460v1",
    "title": "Learning From Limited Data and Feedback for Cell Culture Process Monitoring: A Comparative Study",
    "abstract": "In cell culture bioprocessing, real-time batch process monitoring (BPM) refers to the continuous tracking and analysis of key process variables such as viable cell density, nutrient levels, metabolite concentrations, and product titer throughout the duration of a batch run. This enables early detection of deviations and supports timely control actions to ensure optimal cell growth and product quality. BPM plays a critical role in ensuring the quality and regulatory compliance of biopharmaceutical manufacturing processes. However, the development of accurate soft sensors for BPM is hindered by key challenges, including limited historical data, infrequent feedback, heterogeneous process conditions, and high-dimensional sensory inputs. This study presents a comprehensive benchmarking analysis of machine learning (ML) methods designed to address these challenges, with a focus on learning from historical data with limited volume and relevance in the context of bioprocess monitoring. We evaluate multiple ML approaches including feature dimensionality reduction, online learning, and just-in-time learning across three datasets, one in silico dataset and two real-world experimental datasets. Our findings highlight the importance of training strategies in handling limited data and feedback, with batch learning proving effective in homogeneous settings, while just-in-time learning and online learning demonstrate superior adaptability in cold-start scenarios. Additionally, we identify key meta-features, such as feed media composition and process control strategies, that significantly impact model transferability. The results also suggest that integrating Raman-based predictions with lagged offline measurements enhances monitoring accuracy, offering a promising direction for future bioprocess soft sensor development.",
    "authors": [
      "Johnny Peng",
      "Thanh Tung Khuat",
      "Ellen Otte",
      "Katarzyna Musial",
      "Bogdan Gabrys"
    ],
    "published": "2025-12-03T05:28:33+00:00",
    "url": "https://arxiv.org/pdf/2512.03460v1",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03454v1",
    "title": "Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles",
    "abstract": "Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.",
    "authors": [
      "Haicheng Liao",
      "Huanming Shen",
      "Bonan Wang",
      "Yongkang Li",
      "Yihong Tang",
      "Chengyue Wang",
      "Dingyi Zhuang",
      "Kehua Chen",
      "Hai Yang",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "published": "2025-12-03T05:14:16+00:00",
    "url": "https://arxiv.org/pdf/2512.03454v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03453v1",
    "title": "GeoVideo: Introducing Geometric Regularization into Video Generation Model",
    "abstract": "Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.",
    "authors": [
      "Yunpeng Bai",
      "Shaoheng Fang",
      "Chaohui Yu",
      "Fan Wang",
      "Qixing Huang"
    ],
    "published": "2025-12-03T05:11:57+00:00",
    "url": "https://arxiv.org/pdf/2512.03453v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03451v1",
    "title": "GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers",
    "abstract": "Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.   We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\\times$ and $2.37\\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).",
    "authors": [
      "Zhiye Song",
      "Steve Dai",
      "Ben Keller",
      "Brucek Khailany"
    ],
    "published": "2025-12-03T05:08:18+00:00",
    "url": "https://arxiv.org/pdf/2512.03451v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03450v1",
    "title": "KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models",
    "abstract": "Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.",
    "authors": [
      "Rhys Newbury",
      "Juyan Zhang",
      "Tin Tran",
      "Hanna Kurniawati",
      "Dana Kuli\u0107"
    ],
    "published": "2025-12-03T05:08:03+00:00",
    "url": "https://arxiv.org/pdf/2512.03450v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03449v1",
    "title": "LM-CartSeg: Automated Segmentation of Lateral and Medial Cartilage and Subchondral Bone for Radiomics Analysis",
    "abstract": "Background and Objective: Radiomics of knee MRI requires robust, anatomically meaningful regions of interest (ROIs) that jointly capture cartilage and subchondral bone. Most existing work relies on manual ROIs and rarely reports quality control (QC). We present LM-CartSeg, a fully automatic pipeline for cartilage/bone segmentation, geometric lateral/medial (L/M) compartmentalisation and radiomics analysis. Methods: Two 3D nnU-Net models were trained on SKM-TEA (138 knees) and OAIZIB-CM (404 knees). At test time, zero-shot predictions were fused and refined by simple geometric rules: connected-component cleaning, construction of 10 mm subchondral bone bands in physical space, and a data-driven tibial L/M split based on PCA and k-means. Segmentation was evaluated on an OAIZIB-CM test set (103 knees) and on SKI-10 (100 knees). QC used volume and thickness signatures. From 10 ROIs we extracted 4 650 non-shape radiomic features to study inter-compartment similarity, dependence on ROI size, and OA vs. non-OA classification on OAIZIB-CM Results: Post-processing improved macro ASSD on OAIZIB-CM from 2.63 to 0.36 mm and HD95 from 25.2 to 3.35 mm, with DSC 0.91; zero-shot DSC on SKI-10 was 0.80. The geometric L/M rule produced stable compartments across datasets, whereas a direct L/M nnU-Net showed domain-dependent side swaps. Only 6 to 12 percent of features per ROI were strongly correlated with volume or thickness. Radiomics-based models models restricted to size-linked features. Conclusions: LM-CartSeg yields automatic, QCd ROIs and radiomic features that carry discriminative information beyond simple morphometry, providing a practical foundation for multi-centre knee OA radiomics studies.",
    "authors": [
      "Tongxu Zhang"
    ],
    "published": "2025-12-03T05:07:56+00:00",
    "url": "https://arxiv.org/pdf/2512.03449v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03445v1",
    "title": "Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation",
    "abstract": "Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.",
    "authors": [
      "Xieji Li",
      "Siyuan Yan",
      "Yingsheng Liu",
      "H. Peter Soyer",
      "Monika Janda",
      "Victoria Mar",
      "Zongyuan Ge"
    ],
    "published": "2025-12-03T04:55:54+00:00",
    "url": "https://arxiv.org/pdf/2512.03445v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03442v1",
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
    "authors": [
      "Xingrun Xing",
      "Zhiyuan Fan",
      "Jie Lou",
      "Guoqi Li",
      "Jiajun Zhang",
      "Debing Zhang"
    ],
    "published": "2025-12-03T04:51:32+00:00",
    "url": "https://arxiv.org/pdf/2512.03442v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03438v1",
    "title": "Multimodal Reinforcement Learning with Agentic Verifier for AI Agents",
    "abstract": "Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.",
    "authors": [
      "Reuben Tan",
      "Baolin Peng",
      "Zhengyuan Yang",
      "Hao Cheng",
      "Oier Mees",
      "Theodore Zhao",
      "Andrea Tupini",
      "Isar Meijier",
      "Qianhui Wu",
      "Yuncong Yang",
      "Lars Liden",
      "Yu Gu",
      "Sheng Zhang",
      "Xiaodong Liu",
      "Lijuan Wang",
      "Marc Pollefeys",
      "Yong Jae Lee",
      "Jianfeng Gao"
    ],
    "published": "2025-12-03T04:42:47+00:00",
    "url": "https://arxiv.org/pdf/2512.03438v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03430v1",
    "title": "Label-Efficient Hyperspectral Image Classification via Spectral FiLM Modulation of Low-Level Pretrained Diffusion Features",
    "abstract": "Hyperspectral imaging (HSI) enables detailed land cover classification, yet low spatial resolution and sparse annotations pose significant challenges. We present a label-efficient framework that leverages spatial features from a frozen diffusion model pretrained on natural images. Our approach extracts low-level representations from high-resolution decoder layers at early denoising timesteps, which transfer effectively to the low-texture structure of HSI. To integrate spectral and spatial information, we introduce a lightweight FiLM-based fusion module that adaptively modulates frozen spatial features using spectral cues, enabling robust multimodal learning under sparse supervision. Experiments on two recent hyperspectral datasets demonstrate that our method outperforms state-of-the-art approaches using only the provided sparse training labels. Ablation studies further highlight the benefits of diffusion-derived features and spectral-aware fusion. Overall, our results indicate that pretrained diffusion models can support domain-agnostic, label-efficient representation learning for remote sensing and broader scientific imaging tasks.",
    "authors": [
      "Yuzhen Hu",
      "Biplab Banerjee",
      "Saurabh Prasad"
    ],
    "published": "2025-12-03T04:23:54+00:00",
    "url": "https://arxiv.org/pdf/2512.03430v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03429v1",
    "title": "World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations",
    "abstract": "Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.",
    "authors": [
      "Raul Steinmetz",
      "Fabio Demo Rosa",
      "Victor Augusto Kich",
      "Jair Augusto Bottega",
      "Ricardo Bedin Grando",
      "Daniel Fernando Tello Gamarra"
    ],
    "published": "2025-12-03T04:15:31+00:00",
    "url": "https://arxiv.org/pdf/2512.03429v1",
    "categories": [
      "cs.RO",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03427v1",
    "title": "Generalization Evaluation of Deep Stereo Matching Methods for UAV-Based Forestry Applications",
    "abstract": "Autonomous UAV forestry operations require robust depth estimation methods with strong cross-domain generalization. However, existing evaluations focus on urban and indoor scenarios, leaving a critical gap for specialized vegetation-dense environments. We present the first systematic zero-shot evaluation of eight state-of-the-art stereo methods--RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM (plus baseline methods ACVNet, PSMNet, TCstereo)--spanning iterative refinement, foundation model, and zero-shot adaptation paradigms. All methods are trained exclusively on Scene Flow and evaluated without fine-tuning on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury forestry dataset captured with ZED Mini camera (1920x1080). Performance reveals scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D, 0.83-1.07 px on KITTI; DEFOM: 0.35-4.65 px across benchmarks), while iterative methods maintain cross-domain robustness (IGEV++: 0.36-6.77 px; IGEV: 0.33-21.91 px). Critical finding: RAFT-Stereo exhibits catastrophic ETH3D failure (26.23 px EPE, 98 percent error rate) due to negative disparity predictions, while performing normally on KITTI (0.90-1.11 px). Qualitative evaluation on Canterbury forestry dataset identifies DEFOM as the optimal gold-standard baseline for vegetation depth estimation, exhibiting superior depth smoothness, occlusion handling, and cross-domain consistency compared to IGEV++, despite IGEV++'s finer detail preservation.",
    "authors": [
      "Yida Lin",
      "Bing Xue",
      "Mengjie Zhang",
      "Sam Schofield",
      "Richard Green"
    ],
    "published": "2025-12-03T04:14:08+00:00",
    "url": "https://arxiv.org/pdf/2512.03427v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03424v1",
    "title": "DM3D: Deformable Mamba via Offset-Guided Gaussian Sequencing for Point Cloud Understanding",
    "abstract": "State Space Models (SSMs) demonstrate significant potential for long-sequence modeling, but their reliance on input order conflicts with the irregular nature of point clouds. Existing approaches often rely on predefined serialization strategies, which cannot adjust based on diverse geometric structures. To overcome this limitation, we propose \\textbf{DM3D}, a deformable Mamba architecture for point cloud understanding. Specifically, DM3D introduces an offset-guided Gaussian sequencing mechanism that unifies local resampling and global reordering within a deformable scan. The Gaussian-based KNN Resampling (GKR) enhances structural awareness by adaptively reorganizing neighboring points, while the Gaussian-based Differentiable Reordering (GDR) enables end-to-end optimization of serialization order. Furthermore, a Tri-Path Frequency Fusion module enhances feature complementarity and reduces aliasing. Together, these components enable structure-adaptive serialization of point clouds. Extensive experiments on benchmark datasets show that DM3D achieves state-of-the-art performance in classification, few-shot learning, and part segmentation, demonstrating that adaptive serialization effectively unlocks the potential of SSMs for point cloud understanding.",
    "authors": [
      "Bin Liu",
      "Chunyang Wang",
      "Xuelian Liu"
    ],
    "published": "2025-12-03T03:57:41+00:00",
    "url": "https://arxiv.org/pdf/2512.03424v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03422v1",
    "title": "What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models",
    "abstract": "In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.",
    "authors": [
      "Tianchen Deng",
      "Yue Pan",
      "Shenghai Yuan",
      "Dong Li",
      "Chen Wang",
      "Mingrui Li",
      "Long Chen",
      "Lihua Xie",
      "Danwei Wang",
      "Jingchuan Wang",
      "Javier Civera",
      "Hesheng Wang",
      "Weidong Chen"
    ],
    "published": "2025-12-03T03:57:01+00:00",
    "url": "https://arxiv.org/pdf/2512.03422v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03418v1",
    "title": "YOLOA: Real-Time Affordance Detection via LLM Adapter",
    "abstract": "Affordance detection aims to jointly address the fundamental \"what-where-how\" challenge in embodied AI by understanding \"what\" an object is, \"where\" the object is located, and \"how\" it can be used. However, most affordance learning methods focus solely on \"how\" objects can be used while neglecting the \"what\" and \"where\" aspects. Other affordance detection methods treat object detection and affordance learning as two independent tasks, lacking effective interaction and real-time capability. To overcome these limitations, we introduce YOLO Affordance (YOLOA), a real-time affordance detection model that jointly handles these two tasks via a large language model (LLM) adapter. Specifically, YOLOA employs a lightweight detector consisting of object detection and affordance learning branches refined through the LLM Adapter. During training, the LLM Adapter interacts with object and affordance preliminary predictions to refine both branches by generating more accurate class priors, box offsets, and affordance gates. Experiments on our relabeled ADG-Det and IIT-Heat benchmarks demonstrate that YOLOA achieves state-of-the-art accuracy (52.8 / 73.1 mAP on ADG-Det / IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, and up to 846.24 FPS for the lightweight variant). This indicates that YOLOA achieves an excellent trade-off between accuracy and efficiency.",
    "authors": [
      "Yuqi Ji",
      "Junjie Ke",
      "Lihuo He",
      "Jun Liu",
      "Kaifan Zhang",
      "Yu-Kun Lai",
      "Guiguang Ding",
      "Xinbo Gao"
    ],
    "published": "2025-12-03T03:53:31+00:00",
    "url": "https://arxiv.org/pdf/2512.03418v1",
    "categories": [
      "cs.CV",
      "cs.HC"
    ]
  },
  {
    "arxiv_id": "2512.03413v1",
    "title": "BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents",
    "abstract": "As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information from external complex documents, has attracted tremendous attention from both industry and academia. Existing RAG approaches often focus on general documents, and they overlook the fact that many real-world documents (such as books, booklets, handbooks, etc.) have a hierarchical structure, which organizes their content from different granularity levels, leading to poor performance for the QA task. To address these limitations, we introduce BookRAG, a novel RAG approach targeted for documents with a hierarchical structure, which exploits logical hierarchies and traces entity relations to query the highly relevant information. Specifically, we build a novel index structure, called BookIndex, by extracting a hierarchical tree from the document, which serves as the role of its table of contents, using a graph to capture the intricate relationships between entities, and mapping entities to tree nodes. Leveraging the BookIndex, we then propose an agent-based query method inspired by the Information Foraging Theory, which dynamically classifies queries and employs a tailored retrieval workflow. Extensive experiments on three widely adopted benchmarks demonstrate that BookRAG achieves state-of-the-art performance, significantly outperforming baselines in both retrieval recall and QA accuracy while maintaining competitive efficiency.",
    "authors": [
      "Shu Wang",
      "Yingli Zhou",
      "Yixiang Fang"
    ],
    "published": "2025-12-03T03:40:49+00:00",
    "url": "https://arxiv.org/pdf/2512.03413v1",
    "categories": [
      "cs.IR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03405v1",
    "title": "ViDiC: Video Difference Captioning",
    "abstract": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
    "authors": [
      "Jiangtao Wu",
      "Shihao Li",
      "Zhaozhou Bian",
      "Yuanxing Zhang",
      "Jialu Chen",
      "Runzhe Wen",
      "An Ping",
      "Yiwen He",
      "Jiakai Wang",
      "Jiaheng Liu"
    ],
    "published": "2025-12-03T03:23:24+00:00",
    "url": "https://arxiv.org/pdf/2512.03405v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03404v1",
    "title": "MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification",
    "abstract": "Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.",
    "authors": [
      "Yujian Zhao",
      "Hankun Liu",
      "Guanglin Niu"
    ],
    "published": "2025-12-03T03:23:19+00:00",
    "url": "https://arxiv.org/pdf/2512.03404v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03402v1",
    "title": "Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates",
    "abstract": "Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language generation (NLG), understanding (NLU), and commonsense reasoning datasets on GPT-2, RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.",
    "authors": [
      "Yixing Xu",
      "Chao Li",
      "Xuanwu Yin",
      "Spandan Tiwari",
      "Dong Li",
      "Ashish Sirasao",
      "Emad Barsoum"
    ],
    "published": "2025-12-03T03:14:09+00:00",
    "url": "https://arxiv.org/pdf/2512.03402v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03400v1",
    "title": "Better World Models Can Lead to Better Post-Training Performance",
    "abstract": "In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.",
    "authors": [
      "Prakhar Gupta",
      "Henry Conklin",
      "Sarah-Jane Leslie",
      "Andrew Lee"
    ],
    "published": "2025-12-03T03:13:20+00:00",
    "url": "https://arxiv.org/pdf/2512.03400v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03394v1",
    "title": "VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing",
    "abstract": "Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.",
    "authors": [
      "Hamed Poursiami",
      "Shay Snyder",
      "Guojing Cong",
      "Thomas Potok",
      "Maryam Parsa"
    ],
    "published": "2025-12-03T03:03:44+00:00",
    "url": "https://arxiv.org/pdf/2512.03394v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ]
  },
  {
    "arxiv_id": "2512.03383v1",
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "abstract": "Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.",
    "authors": [
      "Hung-Yueh Chiang",
      "Chi-Chih Chang",
      "Yu-Chen Lu",
      "Chien-Yu Lin",
      "Kai-Chiang Wu",
      "Mohamed S. Abdelfattah",
      "Diana Marculescu"
    ],
    "published": "2025-12-03T02:33:39+00:00",
    "url": "https://arxiv.org/pdf/2512.03383v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03381v1",
    "title": "Characterizing Language Use in a Collaborative Situated Game",
    "abstract": "Cooperative video games, where multiple participants must coordinate by communicating and reasoning under uncertainty in complex environments, yield a rich source of language data. We collect the Portal Dialogue Corpus: a corpus of 11.5 hours of spoken human dialogue in the co-op mode of the popular Portal 2 virtual puzzle game, comprising 24.5K total utterances. We analyze player language and behavior, identifying a number of linguistic phenomena that rarely appear in most existing chitchat or task-oriented dialogue corpora, including complex spatial reference, clarification and repair, and ad-hoc convention formation. To support future analyses of language use in complex, situated, collaborative problem-solving scenarios, we publicly release the corpus, which comprises player videos, audio, transcripts, game state data, and both manual and automatic annotations of language data.",
    "authors": [
      "Nicholas Tomlin",
      "Naitian Zhou",
      "Eve Fleisig",
      "Liangyuan",
      "Chen",
      "T\u00e9a Wright",
      "Lauren Vinh",
      "Laura X. Ma",
      "Seun Eisape",
      "Ellie French",
      "Tingting Du",
      "Tianjiao Zhang",
      "Alexander Koller",
      "Alane Suhr"
    ],
    "published": "2025-12-03T02:29:53+00:00",
    "url": "https://arxiv.org/pdf/2512.03381v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03377v1",
    "title": "Nexus: Higher-Order Attention Mechanisms in Transformers",
    "abstract": "Transformers have achieved significant success across various domains, relying on self-attention to capture dependencies. However, the standard first-order attention mechanism is often limited by a low-rank bottleneck, struggling to capture intricate, multi-hop relationships within a single layer. In this paper, we propose the \\textbf{Higher-Order Attention Network (Hon)}, a novel architecture designed to enhance representational power through a recursive framework. Unlike standard approaches that use static linear projections for Queries and Keys, Hon dynamically refines these representations via nested self-attention mechanisms. Specifically, the Query and Key vectors are themselves outputs of inner attention loops, allowing tokens to aggregate global context and model high-order correlations \\textit{prior} to the final attention computation. We enforce a parameter-efficient weight-sharing strategy across recursive steps, ensuring that this enhanced expressivity incurs $\\mathcal{O}(1)$ additional parameters. We provide theoretical analysis demonstrating that our method breaks the linear bottleneck of standard attention. Empirically, Hon outperforms standard Transformers on multiple benchmarks.",
    "authors": [
      "Hanting Chen",
      "Chu Zhong",
      "Kai Han",
      "Yuchuan Tian",
      "Yuchen Liang",
      "Tianyu Guo",
      "Xinghao Chen",
      "Dacheng Tao",
      "Yunhe Wang"
    ],
    "published": "2025-12-03T02:25:38+00:00",
    "url": "https://arxiv.org/pdf/2512.03377v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03373v1",
    "title": "LLM-Generated Ads: From Personalization Parity to Persuasion Superiority",
    "abstract": "As large language models (LLMs) become increasingly capable of generating persuasive content, understanding their effectiveness across different advertising strategies becomes critical. This paper presents a two-part investigation examining LLM-generated advertising through complementary lenses: (1) personality-based and (2) psychological persuasion principles.   In our first study (n=400), we tested whether LLMs could generate personalized advertisements tailored to specific personality traits (openness and neuroticism) and how their performance compared to human experts. Results showed that LLM-generated ads achieved statistical parity with human-written ads (51.1% vs. 48.9%, p > 0.05), with no significant performance differences for matched personalities.   Building on these insights, our second study (n=800) shifted focus from individual personalization to universal persuasion, testing LLM performance across four foundational psychological principles: authority, consensus, cognition, and scarcity. AI-generated ads significantly outperformed human-created content, achieving a 59.1% preference rate (vs. 40.9%, p < 0.001), with the strongest performance in authority (63.0%) and consensus (62.5%) appeals. Qualitative analysis revealed AI's advantage stems from crafting more sophisticated, aspirational messages and achieving superior visual-narrative coherence. Critically, this quality advantage proved robust: even after applying a 21.2 percentage point detection penalty when participants correctly identified AI-origin, AI ads still outperformed human ads, and 29.4% of participants chose AI content despite knowing its origin. These findings demonstrate LLMs' evolution from parity in personalization to superiority in persuasive storytelling, with significant implications for advertising practice given LLMs' near-zero marginal cost and time requirements compared to human experts.",
    "authors": [
      "Elyas Meguellati",
      "Stefano Civelli",
      "Lei Han",
      "Abraham Bernstein",
      "Shazia Sadiq",
      "Gianluca Demartini"
    ],
    "published": "2025-12-03T02:13:38+00:00",
    "url": "https://arxiv.org/pdf/2512.03373v1",
    "categories": [
      "cs.CY",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03370v1",
    "title": "ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding",
    "abstract": "We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.",
    "authors": [
      "Lingjun Zhao",
      "Yandong Luo",
      "James Hay",
      "Lu Gan"
    ],
    "published": "2025-12-03T02:06:09+00:00",
    "url": "https://arxiv.org/pdf/2512.03370v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03369v1",
    "title": "FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting",
    "abstract": "Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.",
    "authors": [
      "Nan Zhou",
      "Huandong Wang",
      "Jiahao Li",
      "Han Li",
      "Yali Song",
      "Qiuhua Wang",
      "Yong Li",
      "Xinlei Chen"
    ],
    "published": "2025-12-03T02:02:47+00:00",
    "url": "https://arxiv.org/pdf/2512.03369v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03360v1",
    "title": "From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation",
    "abstract": "Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.",
    "authors": [
      "Qingchuan Li",
      "Mingyue Cheng",
      "Zirui Liu",
      "Daoyu Wang",
      "Yuting Zeng",
      "Tongxuan Liu"
    ],
    "published": "2025-12-03T01:52:31+00:00",
    "url": "https://arxiv.org/pdf/2512.03360v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03359v1",
    "title": "A Hybrid Deep Learning Framework with Explainable AI for Lung Cancer Classification with DenseNet169 and SVM",
    "abstract": "Lung cancer is a very deadly disease worldwide, and its early diagnosis is crucial for increasing patient survival rates. Computed tomography (CT) scans are widely used for lung cancer diagnosis as they can give detailed lung structures. However, manual interpretation is time-consuming and prone to human error. To surmount this challenge, the study proposes a deep learning-based automatic lung cancer classification system to enhance detection accuracy and interpretability. The IQOTHNCCD lung cancer dataset is utilized, which is a public CT scan dataset consisting of cases categorized into Normal, Benign, and Malignant and used DenseNet169, which includes Squeezeand-Excitation blocks for attention-based feature extraction, Focal Loss for handling class imbalance, and a Feature Pyramid Network (FPN) for multi-scale feature fusion. In addition, an SVM model was developed using MobileNetV2 for feature extraction, improving its classification performance. For model interpretability enhancement, the study integrated Grad-CAM for the visualization of decision-making regions in CT scans and SHAP (Shapley Additive Explanations) for explanation of feature contributions within the SVM model. Intensive evaluation was performed, and it was found that both DenseNet169 and SVM models achieved 98% accuracy, suggesting their robustness for real-world medical practice. These results open up the potential for deep learning to improve the diagnosis of lung cancer by a higher level of accuracy, transparency, and robustness.",
    "authors": [
      "Md Rashidul Islam",
      "Bakary Gibba",
      "Altagi Abdallah Bakheit Abdelgadir"
    ],
    "published": "2025-12-03T01:48:56+00:00",
    "url": "https://arxiv.org/pdf/2512.03359v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03350v1",
    "title": "SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation",
    "abstract": "Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\\to$4D$\\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.",
    "authors": [
      "Yu Yuan",
      "Tharindu Wickremasinghe",
      "Zeeshan Nadir",
      "Xijun Wang",
      "Yiheng Chi",
      "Stanley H. Chan"
    ],
    "published": "2025-12-03T01:30:45+00:00",
    "url": "https://arxiv.org/pdf/2512.03350v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03346v1",
    "title": "Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus",
    "abstract": "The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge. The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention. This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved. This study presents a controlled comparison of sixteen modern deep learning architectures spanning 2D/3D convolutional, hybrid, and volumetric transformer families for subclinical keratoconus (SKC) detection from 3D anterior segment OCT volumes. We demonstrate that hierarchical attention models offer a superior and more parameter-efficient inductive bias, surpassing the performance of both 2D and 3D CNNs and ViTs. Our results show 21-23% higher sensitivity and specificity in the sparse anomaly (subclinical) regime. Mechanistic analyses reveal that this advantage stems from precise spatial scale alignment: hierarchical windowing produces effective receptive fields matched to the intermediate, multi-slice extent of subclinical abnormalities. This avoids excessive CNN locality and diffuse global attention. Attention-distance measurements confirm a key insight into architectural adaptation: the required spatial integration length shifts significantly based on the signal strength, with subclinical cases necessitating longer integration compared to both healthy and manifest disease states. Representational similarity and auxiliary age/sex prediction tasks further support the generalizability of these inductive principles. The findings provide design guidance for future volumetric anomaly detection systems, establishing hierarchical attention as a principled and effective approach for early pathological change analysis in 3D medical imaging.",
    "authors": [
      "Lynn Kandakji",
      "William Woof",
      "Nikolas Pontikos"
    ],
    "published": "2025-12-03T01:20:13+00:00",
    "url": "https://arxiv.org/pdf/2512.03346v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03345v1",
    "title": "HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration",
    "abstract": "Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.",
    "authors": [
      "Seunghoi Kim",
      "Henry F. J. Tregidgo",
      "Chen Jin",
      "Matteo Figini",
      "Daniel C. Alexander"
    ],
    "published": "2025-12-03T01:20:00+00:00",
    "url": "https://arxiv.org/pdf/2512.03345v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03343v1",
    "title": "Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning",
    "abstract": "Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \\citep{holtzman2019curious}. While scaling model size mitigates this \\citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.",
    "authors": [
      "Darshan Fofadiya"
    ],
    "published": "2025-12-03T01:17:07+00:00",
    "url": "https://arxiv.org/pdf/2512.03343v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03340v1",
    "title": "PERCS: Persona-Guided Controllable Biomedical Summarization Dataset",
    "abstract": "Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.",
    "authors": [
      "Rohan Charudatt Salvi",
      "Chirag Chawla",
      "Dhruv Jain",
      "Swapnil Panigrahi",
      "Md Shad Akhtar",
      "Shweta Yadav"
    ],
    "published": "2025-12-03T01:13:56+00:00",
    "url": "https://arxiv.org/pdf/2512.03340v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03339v1",
    "title": "ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography",
    "abstract": "Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\\pm$2.68 to 79.64$\\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF",
    "authors": [
      "Yeganeh Ghamary",
      "Victoria Wu",
      "Hooman Vaseli",
      "Christina Luong",
      "Teresa Tsang",
      "Siavash Bigdeli",
      "Purang Abolmaesumi"
    ],
    "published": "2025-12-03T01:11:28+00:00",
    "url": "https://arxiv.org/pdf/2512.03339v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03337v1",
    "title": "Epistemic Substitution: How Grokipedia's AI-Generated Encyclopedia Restructures Authority",
    "abstract": "A quarter century ago, Wikipedia's decentralized, crowdsourced, and consensus-driven model replaced the centralized, expert-driven, and authority-based standard for encyclopedic knowledge curation. The emergence of generative AI encyclopedias, such as Grokipedia, possibly presents another potential shift in epistemic evolution. This study investigates whether AI- and human-curated encyclopedias rely on the same foundations of authority. We conducted a multi-scale comparative analysis of the citation networks from 72 matched article pairs, which cite a total of almost 60,000 sources. Using an 8-category epistemic classification, we mapped the \"epistemic profiles\" of the articles on each platform. Our findings reveal several quantitative and qualitative differences in how knowledge is sourced and encyclopedia claims are epistemologically justified. Grokipedia replaces Wikipedia's heavy reliance on peer-reviewed \"Academic & Scholarly\" work with a notable increase in \"User-generated\" and \"Civic organization\" sources. Comparative network analyses further show that Grokipedia employs very different epistemological profiles when sourcing leisure topics (such as Sports and Entertainment) and more societal sensitive civic topics (such as Politics & Conflicts, Geographical Entities, and General Knowledge & Society). Finally, we find a \"scaling-law for AI-generated knowledge sourcing\" that shows a linear relationship between article length and citation density, which is distinct from collective human reference sourcing. We conclude that this first implementation of an LLM-based encyclopedia does not merely automate knowledge production but restructures it. Given the notable changes and the important role of encyclopedias, we suggest the continuation and deepening of algorithm audits, such as the one presented here, in order to understand the ongoing epistemological shifts.",
    "authors": [
      "Aliakbar Mehdizadeh",
      "Martin Hilbert"
    ],
    "published": "2025-12-03T01:05:32+00:00",
    "url": "https://arxiv.org/pdf/2512.03337v1",
    "categories": [
      "cs.SI",
      "cs.CL",
      "cs.CY",
      "cs.DL"
    ]
  },
  {
    "arxiv_id": "2512.03336v1",
    "title": "Single-Round Scalable Analytic Federated Learning",
    "abstract": "Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.",
    "authors": [
      "Alan T. L. Bacellar",
      "Mustafa Munir",
      "Felipe M. G. Fran\u00e7a",
      "Priscila M. V. Lima",
      "Radu Marculescu",
      "Lizy K. John"
    ],
    "published": "2025-12-03T01:00:37+00:00",
    "url": "https://arxiv.org/pdf/2512.03336v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.03335v1",
    "title": "Step-by-step Layered Design Generation",
    "abstract": "Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.",
    "authors": [
      "Faizan Farooq Khan",
      "K J Joseph",
      "Koustava Goswami",
      "Mohamed Elhoseiny",
      "Balaji Vasan Srinivasan"
    ],
    "published": "2025-12-03T00:59:43+00:00",
    "url": "https://arxiv.org/pdf/2512.03335v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03334v1",
    "title": "Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaran\u00ed",
    "abstract": "This study presents an LLM-assisted annotation pipeline for the sociolinguistic and topical analysis of bilingual discourse in two typologically distinct contexts: Spanish-English and Spanish-Guaran\u00ed. Using large language models, we automatically labeled topic, genre, and discourse-pragmatic functions across a total of 3,691 code-switched sentences, integrated demographic metadata from the Miami Bilingual Corpus, and enriched the Spanish-Guaran\u00ed dataset with new topic annotations. The resulting distributions reveal systematic links between gender, language dominance, and discourse function in the Miami data, and a clear diglossic division between formal Guaran\u00ed and informal Spanish in Paraguayan texts. These findings replicate and extend earlier interactional and sociolinguistic observations with corpus-scale quantitative evidence. The study demonstrates that large language models can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, advancing computational methods for cross-linguistic and low-resource bilingual research.",
    "authors": [
      "Nemika Tyagi",
      "Nelvin Licona Guevara",
      "Olga Kellert"
    ],
    "published": "2025-12-03T00:56:27+00:00",
    "url": "https://arxiv.org/pdf/2512.03334v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03324v1",
    "title": "Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs",
    "abstract": "Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.",
    "authors": [
      "Ngoc Bui",
      "Shubham Sharma",
      "Simran Lamba",
      "Saumitra Mishra",
      "Rex Ying"
    ],
    "published": "2025-12-03T00:20:35+00:00",
    "url": "https://arxiv.org/pdf/2512.03324v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03318v1",
    "title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia",
    "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.",
    "authors": [
      "Chandler Smith",
      "Marwa Abdulhai",
      "Manfred Diaz",
      "Marko Tesic",
      "Rakshit S. Trivedi",
      "Alexander Sasha Vezhnevets",
      "Lewis Hammond",
      "Jesse Clifton",
      "Minsuk Chang",
      "Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n",
      "John P. Agapiou",
      "Jayd Matyas",
      "Danny Karmon",
      "Akash Kundu",
      "Aliaksei Korshuk",
      "Ananya Ananya",
      "Arrasy Rahman",
      "Avinaash Anand Kulandaivel",
      "Bain McHale",
      "Beining Zhang",
      "Buyantuev Alexander",
      "Carlos Saith Rodriguez Rojas",
      "Caroline Wang",
      "Chetan Talele",
      "Chenao Liu",
      "Chichen Lin",
      "Diana Riazi",
      "Di Yang Shi",
      "Emanuel Tewolde",
      "Elizaveta Tennant",
      "Fangwei Zhong",
      "Fuyang Cui",
      "Gang Zhao",
      "Gema Parre\u00f1o Piqueras",
      "Hyeonggeun Yun",
      "Ilya Makarov",
      "Jiaxun Cui",
      "Jebish Purbey",
      "Jim Dilkes",
      "Jord Nguyen",
      "Lingyun Xiao",
      "Luis Felipe Giraldo",
      "Manuela Chacon-Chamorro",
      "Manuel Sebastian Rios Beltran",
      "Marta Emili Garc\u00eda Segura",
      "Mengmeng Wang",
      "Mogtaba Alim",
      "Nicanor Quijano",
      "Nico Schiavone",
      "Olivia Macmillan-Scott",
      "Oswaldo Pe\u00f1a",
      "Peter Stone",
      "Ram Mohan Rao Kadiyala",
      "Rolando Fernandez",
      "Ruben Manrique",
      "Sunjia Lu",
      "Sheila A. McIlraith",
      "Shamika Dhuri",
      "Shuqing Shi",
      "Siddhant Gupta",
      "Sneheel Sarangi",
      "Sriram Ganapathi Subramanian",
      "Taehun Cha",
      "Toryn Q. Klassen",
      "Wenming Tu",
      "Weijian Fan",
      "Wu Ruiyang",
      "Xue Feng",
      "Yali Du",
      "Yang Liu",
      "Yiding Wang",
      "Yipeng Kang",
      "Yoonchang Sung",
      "Yuxuan Chen",
      "Zhaowei Zhang",
      "Zhihan Wang",
      "Zhiqiang Wu",
      "Ziang Chen",
      "Zilong Zheng",
      "Zixia Jia",
      "Ziyan Wang",
      "Dylan Hadfield-Menell",
      "Natasha Jaques",
      "Tim Baarslag",
      "Jose Hernandez-Orallo",
      "Joel Z. Leibo"
    ],
    "published": "2025-12-03T00:11:05+00:00",
    "url": "https://arxiv.org/pdf/2512.03318v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03317v1",
    "title": "NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction",
    "abstract": "Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion",
    "authors": [
      "Thomas Monninger",
      "Zihan Zhang",
      "Steffen Staab",
      "Sihao Ding"
    ],
    "published": "2025-12-03T00:10:47+00:00",
    "url": "https://arxiv.org/pdf/2512.03317v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.03310v1",
    "title": "Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs",
    "abstract": "The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.",
    "authors": [
      "Kunj Joshi",
      "David A. Smith"
    ],
    "published": "2025-12-02T23:46:42+00:00",
    "url": "https://arxiv.org/pdf/2512.03310v1",
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03309v1",
    "title": "Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates",
    "abstract": "Coarse resolution, imperfect parameterizations, and uncertain initial states and forcings limit Earth-system model (ESM) predictions. Traditional bias correction via data assimilation improves constrained simulations but offers limited benefit once models run freely. We introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies and applies them online during integration. Building on a U-Net backbone, we develop two operator architectures Inception U-Net (IUNet) and a multi-scale network (M\\&M) that combine diverse upsampling and receptive fields to capture multiscale nonlinear features under Energy Exascale Earth System Model (E3SM) runtime constraints. Trained on two years E3SM simulations nudged toward ERA5 reanalysis, the operators generalize across height levels and seasons. Both architectures outperform standard U-Net baselines in offline tests, indicating that functional richness rather than parameter count drives performance. In online hybrid E3SM runs, M\\&M delivers the most consistent bias reductions across variables and vertical levels. The ML-augmented configurations remain stable and computationally feasible in multi-year simulations, providing a practical pathway for scalable hybrid modeling. Our framework emphasizes long-term stability, portability, and cadence-limited updates, demonstrating the utility of expressive ML operators for learning structured, cross-scale relationships and retrofitting legacy ESMs.",
    "authors": [
      "Aniruddha Bora",
      "Shixuan Zhang",
      "Khemraj Shukla",
      "Bryce Harrop",
      "George Em. Karniadakis",
      "L. Ruby Leung"
    ],
    "published": "2025-12-02T23:44:49+00:00",
    "url": "https://arxiv.org/pdf/2512.03309v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math-ph"
    ]
  },
  {
    "arxiv_id": "2512.03307v1",
    "title": "Robust Tabular Foundation Models",
    "abstract": "The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.",
    "authors": [
      "Matthew Peroni",
      "Franck Le",
      "Vadim Sheinin"
    ],
    "published": "2025-12-02T23:40:39+00:00",
    "url": "https://arxiv.org/pdf/2512.03307v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03300v1",
    "title": "HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction",
    "abstract": "Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.",
    "authors": [
      "Pengfei Hu",
      "Fan Ming",
      "Xiaoxue Han",
      "Chang Lu",
      "Yue Ning",
      "Dan Lu"
    ],
    "published": "2025-12-02T23:27:17+00:00",
    "url": "https://arxiv.org/pdf/2512.03300v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03298v1",
    "title": "Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction",
    "abstract": "Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.",
    "authors": [
      "Echo Diyun LU",
      "Charles Findling",
      "Marianne Clausel",
      "Alessandro Leite",
      "Wei Gong",
      "Pierric Kersaudy"
    ],
    "published": "2025-12-02T23:21:01+00:00",
    "url": "https://arxiv.org/pdf/2512.03298v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03293v1",
    "title": "Prior preferences in active inference agents: soft, hard, and goal shaping",
    "abstract": "Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).",
    "authors": [
      "Filippo Torresan",
      "Ryota Kanai",
      "Manuel Baltieri"
    ],
    "published": "2025-12-02T23:07:24+00:00",
    "url": "https://arxiv.org/pdf/2512.03293v1",
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ]
  },
  {
    "arxiv_id": "2512.03284v1",
    "title": "SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding",
    "abstract": "Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.",
    "authors": [
      "Hongpei Zheng",
      "Shijie Li",
      "Yanran Li",
      "Hujun Yin"
    ],
    "published": "2025-12-02T22:49:01+00:00",
    "url": "https://arxiv.org/pdf/2512.03284v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03280v1",
    "title": "BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark",
    "abstract": "Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.",
    "authors": [
      "Nicholas Sung",
      "Steven Spreizer",
      "Mohamed Elrefaie",
      "Matthew C. Jones",
      "Faez Ahmed"
    ],
    "published": "2025-12-02T22:39:07+00:00",
    "url": "https://arxiv.org/pdf/2512.03280v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03278v1",
    "title": "Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases",
    "abstract": "In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims$\\unicode{x2014}$often about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases$\\unicode{x2014}$typically a few hundred rows$\\unicode{x2014}$that conveniently fit within an LLM's context window.   In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset$\\unicode{x2014}$the standard benchmark for fact verification over structured data$\\unicode{x2014}$Thucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).",
    "authors": [
      "Michael Theologitis",
      "Dan Suciu"
    ],
    "published": "2025-12-02T22:35:48+00:00",
    "url": "https://arxiv.org/pdf/2512.03278v1",
    "categories": [
      "cs.DB",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03272v1",
    "title": "When Do Symbolic Solvers Enhance Reasoning in Large Language Models?",
    "abstract": "Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models \"overthink\" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.",
    "authors": [
      "Zhiyuan He",
      "Dingmin Wang"
    ],
    "published": "2025-12-02T22:23:26+00:00",
    "url": "https://arxiv.org/pdf/2512.03272v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03262v1",
    "title": "Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks",
    "abstract": "Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.",
    "authors": [
      "Songwen Zhao",
      "Danqing Wang",
      "Kexun Zhang",
      "Jiaxuan Luo",
      "Zhuo Li",
      "Lei Li"
    ],
    "published": "2025-12-02T22:11:56+00:00",
    "url": "https://arxiv.org/pdf/2512.03262v1",
    "categories": [
      "cs.SE",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03257v1",
    "title": "PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery",
    "abstract": "Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.   We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.   Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.",
    "authors": [
      "Mark Moussa",
      "Andre Williams",
      "Seth Roffe",
      "Douglas Morton"
    ],
    "published": "2025-12-02T21:59:45+00:00",
    "url": "https://arxiv.org/pdf/2512.03257v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03248v1",
    "title": "Learning Network Sheaves for AI-native Semantic Communication",
    "abstract": "Recent advances in AI call for a paradigm shift from bit-centric communication to goal- and semantics-oriented architectures, paving the way for AI-native 6G networks. In this context, we address a key open challenge: enabling heterogeneous AI agents to exchange compressed latent-space representations while mitigating semantic noise and preserving task-relevant meaning. We cast this challenge as learning both the communication topology and the alignment maps that govern information exchange among agents, yielding a learned network sheaf equipped with orthogonal maps. This learning process is further supported by a semantic denoising end compression module that constructs a shared global semantic space and derives sparse, structured representations of each agent's latent space. This corresponds to a nonconvex dictionary learning problem solved iteratively with closed-form updates. Experiments with mutiple AI agents pre-trained on real image data show that the semantic denoising and compression facilitates AI agents alignment and the extraction of semantic clusters, while preserving high accuracy in downstream task. The resulting communication network provides new insights about semantic heterogeneity across agents, highlighting the interpretability of our methodology.",
    "authors": [
      "Enrico Grimaldi",
      "Mario Edoardo Pandolfo",
      "Gabriele D'Acunto",
      "Sergio Barbarossa",
      "Paolo Di Lorenzo"
    ],
    "published": "2025-12-02T21:36:44+00:00",
    "url": "https://arxiv.org/pdf/2512.03248v1",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "arxiv_id": "2512.03247v1",
    "title": "PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement",
    "abstract": "Latent Diffusion Models (LDMs) have markedly advanced the quality of image inpainting and local editing. However, the inherent latent compression often introduces pixel-level inconsistencies, such as chromatic shifts, texture mismatches, and visible seams along editing boundaries. Existing remedies, including background-conditioned latent decoding and pixel-space harmonization, usually fail to fully eliminate these artifacts in practice and do not generalize well across different latent representations or tasks. We introduce PixPerfect, a pixel-level refinement framework that delivers seamless, high-fidelity local edits across diverse LDM architectures and tasks. PixPerfect leverages (i) a differentiable discriminative pixel space that amplifies and suppresses subtle color and texture discrepancies, (ii) a comprehensive artifact simulation pipeline that exposes the refiner to realistic local editing artifacts during training, and (iii) a direct pixel-space refinement scheme that ensures broad applicability across diverse latent representations and tasks. Extensive experiments on inpainting, object removal, and insertion benchmarks demonstrate that PixPerfect substantially enhances perceptual fidelity and downstream editing performance, establishing a new standard for robust and high-fidelity localized image editing.",
    "authors": [
      "Haitian Zheng",
      "Yuan Yao",
      "Yongsheng Yu",
      "Yuqian Zhou",
      "Jiebo Luo",
      "Zhe Lin"
    ],
    "published": "2025-12-02T21:35:57+00:00",
    "url": "https://arxiv.org/pdf/2512.03247v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03245v1",
    "title": "2-Shots in the Dark: Low-Light Denoising with Minimal Data Acquisition",
    "abstract": "Raw images taken in low-light conditions are very noisy due to low photon count and sensor noise. Learning-based denoisers have the potential to reconstruct high-quality images. For training, however, these denoisers require large paired datasets of clean and noisy images, which are difficult to collect. Noise synthesis is an alternative to large-scale data acquisition: given a clean image, we can synthesize a realistic noisy counterpart. In this work, we propose a general and practical noise synthesis method that requires only one single noisy image and one single dark frame per ISO setting. We represent signal-dependent noise with a Poisson distribution and introduce a Fourier-domain spectral sampling algorithm to accurately model signal-independent noise. The latter generates diverse noise realizations that maintain the spatial and statistical properties of real sensor noise. As opposed to competing approaches, our method neither relies on simplified parametric models nor on large sets of clean-noisy image pairs. Our synthesis method is not only accurate and practical, it also leads to state-of-the-art performances on multiple low-light denoising benchmarks.",
    "authors": [
      "Liying Lu",
      "Rapha\u00ebl Achddou",
      "Sabine S\u00fcsstrunk"
    ],
    "published": "2025-12-02T21:32:31+00:00",
    "url": "https://arxiv.org/pdf/2512.03245v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03244v1",
    "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
    "abstract": "Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.",
    "authors": [
      "Salman Rahman",
      "Sruthi Gorantla",
      "Arpit Gupta",
      "Swastik Roy",
      "Nanyun Peng",
      "Yang Liu"
    ],
    "published": "2025-12-02T21:30:47+00:00",
    "url": "https://arxiv.org/pdf/2512.03244v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03238v1",
    "title": "How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy",
    "abstract": "High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, \\emph{Differentially Private Synthetic data}, refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization.   In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.",
    "authors": [
      "Natalia Ponomareva",
      "Zheng Xu",
      "H. Brendan McMahan",
      "Peter Kairouz",
      "Lucas Rosenblatt",
      "Vincent Cohen-Addad",
      "Crist\u00f3bal Guzm\u00e1n",
      "Ryan McKenna",
      "Galen Andrew",
      "Alex Bie",
      "Da Yu",
      "Alex Kurakin",
      "Morteza Zadimoghaddam",
      "Sergei Vassilvitskii",
      "Andreas Terzis"
    ],
    "published": "2025-12-02T21:14:39+00:00",
    "url": "https://arxiv.org/pdf/2512.03238v1",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.03237v1",
    "title": "LLM-Guided Material Inference for 3D Point Clouds",
    "abstract": "Most existing 3D shape datasets and models focus solely on geometry, overlooking the material properties that determine how objects appear. We introduce a two-stage large language model (LLM) based method for inferring material composition directly from 3D point clouds with coarse segmentations. Our key insight is to decouple reasoning about what an object is from what it is made of. In the first stage, an LLM predicts the object's semantic; in the second stage, it assigns plausible materials to each geometric segment, conditioned on the inferred semantics. Both stages operate in a zero-shot manner, without task-specific training. Because existing datasets lack reliable material annotations, we evaluate our method using an LLM-as-a-Judge implemented in DeepEval. Across 1,000 shapes from Fusion/ABS and ShapeNet, our method achieves high semantic and material plausibility. These results demonstrate that language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.",
    "authors": [
      "Nafiseh Izadyar",
      "Teseo Schneider"
    ],
    "published": "2025-12-02T21:14:04+00:00",
    "url": "https://arxiv.org/pdf/2512.03237v1",
    "categories": [
      "cs.CV",
      "cs.GR"
    ]
  },
  {
    "arxiv_id": "2512.03233v1",
    "title": "Object Counting with GPT-4o and GPT-5: A Comparative Study",
    "abstract": "Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.",
    "authors": [
      "Richard F\u00fczess\u00e9ry",
      "Kaziwa Saleh",
      "S\u00e1ndor Sz\u00e9n\u00e1si",
      "Zolt\u00e1n V\u00e1mossy"
    ],
    "published": "2025-12-02T21:07:13+00:00",
    "url": "https://arxiv.org/pdf/2512.03233v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03216v1",
    "title": "Kaleidoscopic Scintillation Event Imaging",
    "abstract": "Scintillators are transparent materials that interact with high-energy particles and emit visible light as a result. They are used in state of the art methods of measuring high-energy particles and radiation sources. Most existing methods use fast single-pixel detectors to detect and time scintillation events. Cameras provide spatial resolution but can only capture an average over many events, making it difficult to image the events associated with an individual particle. Emerging single-photon avalanche diode cameras combine speed and spatial resolution to enable capturing images of individual events. This allows us to use machine vision techniques to analyze events, enabling new types of detectors. The main challenge is the very low brightness of the events. Techniques have to work with a very limited number of photons.   We propose a kaleidoscopic scintillator to increase light collection in a single-photon camera while preserving the event's spatial information. The kaleidoscopic geometry creates mirror reflections of the event in known locations for a given event location that are captured by the camera. We introduce theory for imaging an event in a kaleidoscopic scintillator and an algorithm to estimate the event's 3D position. We find that the kaleidoscopic scintillator design provides sufficient light collection to perform high-resolution event measurements for advanced radiation imaging techniques using a commercial CMOS single-photon camera. Code and data are available at https://github.com/bocchs/kaleidoscopic_scintillator.",
    "authors": [
      "Alex Bocchieri",
      "John Mamish",
      "David Appleyard",
      "Andreas Velten"
    ],
    "published": "2025-12-02T20:39:24+00:00",
    "url": "https://arxiv.org/pdf/2512.03216v1",
    "categories": [
      "physics.ins-det",
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "arxiv_id": "2512.03214v1",
    "title": "Identifying attributions of causality in political text",
    "abstract": "Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.",
    "authors": [
      "Paulina Garcia-Corral"
    ],
    "published": "2025-12-02T20:37:07+00:00",
    "url": "https://arxiv.org/pdf/2512.03214v1",
    "categories": [
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.03210v1",
    "title": "Flux4D: Flow-based Unsupervised 4D Reconstruction",
    "abstract": "Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an \"as static as possible\" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.",
    "authors": [
      "Jingkang Wang",
      "Henry Che",
      "Yun Chen",
      "Ze Yang",
      "Lily Goli",
      "Sivabalan Manivasagam",
      "Raquel Urtasun"
    ],
    "published": "2025-12-02T20:28:45+00:00",
    "url": "https://arxiv.org/pdf/2512.03210v1",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.03199v1",
    "title": "Does Head Pose Correction Improve Biometric Facial Recognition?",
    "abstract": "Biometric facial recognition models often demonstrate significant decreases in accuracy when processing real-world images, often characterized by poor quality, non-frontal subject poses, and subject occlusions. We investigate whether targeted, AI-driven, head-pose correction and image restoration can improve recognition accuracy. Using a model-agnostic, large-scale, forensic-evaluation pipeline, we assess the impact of three restoration approaches: 3D reconstruction (NextFace), 2D frontalization (CFR-GAN), and feature enhancement (CodeFormer). We find that naive application of these techniques substantially degrades facial recognition accuracy. However, we also find that selective application of CFR-GAN combined with CodeFormer yields meaningful improvements.",
    "authors": [
      "Justin Norman",
      "Hany Farid"
    ],
    "published": "2025-12-02T19:53:30+00:00",
    "url": "https://arxiv.org/pdf/2512.03199v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03197v1",
    "title": "InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation",
    "abstract": "Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.",
    "authors": [
      "Faezeh Faez",
      "Marzieh S. Tahaei",
      "Yaochen Hu",
      "Ali Pourranjbar",
      "Mahdi Biparva",
      "Mark Coates",
      "Yingxue Zhang"
    ],
    "published": "2025-12-02T19:51:28+00:00",
    "url": "https://arxiv.org/pdf/2512.03197v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03196v1",
    "title": "Ultra-Strong Gradient Diffusion MRI with Self-Supervised Learning for Prostate Cancer Characterization",
    "abstract": "Diffusion MRI (dMRI) enables non-invasive assessment of prostate microstructure but conventional metrics such as the Apparent Diffusion Coefficient in multiparametric MRI lack specificity to underlying histology. Integrating dMRI with the compartment-based biophysical VERDICT (Vascular, Extracellular, and Restricted Diffusion for Cytometry in Tumours) framework offers richer microstructural insights, though clinical gradient systems (40-80 mT/m) suffer from poor signal-to-noise ratio (SNR) at stronger diffusion weightings due to prolonged echo times. Ultra-strong gradients (up to 300 mT/m) can mitigate these limitations by improving SNR and contrast-to-noise ratios (CNR) but their adoption has until recently been limited to research environments due to challenges with peripheral nerve stimulation thresholds and gradient non-uniformity. This study investigates whether physics-informed self-supervised VERDICT (ssVERDICT) fitting applied to ultra-strong gradients enhances prostate cancer characterization relative to current clinical acquisitions. We developed enhanced ssVERDICT fitting approaches using dense multilayer perceptron (Dense MLP) and convolutional U-Net architectures, benchmarking them against non-linear least-squares (NLLS) fitting and Diffusion Kurtosis Imaging across clinical- to ultra-strong gradient systems. Dense ssVERDICT at ultra-strong gradient notably outperformed NLLS VERDICT, boosting median CNR by 47%, cutting inter-patient Coefficient of Variation by 52%, and reducing pooled f_ic variation by 50%. Overall, it delivered the highest CNR, the most stable parameter estimates, and the clearest tumour-normal contrast compared with conventional methods and clinical gradient systems. These findings highlight the potential of advanced gradient systems and deep learning-based modelling to improve non-invasive prostate cancer characterization and reduce unnecessary biopsies.",
    "authors": [
      "Tanishq Patil",
      "Snigdha Sen",
      "Malwina Molendowska",
      "Kieran G. Foley",
      "Fabrizio Fasano",
      "Mara Cercignani",
      "Marco Palombo",
      "Paddy J. Slator",
      "Eleftheria Panagiotaki"
    ],
    "published": "2025-12-02T19:49:49+00:00",
    "url": "https://arxiv.org/pdf/2512.03196v1",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03195v1",
    "title": "Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies",
    "abstract": "This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier",
    "authors": [
      "Stylianos Saroglou",
      "Konstantinos Diamantaras",
      "Francesco Preta",
      "Marina Delianidi",
      "Apostolos Benisis",
      "Christian Johannes Meyer"
    ],
    "published": "2025-12-02T19:49:43+00:00",
    "url": "https://arxiv.org/pdf/2512.03195v1",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03182v1",
    "title": "Drainage: A Unifying Framework for Addressing Class Uncertainty",
    "abstract": "Modern deep learning faces significant challenges with noisy labels, class ambiguity, as well as the need to robustly reject out-of-distribution or corrupted samples. In this work, we propose a unified framework based on the concept of a \"drainage node'' which we add at the output of the network. The node serves to reallocate probability mass toward uncertainty, while preserving desirable properties such as end-to-end training and differentiability. This mechanism provides a natural escape route for highly ambiguous, anomalous, or noisy samples, particularly relevant for instance-dependent and asymmetric label noise. In systematic experiments involving the addition of varying proportions of instance-dependent noise or asymmetric noise to CIFAR-10/100 labels, our drainage formulation achieves an accuracy increase of up to 9\\% over existing approaches in the high-noise regime. Our results on real-world datasets, such as mini-WebVision, mini-ImageNet and Clothing-1M, match or surpass existing state-of-the-art methods. Qualitative analysis reveals a denoising effect, where the drainage neuron consistently absorbs corrupt, mislabeled, or outlier data, leading to more stable decision boundaries. Furthermore, our drainage formulation enables applications well beyond classification, with immediate benefits for web-scale, semi-supervised dataset cleaning, and open-set applications.",
    "authors": [
      "Yasser Taha",
      "Gr\u00e9goire Montavon",
      "Nils K\u00f6rber"
    ],
    "published": "2025-12-02T19:31:01+00:00",
    "url": "https://arxiv.org/pdf/2512.03182v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03176v1",
    "title": "Plantain: Plan-Answer Interleaved Reasoning",
    "abstract": "Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard \"think-then-answer\" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.",
    "authors": [
      "Anthony Liang",
      "Jonathan Berant",
      "Adam Fisch",
      "Abhimanyu Goyal",
      "Kalpesh Krishna",
      "Jacob Eisenstein"
    ],
    "published": "2025-12-02T19:22:12+00:00",
    "url": "https://arxiv.org/pdf/2512.03176v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03173v1",
    "title": "Culture Affordance Atlas: Reconciling Object Diversity Through Functional Mapping",
    "abstract": "Culture shapes the objects people use and for what purposes, yet mainstream Vision-Language (VL) datasets frequently exhibit cultural biases, disproportionately favoring higher-income, Western contexts. This imbalance reduces model generalizability and perpetuates performance disparities, especially impacting lower-income and non-Western communities. To address these disparities, we propose a novel function-centric framework that categorizes objects by the functions they fulfill, across diverse cultural and economic contexts. We implement this framework by creating the Culture Affordance Atlas, a re-annotated and culturally grounded restructuring of the Dollar Street dataset spanning 46 functions and 288 objects publicly available at https://lit.eecs.umich.edu/CultureAffordance-Atlas/index.html. Through extensive empirical analyses using the CLIP model, we demonstrate that function-centric labels substantially reduce socioeconomic performance gaps between high- and low-income groups by a median of 6 pp (statistically significant), improving model effectiveness for lower-income contexts. Furthermore, our analyses reveals numerous culturally essential objects that are frequently overlooked in prominent VL datasets. Our contributions offer a scalable pathway toward building inclusive VL datasets and equitable AI systems.",
    "authors": [
      "Joan Nwatu",
      "Longju Bai",
      "Oana Ignat",
      "Rada Mihalcea"
    ],
    "published": "2025-12-02T19:16:39+00:00",
    "url": "https://arxiv.org/pdf/2512.03173v1",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03166v1",
    "title": "Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments",
    "abstract": "The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.",
    "authors": [
      "Aya Taourirte",
      "Md Sohag Mia"
    ],
    "published": "2025-12-02T19:11:44+00:00",
    "url": "https://arxiv.org/pdf/2512.03166v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03046v1",
    "title": "MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues",
    "abstract": "We propose MagicQuill V2, a novel system that introduces a \\textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.",
    "authors": [
      "Zichen Liu",
      "Yue Yu",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Shuailei Ma",
      "Ka Leong Cheng",
      "Wen Wang",
      "Qingyan Bai",
      "Yuxuan Zhang",
      "Yanhong Zeng",
      "Yixuan Li",
      "Xing Zhu",
      "Yujun Shen",
      "Qifeng Chen"
    ],
    "published": "2025-12-02T18:59:58+00:00",
    "url": "https://arxiv.org/pdf/2512.03046v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03045v1",
    "title": "CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models",
    "abstract": "Multi-view diffusion models have recently emerged as a powerful paradigm for novel view synthesis, yet the underlying mechanism that enables their view-consistency remains unclear. In this work, we first verify that the attention maps of these models acquire geometric correspondence throughout training, attending to the geometrically corresponding regions across reference and target views for view-consistent generation. However, this correspondence signal remains incomplete, with its accuracy degrading under large viewpoint changes. Building on these findings, we introduce CAMEO, a simple yet effective training technique that directly supervises attention maps using geometric correspondence to enhance both the training efficiency and generation quality of multi-view diffusion models. Notably, supervising a single attention layer is sufficient to guide the model toward learning precise correspondences, thereby preserving the geometry and structure of reference images, accelerating convergence, and improving novel view synthesis performance. CAMEO reduces the number of training iterations required for convergence by half while achieving superior performance at the same iteration counts. We further demonstrate that CAMEO is model-agnostic and can be applied to any multi-view diffusion model.",
    "authors": [
      "Minkyung Kwon",
      "Jinhyeok Choi",
      "Jiho Park",
      "Seonghu Jeon",
      "Jinhyuk Jang",
      "Junyoung Seo",
      "Minseop Kwak",
      "Jin-Hwa Kim",
      "Seungryong Kim"
    ],
    "published": "2025-12-02T18:59:57+00:00",
    "url": "https://arxiv.org/pdf/2512.03045v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03043v2",
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "abstract": "Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.",
    "authors": [
      "Kaituo Feng",
      "Manyuan Zhang",
      "Hongyu Li",
      "Kaixuan Fan",
      "Shuang Chen",
      "Yilei Jiang",
      "Dian Zheng",
      "Peiwen Sun",
      "Yiyuan Zhang",
      "Haoze Sun",
      "Yan Feng",
      "Peng Pei",
      "Xunliang Cai",
      "Xiangyu Yue"
    ],
    "published": "2025-12-02T18:59:52+00:00",
    "url": "https://arxiv.org/pdf/2512.03043v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03042v1",
    "title": "PPTArena: A Benchmark for Agentic PowerPoint Editing",
    "abstract": "We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.",
    "authors": [
      "Michael Ofengenden",
      "Yunze Man",
      "Ziqi Pang",
      "Yu-Xiong Wang"
    ],
    "published": "2025-12-02T18:59:50+00:00",
    "url": "https://arxiv.org/pdf/2512.03042v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03041v1",
    "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
    "abstract": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
    "authors": [
      "Qinghe Wang",
      "Xiaoyu Shi",
      "Baolu Li",
      "Weikang Bian",
      "Quande Liu",
      "Huchuan Lu",
      "Xintao Wang",
      "Pengfei Wan",
      "Kun Gai",
      "Xu Jia"
    ],
    "published": "2025-12-02T18:59:48+00:00",
    "url": "https://arxiv.org/pdf/2512.03041v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03040v1",
    "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
    "abstract": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
    "authors": [
      "Zeqi Xiao",
      "Yiwei Zhao",
      "Lingxiao Li",
      "Yushi Lan",
      "Yu Ning",
      "Rahul Garg",
      "Roshni Cooper",
      "Mohammad H. Taghavi",
      "Xingang Pan"
    ],
    "published": "2025-12-02T18:59:44+00:00",
    "url": "https://arxiv.org/pdf/2512.03040v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03127v1",
    "title": "Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra",
    "abstract": "Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.",
    "authors": [
      "Ziyu Xiong",
      "Yichi Zhang",
      "Foyez Alauddin",
      "Chu Xin Cheng",
      "Joon Soo An",
      "Mohammad R. Seyedsayamdost",
      "Ellen D. Zhong"
    ],
    "published": "2025-12-02T18:59:13+00:00",
    "url": "https://arxiv.org/pdf/2512.03127v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph"
    ]
  },
  {
    "arxiv_id": "2512.03036v1",
    "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
    "abstract": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.",
    "authors": [
      "Mengchen Zhang",
      "Qi Chen",
      "Tong Wu",
      "Zihan Liu",
      "Dahua Lin"
    ],
    "published": "2025-12-02T18:56:12+00:00",
    "url": "https://arxiv.org/pdf/2512.03036v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03034v1",
    "title": "MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation",
    "abstract": "We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation. Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion. To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions. The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.",
    "authors": [
      "Youxin Pang",
      "Jiajun Liu",
      "Lingfeng Tan",
      "Yong Zhang",
      "Feng Gao",
      "Xiang Deng",
      "Zhuoliang Kang",
      "Xiaoming Wei",
      "Yebin Liu"
    ],
    "published": "2025-12-02T18:55:53+00:00",
    "url": "https://arxiv.org/pdf/2512.03034v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03028v2",
    "title": "SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control",
    "abstract": "Data-driven motion priors that can guide agents toward producing naturalistic behaviors play a pivotal role in creating life-like virtual characters. Adversarial imitation learning has been a highly effective method for learning motion priors from reference motion data. However, adversarial priors, with few exceptions, need to be retrained for each new controller, thereby limiting their reusability and necessitating the retention of the reference motion data when training on downstream tasks. In this work, we present Score-Matching Motion Priors (SMP), which leverages pre-trained motion diffusion models and score distillation sampling (SDS) to create reusable task-agnostic motion priors. SMPs can be pre-trained on a motion dataset, independent of any control policy or task. Once trained, SMPs can be kept frozen and reused as general-purpose reward functions to train policies to produce naturalistic behaviors for downstream tasks. We show that a general motion prior trained on large-scale datasets can be repurposed into a variety of style-specific priors. Furthermore SMP can compose different styles to synthesize new styles not present in the original dataset. Our method produces high-quality motion comparable to state-of-the-art adversarial imitation learning methods through reusable and modular motion priors. We demonstrate the effectiveness of SMP across a diverse suite of control tasks with physically simulated humanoid characters. Video demo available at https://youtu.be/ravlZJteS20",
    "authors": [
      "Yuxuan Mu",
      "Ziyu Zhang",
      "Yi Shi",
      "Minami Matsumoto",
      "Kotaro Imamura",
      "Guy Tevet",
      "Chuan Guo",
      "Michael Taylor",
      "Chang Shu",
      "Pengcheng Xi",
      "Xue Bin Peng"
    ],
    "published": "2025-12-02T18:54:12+00:00",
    "url": "https://arxiv.org/pdf/2512.03028v2",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.03026v1",
    "title": "The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models",
    "abstract": "The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.",
    "authors": [
      "Saeid Jamshidi",
      "Kawser Wazed Nafi",
      "Arghavan Moradi Dakhel",
      "Negar Shahabi",
      "Foutse Khomh"
    ],
    "published": "2025-12-02T18:52:29+00:00",
    "url": "https://arxiv.org/pdf/2512.03026v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03025v1",
    "title": "LORE: A Large Generative Model for Search Relevance",
    "abstract": "Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains.",
    "authors": [
      "Chenji Lu",
      "Zhuo Chen",
      "Hui Zhao",
      "Zhiyuan Zeng",
      "Gang Zhao",
      "Junjie Ren",
      "Ruicong Xu",
      "Haoran Li",
      "Songyan Liu",
      "Pengjie Wang",
      "Jian Xu",
      "Bo Zheng"
    ],
    "published": "2025-12-02T18:50:42+00:00",
    "url": "https://arxiv.org/pdf/2512.03025v1",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03024v1",
    "title": "TokenPowerBench: Benchmarking the Power Consumption of LLM Inference",
    "abstract": "Large language model (LLM) services now answer billions of queries per day, and industry reports show that inference, not training, accounts for more than 90% of total power consumption. However, existing benchmarks focus on either training/fine-tuning or performance of inference and provide little support for power consumption measurement and analysis of inference. We introduce TokenPowerBench, the first lightweight and extensible benchmark designed for LLM-inference power consumption studies. The benchmark combines (i) a declarative configuration interface covering model choice, prompt set, and inference engine, (ii) a measurement layer that captures GPU-, node-, and system-level power without specialized power meters, and (iii) a phase-aligned metrics pipeline that attributes energy to the prefill and decode stages of every request. These elements make it straight-forward to explore the power consumed by an LLM inference run; furthermore, by varying batch size, context length, parallelism strategy and quantization, users can quickly assess how each setting affects joules per token and other energy-efficiency metrics. We evaluate TokenPowerBench on four of the most widely used model series (Llama, Falcon, Qwen, and Mistral). Our experiments cover from 1 billion parameters up to the frontier-scale Llama3-405B model. Furthermore, we release TokenPowerBench as open source to help users to measure power consumption, forecast operating expenses, and meet sustainability targets when deploying LLM services.",
    "authors": [
      "Chenxu Niu",
      "Wei Zhang",
      "Jie Li",
      "Yongjian Zhao",
      "Tongyang Wang",
      "Xi Wang",
      "Yong Chen"
    ],
    "published": "2025-12-02T18:50:17+00:00",
    "url": "https://arxiv.org/pdf/2512.03024v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DC"
    ]
  },
  {
    "arxiv_id": "2512.03020v1",
    "title": "Unrolled Networks are Conditional Probability Flows in MRI Reconstruction",
    "abstract": "Magnetic Resonance Imaging (MRI) offers excellent soft-tissue contrast without ionizing radiation, but its long acquisition time limits clinical utility. Recent methods accelerate MRI by under-sampling $k$-space and reconstructing the resulting images using deep learning. Unrolled networks have been widely used for the reconstruction task due to their efficiency, but suffer from unstable evolving caused by freely-learnable parameters in intermediate steps. In contrast, diffusion models based on stochastic differential equations offer theoretical stability in both medical and natural image tasks but are computationally expensive. In this work, we introduce flow ODEs to MRI reconstruction by theoretically proving that unrolled networks are discrete implementations of conditional probability flow ODEs. This connection provides explicit formulations for parameters and clarifies how intermediate states should evolve. Building on this insight, we propose Flow-Aligned Training (FLAT), which derives unrolled parameters from the ODE discretization and aligns intermediate reconstructions with the ideal ODE trajectory to improve stability and convergence. Experiments on three MRI datasets show that FLAT achieves high-quality reconstructions with up to $3\\times$ fewer iterations than diffusion-based generative models and significantly greater stability than unrolled networks.",
    "authors": [
      "Kehan Qi",
      "Saumya Gupta",
      "Qingqiao Hu",
      "Weimin Lyu",
      "Chao Chen"
    ],
    "published": "2025-12-02T18:48:10+00:00",
    "url": "https://arxiv.org/pdf/2512.03020v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03019v1",
    "title": "Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge",
    "abstract": "Thinking Large Language Models (LLMs) used as judges for pairwise preferences remain noisy at the single-sample level, and common aggregation rules (majority vote, soft self-consistency, or instruction-based self-aggregation) are inconsistent when ties are allowed. We study inference-time compute (ITC) for evaluators that generate n independent thinking-rating samples per item, and propose a principled, distribution-calibrated aggregation scheme. Our method models three-way preferences with a Bradley-Terry-Davidson formulation on rating counts, leveraging both polarity (margin among non-ties) and decisiveness (non-tie rate) to distinguish narrow margins from strong consensus. Across various evaluation benchmarks, our approach consistently reduces MAE and increases pairwise accuracy versus standard baselines, and when evaluated against human-consensus meta-labels, matches or exceeds individual human raters. These results show that carefully allocating ITC and aggregating with distribution-aware methods turns noisy individual model judgments into reliable ratings for evaluation.",
    "authors": [
      "Hamid Dadkhahi",
      "Firas Trabelsi",
      "Parker Riley",
      "Juraj Juraska",
      "Mehdi Mirzazadeh"
    ],
    "published": "2025-12-02T18:46:47+00:00",
    "url": "https://arxiv.org/pdf/2512.03019v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03126v1",
    "title": "Hierarchical Process Reward Models are Symbolic Vision Learners",
    "abstract": "Symbolic computer vision represents diagrams through explicit logical rules and structured representations, enabling interpretable understanding in machine vision. This requires fundamentally different learning paradigms from pixel-based visual models. Symbolic visual learners parse diagrams into geometric primitives-points, lines, and shapes-whereas pixel-based learners operate on textures and colors. We propose a novel self-supervised symbolic auto-encoder that encodes diagrams into structured primitives and their interrelationships within the latent space, and decodes them through our executable engine to reconstruct the input diagrams. Central to this architecture is Symbolic Hierarchical Process Reward Modeling, which applies hierarchical step-level parsing rewards to enforce point-on-line, line-on-shape, and shape-on-relation consistency. Since vanilla reinforcement learning exhibits poor exploration in the policy space during diagram reconstruction; we thus introduce stabilization mechanisms to balance exploration and exploitation. We fine-tune our symbolic encoder on downstream tasks, developing a neuro-symbolic system that integrates the reasoning capabilities of neural networks with the interpretability of symbolic models through reasoning-grounded visual rewards. Evaluations across reconstruction, perception, and reasoning tasks demonstrate the effectiveness of our approach: achieving a 98.2% reduction in MSE for geometric diagram reconstruction, surpassing GPT-4o by 0.6% with a 7B model on chart reconstruction, and improving by +13% on the MathGlance perception benchmark, and by +3% on MathVerse and GeoQA reasoning benchmarks.",
    "authors": [
      "Shan Zhang",
      "Aotian Chen",
      "Kai Zou",
      "Jindong Gu",
      "Yuan Xue",
      "Anton van den Hengel"
    ],
    "published": "2025-12-02T18:46:40+00:00",
    "url": "https://arxiv.org/pdf/2512.03126v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03018v1",
    "title": "AutoBrep: Autoregressive B-Rep Generation with Unified Topology and Geometry",
    "abstract": "The boundary representation (B-Rep) is the standard data structure used in Computer-Aided Design (CAD) for defining solid models. Despite recent progress, directly generating B-Reps end-to-end with precise geometry and watertight topology remains a challenge. This paper presents AutoBrep, a novel Transformer model that autoregressively generates B-Reps with high quality and validity. AutoBrep employs a unified tokenization scheme that encodes both geometric and topological characteristics of a B-Rep model as a sequence of discrete tokens. Geometric primitives (i.e., surfaces and curves) are encoded as latent geometry tokens, and their structural relationships are defined as special topological reference tokens. Sequence order in AutoBrep naturally follows a breadth first traversal of the B-Rep face adjacency graph. At inference time, neighboring faces and edges along with their topological structure are progressively generated. Extensive experiments demonstrate the advantages of our unified representation when coupled with next-token prediction for B-Rep generation. AutoBrep outperforms baselines with better quality and watertightness. It is also highly scalable to complex solids with good fidelity and inference speed. We further show that autocompleting B-Reps is natively supported through our unified tokenization, enabling user-controllable CAD generation with minimal changes. Code is available at https://github.com/AutodeskAILab/AutoBrep.",
    "authors": [
      "Xiang Xu",
      "Pradeep Kumar Jayaraman",
      "Joseph G. Lambourne",
      "Yilin Liu",
      "Durvesh Malpure",
      "Pete Meltzer"
    ],
    "published": "2025-12-02T18:46:25+00:00",
    "url": "https://arxiv.org/pdf/2512.03018v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03014v1",
    "title": "Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks",
    "abstract": "When applied sequentially to video, frame-based networks often exhibit temporal inconsistency - for example, outputs that flicker between frames. This problem is amplified when the network inputs contain time-varying corruptions. In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video. We describe a class of stability adapters that can be inserted into virtually any architecture and a resource-efficient training process that can be performed with a frozen base network. We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss. By analyzing the theoretical properties of this loss, we identify the conditions where it produces well-behaved stabilizer training. Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+). Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions.",
    "authors": [
      "Matthew Dutson",
      "Nathan Labiosa",
      "Yin Li",
      "Mohit Gupta"
    ],
    "published": "2025-12-02T18:41:10+00:00",
    "url": "https://arxiv.org/pdf/2512.03014v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03013v1",
    "title": "In-Context Sync-LoRA for Portrait Video Editing",
    "abstract": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
    "authors": [
      "Sagi Polaczek",
      "Or Patashnik",
      "Ali Mahdavi-Amiri",
      "Daniel Cohen-Or"
    ],
    "published": "2025-12-02T18:40:35+00:00",
    "url": "https://arxiv.org/pdf/2512.03013v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ]
  },
  {
    "arxiv_id": "2512.03125v1",
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "abstract": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git",
    "authors": [
      "Xiwen Wei",
      "Mustafa Munir",
      "Radu Marculescu"
    ],
    "published": "2025-12-02T18:36:26+00:00",
    "url": "https://arxiv.org/pdf/2512.03125v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03010v1",
    "title": "SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting",
    "abstract": "LiDAR-captured point clouds are often considered the gold standard in active 3D reconstruction. While their accuracy is exceptional in flat regions, the capturing is susceptible to miss small geometric structures and may fail with dark, absorbent materials. Alternatively, capturing multiple photos of the scene and applying 3D photogrammetry can infer these details as they often represent feature-rich regions. However, the accuracy of LiDAR for featureless regions is rarely reached. Therefore, we suggest combining the strengths of LiDAR and camera-based capture by introducing SurfFill: a Gaussian surfel-based LiDAR completion scheme. We analyze LiDAR capturings and attribute LiDAR beam divergence as a main factor for artifacts, manifesting mostly at thin structures and edges. We use this insight to introduce an ambiguity heuristic for completed scans by evaluating the change in density in the point cloud. This allows us to identify points close to missed areas, which we can then use to grow additional points from to complete the scan. For this point growing, we constrain Gaussian surfel reconstruction [Huang et al. 2024] to focus optimization and densification on these ambiguous areas. Finally, Gaussian primitives of the reconstruction in ambiguous areas are extracted and sampled for points to complete the point cloud. To address the challenges of large-scale reconstruction, we extend this pipeline with a divide-and-conquer scheme for building-sized point cloud completion. We evaluate on the task of LiDAR point cloud completion of synthetic and real-world scenes and find that our method outperforms previous reconstruction methods.",
    "authors": [
      "Svenja Strobel",
      "Matthias Innmann",
      "Bernhard Egger",
      "Marc Stamminger",
      "Linus Franke"
    ],
    "published": "2025-12-02T18:35:54+00:00",
    "url": "https://arxiv.org/pdf/2512.03010v1",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.03005v1",
    "title": "From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?",
    "abstract": "The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.",
    "authors": [
      "Dawei Li",
      "Abdullah Alnaibari",
      "Arslan Bisharat",
      "Manny Sandoval",
      "Deborah Hall",
      "Yasin Silva",
      "Huan Liu"
    ],
    "published": "2025-12-02T18:31:18+00:00",
    "url": "https://arxiv.org/pdf/2512.03005v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03004v1",
    "title": "DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images",
    "abstract": "Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \\textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.",
    "authors": [
      "Xiaoxue Chen",
      "Ziyi Xiong",
      "Yuantao Chen",
      "Gen Li",
      "Nan Wang",
      "Hongcheng Luo",
      "Long Chen",
      "Haiyang Sun",
      "Bing Wang",
      "Guang Chen",
      "Hangjun Ye",
      "Hongyang Li",
      "Ya-Qin Zhang",
      "Hao Zhao"
    ],
    "published": "2025-12-02T18:29:18+00:00",
    "url": "https://arxiv.org/pdf/2512.03004v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03001v1",
    "title": "Invasive Context Engineering to Control Large Language Models",
    "abstract": "Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.",
    "authors": [
      "Thomas Rivasseau"
    ],
    "published": "2025-12-02T18:25:55+00:00",
    "url": "https://arxiv.org/pdf/2512.03001v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03000v2",
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
    "authors": [
      "Kairun Wen",
      "Yuzhi Huang",
      "Runyu Chen",
      "Hui Zheng",
      "Yunlong Lin",
      "Panwang Pan",
      "Chenxin Li",
      "Wenyan Cong",
      "Jian Zhang",
      "Junbin Lu",
      "Chenguo Lin",
      "Dilin Wang",
      "Zhicheng Yan",
      "Hongyu Xu",
      "Justin Theiss",
      "Yue Huang",
      "Xinghao Ding",
      "Rakesh Ranjan",
      "Zhiwen Fan"
    ],
    "published": "2025-12-02T18:24:27+00:00",
    "url": "https://arxiv.org/pdf/2512.03000v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02993v1",
    "title": "TEXTRIX: Latent Attribute Grid for Native Texture Generation and Beyond",
    "abstract": "Prevailing 3D texture generation methods, which often rely on multi-view fusion, are frequently hindered by inter-view inconsistencies and incomplete coverage of complex surfaces, limiting the fidelity and completeness of the generated content. To overcome these challenges, we introduce TEXTRIX, a native 3D attribute generation framework for high-fidelity texture synthesis and downstream applications such as precise 3D part segmentation. Our approach constructs a latent 3D attribute grid and leverages a Diffusion Transformer equipped with sparse attention, enabling direct coloring of 3D models in volumetric space and fundamentally avoiding the limitations of multi-view fusion. Built upon this native representation, the framework naturally extends to high-precision 3D segmentation by training the same architecture to predict semantic attributes on the grid. Extensive experiments demonstrate state-of-the-art performance on both tasks, producing seamless, high-fidelity textures and accurate 3D part segmentation with precise boundaries.",
    "authors": [
      "Yifei Zeng",
      "Yajie Bao",
      "Jiachen Qian",
      "Shuang Wu",
      "Youtian Lin",
      "Hao Zhu",
      "Buyu Li",
      "Feihu Zhang",
      "Xun Cao",
      "Yao Yao"
    ],
    "published": "2025-12-02T18:18:20+00:00",
    "url": "https://arxiv.org/pdf/2512.02993v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02991v1",
    "title": "GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection",
    "abstract": "Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\\% AP$_{25}$ and 51.2\\% AP$_{50}$) and ScanNetV2 (75.1\\% AP$_{25}$ and 60.8\\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.",
    "authors": [
      "Md Sohag Mia",
      "Md Nahid Hasan",
      "Tawhid Ahmed",
      "Muhammad Abdullah Adnan"
    ],
    "published": "2025-12-02T18:05:02+00:00",
    "url": "https://arxiv.org/pdf/2512.02991v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02987v1",
    "title": "Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic",
    "abstract": "Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.",
    "authors": [
      "Muyu Pan",
      "Dheeraj Kodakandla",
      "Mahfuza Farooque"
    ],
    "published": "2025-12-02T18:03:06+00:00",
    "url": "https://arxiv.org/pdf/2512.02987v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02982v1",
    "title": "U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences",
    "abstract": "Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a \"hard-to-easy\" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation.",
    "authors": [
      "Xiang Xu",
      "Ao Liang",
      "Youquan Liu",
      "Linfeng Li",
      "Lingdong Kong",
      "Ziwei Liu",
      "Qingshan Liu"
    ],
    "published": "2025-12-02T17:59:57+00:00",
    "url": "https://arxiv.org/pdf/2512.02982v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.02981v1",
    "title": "InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration",
    "abstract": "Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness.",
    "authors": [
      "Zhongyu Yang",
      "Yingfang Yuan",
      "Xuanming Jiang",
      "Baoyi An",
      "Wei Pang"
    ],
    "published": "2025-12-02T17:59:52+00:00",
    "url": "https://arxiv.org/pdf/2512.02981v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02978v1",
    "title": "Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding",
    "abstract": "Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique.",
    "authors": [
      "Paul Barbaste",
      "Olivier Oullier",
      "Xavier Vasques"
    ],
    "published": "2025-12-02T17:56:46+00:00",
    "url": "https://arxiv.org/pdf/2512.02978v1",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02973v1",
    "title": "Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities",
    "abstract": "While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\\% and 91.07\\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.",
    "authors": [
      "Yuan Xiong",
      "Ziqi Miao",
      "Lijun Li",
      "Chen Qian",
      "Jie Li",
      "Jing Shao"
    ],
    "published": "2025-12-02T17:51:02+00:00",
    "url": "https://arxiv.org/pdf/2512.02973v1",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "arxiv_id": "2512.02972v1",
    "title": "BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection",
    "abstract": "Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.",
    "authors": [
      "Guowen Zhang",
      "Chenhang He",
      "Liyi Chen",
      "Lei Zhang"
    ],
    "published": "2025-12-02T17:50:33+00:00",
    "url": "https://arxiv.org/pdf/2512.02972v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.02966v1",
    "title": "Lumos: Let there be Language Model System Certification",
    "abstract": "We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.",
    "authors": [
      "Isha Chaudhary",
      "Vedaant Jain",
      "Avaljot Singh",
      "Kavya Sachdeva",
      "Sayan Ranu",
      "Gagandeep Singh"
    ],
    "published": "2025-12-02T17:44:47+00:00",
    "url": "https://arxiv.org/pdf/2512.02966v1",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.MA"
    ]
  },
  {
    "arxiv_id": "2512.02965v1",
    "title": "A Lightweight Real-Time Low-Light Enhancement Network for Embedded Automotive Vision Systems",
    "abstract": "In low-light environments like nighttime driving, image degradation severely challenges in-vehicle camera safety. Since existing enhancement algorithms are often too computationally intensive for vehicular applications, we propose UltraFast-LieNET, a lightweight multi-scale shifted convolutional network for real-time low-light image enhancement. We introduce a Dynamic Shifted Convolution (DSConv) kernel with only 12 learnable parameters for efficient feature extraction. By integrating DSConv with varying shift distances, a Multi-scale Shifted Residual Block (MSRB) is constructed to significantly expand the receptive field. To mitigate lightweight network instability, a residual structure and a novel multi-level gradient-aware loss function are incorporated. UltraFast-LieNET allows flexible parameter configuration, with a minimum size of only 36 parameters. Results on the LOLI-Street dataset show a PSNR of 26.51 dB, outperforming state-of-the-art methods by 4.6 dB while utilizing only 180 parameters. Experiments across four benchmark datasets validate its superior balance of real-time performance and enhancement quality under limited resources. Code is available at https://githubhttps://github.com/YuhanChen2024/UltraFast-LiNET",
    "authors": [
      "Yuhan Chen",
      "Yicui Shi",
      "Guofa Li",
      "Guangrui Bai",
      "Jinyuan Shao",
      "Xiangfei Huang",
      "Wenbo Chu",
      "Keqiang Li"
    ],
    "published": "2025-12-02T17:44:25+00:00",
    "url": "https://arxiv.org/pdf/2512.02965v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02952v1",
    "title": "Layout Anything: One Transformer for Universal Room Layout Estimation",
    "abstract": "We present Layout Anything, a transformer-based framework for indoor layout estimation that adapts the OneFormer's universal segmentation architecture to geometric structure prediction. Our approach integrates OneFormer's task-conditioned queries and contrastive learning with two key modules: (1) a layout degeneration strategy that augments training data while preserving Manhattan-world constraints through topology-aware transformations, and (2) differentiable geometric losses that directly enforce planar consistency and sharp boundary predictions during training. By unifying these components in an end-to-end framework, the model eliminates complex post-processing pipelines while achieving high-speed inference at 114ms. Extensive experiments demonstrate state-of-the-art performance across standard benchmarks, with pixel error (PE) of 5.43% and corner error (CE) of 4.02% on the LSUN, PE of 7.04% (CE 5.17%) on the Hedau and PE of 4.03% (CE 3.15%) on the Matterport3D-Layout datasets. The framework's combination of geometric awareness and computational efficiency makes it particularly suitable for augmented reality applications and large-scale 3D scene reconstruction tasks.",
    "authors": [
      "Md Sohag Mia",
      "Muhammad Abdullah Adnan"
    ],
    "published": "2025-12-02T17:28:03+00:00",
    "url": "https://arxiv.org/pdf/2512.02952v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02942v1",
    "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
    "abstract": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.",
    "authors": [
      "Lanxiang Hu",
      "Abhilash Shankarampeta",
      "Yixin Huang",
      "Zilin Dai",
      "Haoyang Yu",
      "Yujie Zhao",
      "Haoqiang Kang",
      "Daniel Zhao",
      "Tajana Rosing",
      "Hao Zhang"
    ],
    "published": "2025-12-02T17:11:23+00:00",
    "url": "https://arxiv.org/pdf/2512.02942v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02933v2",
    "title": "LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization",
    "abstract": "Text-guided video editing, particularly for object removal and addition, remains a challenging task due to the need for precise spatial and temporal consistency. Existing methods often rely on auxiliary masks or reference images for editing guidance, which limits their scalability and generalization. To address these issues, we propose LoVoRA, a novel framework for mask-free video object removal and addition using object-aware localization mechanism. Our approach utilizes a unique dataset construction pipeline that integrates image-to-video translation, optical flow-based mask propagation, and video inpainting, enabling temporally consistent edits. The core innovation of LoVoRA is its learnable object-aware localization mechanism, which provides dense spatio-temporal supervision for both object insertion and removal tasks. By leveraging a Diffusion Mask Predictor, LoVoRA achieves end-to-end video editing without requiring external control signals during inference. Extensive experiments and human evaluation demonstrate the effectiveness and high-quality performance of LoVoRA. https://cz-5f.github.io/LoVoRA.github.io",
    "authors": [
      "Zhihan Xiao",
      "Lin Liu",
      "Yixin Gao",
      "Xiaopeng Zhang",
      "Haoxuan Che",
      "Songping Mai",
      "Qi Tian"
    ],
    "published": "2025-12-02T17:01:07+00:00",
    "url": "https://arxiv.org/pdf/2512.02933v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02932v1",
    "title": "EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis",
    "abstract": "Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.",
    "authors": [
      "Yancheng Zhang",
      "Guangyu Sun",
      "Chen Chen"
    ],
    "published": "2025-12-02T17:01:00+00:00",
    "url": "https://arxiv.org/pdf/2512.02932v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02931v1",
    "title": "DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation",
    "abstract": "In this paper, we investigate the underexplored challenge of sample diversity in autoregressive (AR) generative models with bitwise visual tokenizers. We first analyze the factors that limit diversity in bitwise AR models and identify two key issues: (1) the binary classification nature of bitwise modeling, which restricts the prediction space, and (2) the overly sharp logits distribution, which causes sampling collapse and reduces diversity. Building on these insights, we propose DiverseAR, a principled and effective method that enhances image diversity without sacrificing visual quality. Specifically, we introduce an adaptive logits distribution scaling mechanism that dynamically adjusts the sharpness of the binary output distribution during sampling, resulting in smoother predictions and greater diversity. To mitigate potential fidelity loss caused by distribution smoothing, we further develop an energy-based generation path search algorithm that avoids sampling low-confidence tokens, thereby preserving high visual quality. Extensive experiments demonstrate that DiverseAR substantially improves sample diversity in bitwise autoregressive image generation.",
    "authors": [
      "Ying Yang",
      "Zhengyao Lv",
      "Tianlin Pan",
      "Haofan Wang",
      "Binxin Yang",
      "Hubery Yin",
      "Chen Li",
      "Chenyang Si"
    ],
    "published": "2025-12-02T16:54:36+00:00",
    "url": "https://arxiv.org/pdf/2512.02931v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02924v1",
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "abstract": "While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.",
    "authors": [
      "Wei Chen",
      "Liangmin Wu",
      "Yunhai Hu",
      "Zhiyuan Li",
      "Zhiyuan Cheng",
      "Yicheng Qian",
      "Lingyue Zhu",
      "Zhipeng Hu",
      "Luoyi Liang",
      "Qiang Tang",
      "Zhen Liu",
      "Han Yang"
    ],
    "published": "2025-12-02T16:45:25+00:00",
    "url": "https://arxiv.org/pdf/2512.02924v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02920v1",
    "title": "Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation",
    "abstract": "We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes. Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings. In this work, we construct a large multimodal dataset across six U.S. states, containing nine million traffic accident records from official sources, and one million high-resolution satellite images for each node of the road network. Additionally, every node is annotated with features such as the region's weather statistics and road type (e.g., residential vs. motorway), and each edge is annotated with traffic volume information (i.e., Average Annual Daily Traffic). Utilizing this dataset, we conduct a comprehensive evaluation of multimodal learning methods that integrate both visual and network embeddings. Our findings show that integrating both data modalities improves prediction accuracy, achieving an average AUROC of $90.1\\%$, which is a $3.7\\%$ gain over graph neural network models that only utilize graph structures. With the improved embeddings, we conduct a causal analysis based on a matching estimator to estimate the key contributing factors influencing traffic accidents. We find that accident rates rise by $24\\%$ under higher precipitation, by $22\\%$ on higher-speed roads such as motorways, and by $29\\%$ due to seasonal patterns, after adjusting for other confounding factors. Ablation studies confirm that satellite imagery features are essential for achieving accurate prediction.",
    "authors": [
      "Ziniu Zhang",
      "Minxuan Duan",
      "Haris N. Koutsopoulos",
      "Hongyang R. Zhang"
    ],
    "published": "2025-12-02T16:39:32+00:00",
    "url": "https://arxiv.org/pdf/2512.02920v1",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.SI"
    ]
  },
  {
    "arxiv_id": "2512.02914v1",
    "title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning",
    "abstract": "Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.",
    "authors": [
      "Zhonghao He",
      "Tianyi Qiu",
      "Hirokazu Shirado",
      "Maarten Sap"
    ],
    "published": "2025-12-02T16:34:05+00:00",
    "url": "https://arxiv.org/pdf/2512.02914v1",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02910v1",
    "title": "In Silico Development of Psychometric Scales: Feasibility of Representative Population Data Simulation with LLMs",
    "abstract": "Developing and validating psychometric scales requires large samples, multiple testing phases, and substantial resources. Recent advances in Large Language Models (LLMs) enable the generation of synthetic participant data by prompting models to answer items while impersonating individuals of specific demographic profiles, potentially allowing in silico piloting before real data collection. Across four preregistered studies (N = circa 300 each), we tested whether LLM-simulated datasets can reproduce the latent structures and measurement properties of human responses. In Studies 1-2, we compared LLM-generated data with real datasets for two validated scales; in Studies 3-4, we created new scales using EFA on simulated data and then examined whether these structures generalized to newly collected human samples. Simulated datasets replicated the intended factor structures in three of four studies and showed consistent configural and metric invariance, with scalar invariance achieved for the two newly developed scales. However, correlation-based tests revealed substantial differences between real and synthetic datasets, and notable discrepancies appeared in score distributions and variances. Thus, while LLMs capture group-level latent structures, they do not approximate individual-level data properties. Simulated datasets also showed full internal invariance across gender. Overall, LLM-generated data appear useful for early-stage, group-level psychometric prototyping, but not as substitutes for individual-level validation. We discuss methodological limitations, risks of bias and data pollution, and ethical considerations related to in silico psychometric simulations.",
    "authors": [
      "Enrico Cipriani",
      "Pavel Okopnyi",
      "Danilo Menicucci",
      "Simone Grassini"
    ],
    "published": "2025-12-02T16:26:17+00:00",
    "url": "https://arxiv.org/pdf/2512.02910v1",
    "categories": [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02906v2",
    "title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding",
    "abstract": "Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.",
    "authors": [
      "Fan Yang",
      "Kaihao Zhang"
    ],
    "published": "2025-12-02T16:22:01+00:00",
    "url": "https://arxiv.org/pdf/2512.02906v2",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ]
  },
  {
    "arxiv_id": "2512.02904v1",
    "title": "Towards a fully differentiable digital twin for solar cells",
    "abstract": "Maximizing energy yield (EY) - the total electric energy generated by a solar cell within a year at a specific location - is crucial in photovoltaics (PV), especially for emerging technologies. Computational methods provide the necessary insights and guidance for future research. However, existing simulations typically focus on only isolated aspects of solar cells. This lack of consistency highlights the need for a framework unifying all computational levels, from material to cell properties, for accurate prediction and optimization of EY prediction. To address this challenge, a differentiable digital twin, Sol(Di)$^2$T, is introduced to enable comprehensive end-to-end optimization of solar cells. The workflow starts with material properties and morphological processing parameters, followed by optical and electrical simulations. Finally, climatic conditions and geographic location are incorporated to predict the EY. Each step is either intrinsically differentiable or replaced with a machine-learned surrogate model, enabling not only accurate EY prediction but also gradient-based optimization with respect to input parameters. Consequently, Sol(Di)$^2$T extends EY predictions to previously unexplored conditions. Demonstrated for an organic solar cell, the proposed framework marks a significant step towards tailoring solar cells for specific applications while ensuring maximal performance.",
    "authors": [
      "Marie Louise Schubert",
      "Houssam Metni",
      "Jan David Fischbach",
      "Benedikt Zerulla",
      "Marjan Krsti\u0107",
      "Ulrich W. Paetzold",
      "Seyedamir Orooji",
      "Olivier J. J. Ronsin",
      "Yasin Ameslon",
      "Jens Harting",
      "Thomas Kirchartz",
      "Sandheep Ravishankar",
      "Chris Dreessen",
      "Eunchi Kim",
      "Christian Sprau",
      "Mohamed Hussein",
      "Alexander Colsmann",
      "Karen Forberich",
      "Klaus J\u00e4ger",
      "Pascal Friederich",
      "Carsten Rockstuhl"
    ],
    "published": "2025-12-02T16:20:58+00:00",
    "url": "https://arxiv.org/pdf/2512.02904v1",
    "categories": [
      "physics.comp-ph",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02902v1",
    "title": "VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling",
    "abstract": "Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.",
    "authors": [
      "Weiqi Li",
      "Quande Zhang",
      "Ruifeng Zhai",
      "Liang Lin",
      "Guangrun Wang"
    ],
    "published": "2025-12-02T16:16:13+00:00",
    "url": "https://arxiv.org/pdf/2512.02902v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02901v2",
    "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in $\\{\\pm 1, \\pm i\\}$",
    "abstract": "Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.",
    "authors": [
      "Feiyu Wang",
      "Xinyu Tan",
      "Bokai Huang",
      "Yihao Zhang",
      "Guoan Wang",
      "Peizhuang Cong",
      "Tong Yang"
    ],
    "published": "2025-12-02T16:14:08+00:00",
    "url": "https://arxiv.org/pdf/2512.02901v2",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02899v1",
    "title": "Glance: Accelerating Diffusion Models with 1 Sample",
    "abstract": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
    "authors": [
      "Zhuobai Dong",
      "Rui Zhao",
      "Songjie Wu",
      "Junchao Yi",
      "Linjie Li",
      "Zhengyuan Yang",
      "Lijuan Wang",
      "Alex Jinpeng Wang"
    ],
    "published": "2025-12-02T16:05:21+00:00",
    "url": "https://arxiv.org/pdf/2512.02899v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02898v1",
    "title": "Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits",
    "abstract": "Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults.   This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).",
    "authors": [
      "Pedro Orvalho",
      "Marta Kwiatkowska",
      "Mikol\u00e1\u0161 Janota",
      "Vasco Manquinho"
    ],
    "published": "2025-12-02T16:04:51+00:00",
    "url": "https://arxiv.org/pdf/2512.02898v1",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LO",
      "cs.SC"
    ]
  },
  {
    "arxiv_id": "2512.02897v1",
    "title": "Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models",
    "abstract": "This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model. We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself. Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy. Experiments with different datasets, including integration into an operational place recognition policy, validate the practical relevance of these findings and demonstrate that carefully designed projections can serve as an effective surrogate for end-to-end 3-D learning in LiDAR place recognition.",
    "authors": [
      "Pierpaolo Serio",
      "Giulio Pisaneschi",
      "Andrea Dan Ryals",
      "Vincenzo Infantino",
      "Lorenzo Gentilini",
      "Valentina Donzella",
      "Lorenzo Pollini"
    ],
    "published": "2025-12-02T16:04:17+00:00",
    "url": "https://arxiv.org/pdf/2512.02897v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.02895v2",
    "title": "MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm",
    "abstract": "We present MindGPT-4ov, a multimodal large language model (MLLM) that introduces a general post-training paradigm spanning data production, model training, and efficient deployment. It achieves state-of-the-art performance across multiple benchmarks at low cost, effectively enhancing the foundational capabilities of MLLMs and the generalization ability. Focusing on data construction, supervised fine-tuning strategies, and multimodal reinforcement learning methods, this work proposes three key innovations: (1) An information density-based data generation scheme, integrated with a dual-dimensional tree-structured label system, enabling automated generation of high-quality cross-domain data. (2) A collaborative curriculum supervised fine-tuning approach that balances the injection of domain-specific knowledge with the preservation of general capabilities. (3) A hybrid reinforcement learning paradigm that enhances reasoning ability while simultaneously addressing multi-objective optimization such as diversity exploration, maintenance of multimodal perception, and response conciseness. Moreover, we implement a series of infrastructure optimizations, such as 5D parallel training, operator optimization, and inference quantization to enhance training and inference efficiency while reducing the cost of domain adaptation. Experimental results demonstrate that the MindGPT-4ov model outperforms state-of-the-art models on benchmarks such as MMBench, MMStar, MathVision, and MathVista. In addition, MindGPT-4ov also demonstrates superior user experience in vertical domain tasks, enabling a seamless transition from academic research to industrial deployment. MindGPT-4ov provides a general post-training paradigm applicable to a wide range of MLLMs. The model weights, datasets, and code for the Qwen3-VL-based variants will be recently open-sourced to support the community's development of MLLMs.",
    "authors": [
      "Wei Chen",
      "Chaoqun Du",
      "Feng Gu",
      "Wei He",
      "Qizhen Li",
      "Zide Liu",
      "Xuhao Pan",
      "Chang Ren",
      "Xudong Rao",
      "Chenfeng Wang",
      "Tao Wei",
      "Chengjun Yu",
      "Pengfei Yu",
      "Yufei Zheng",
      "Chunpeng Zhou",
      "Pan Zhou",
      "Xuhan Zhu"
    ],
    "published": "2025-12-02T16:04:11+00:00",
    "url": "https://arxiv.org/pdf/2512.02895v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02892v1",
    "title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules",
    "abstract": "Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\\times$ speedups while retaining $99.8$-$100\\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\\%$ performance retention, with up to $2.34\\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $\u03b3{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.",
    "authors": [
      "Amr Mohamed",
      "Yang Zhang",
      "Michalis Vazirgiannis",
      "Guokan Shang"
    ],
    "published": "2025-12-02T16:01:08+00:00",
    "url": "https://arxiv.org/pdf/2512.02892v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02882v1",
    "title": "OptPO: Optimal Rollout Allocation for Test-time Policy Optimization",
    "abstract": "Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.",
    "authors": [
      "Youkang Wang",
      "Jian Wang",
      "Rubing Chen",
      "Tianyi Zeng",
      "Xiao-Yong Wei",
      "Qing Li"
    ],
    "published": "2025-12-02T15:38:52+00:00",
    "url": "https://arxiv.org/pdf/2512.02882v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02879v1",
    "title": "The future of AI in critical mineral exploration",
    "abstract": "The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage",
    "authors": [
      "Jef Caers"
    ],
    "published": "2025-12-02T15:37:48+00:00",
    "url": "https://arxiv.org/pdf/2512.02879v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02874v1",
    "title": "Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning",
    "abstract": "Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a \"majority\" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.",
    "authors": [
      "Haonan Wang",
      "Chao Du",
      "Kenji Kawaguchi",
      "Tianyu Pang"
    ],
    "published": "2025-12-02T15:35:31+00:00",
    "url": "https://arxiv.org/pdf/2512.02874v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02870v1",
    "title": "Taming Camera-Controlled Video Generation with Verifiable Geometry Reward",
    "abstract": "Recent advances in video diffusion models have remarkably improved camera-controlled video generation, but most methods rely solely on supervised fine-tuning (SFT), leaving online reinforcement learning (RL) post-training largely underexplored. In this work, we introduce an online RL post-training framework that optimizes a pretrained video generator for precise camera control. To make RL effective in this setting, we design a verifiable geometry reward that delivers dense segment-level feedback to guide model optimization. Specifically, we estimate the 3D camera trajectories for both generated and reference videos, divide each trajectory into short segments, and compute segment-wise relative poses. The reward function then compares each generated-reference segment pair and assigns an alignment score as the reward signal, which helps alleviate reward sparsity and improve optimization efficiency. Moreover, we construct a comprehensive dataset featuring diverse large-amplitude camera motions and scenes with varied subject dynamics. Extensive experiments show that our online RL post-training clearly outperforms SFT baselines across multiple aspects, including camera-control accuracy, geometric consistency, and visual quality, demonstrating its superiority in advancing camera-controlled video generation.",
    "authors": [
      "Zhaoqing Wang",
      "Xiaobo Xia",
      "Zhuolin Bie",
      "Jinlin Liu",
      "Dongdong Yu",
      "Jia-Wang Bian",
      "Changhu Wang"
    ],
    "published": "2025-12-02T15:33:19+00:00",
    "url": "https://arxiv.org/pdf/2512.02870v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02867v1",
    "title": "MICCAI STSR 2025 Challenge: Semi-Supervised Teeth and Pulp Segmentation and CBCT-IOS Registration",
    "abstract": "Cone-Beam Computed Tomography (CBCT) and Intraoral Scanning (IOS) are essential for digital dentistry, but annotated data scarcity limits automated solutions for pulp canal segmentation and cross-modal registration. To benchmark semi-supervised learning (SSL) in this domain, we organized the STSR 2025 Challenge at MICCAI 2025, featuring two tasks: (1) semi-supervised segmentation of teeth and pulp canals in CBCT, and (2) semi-supervised rigid registration of CBCT and IOS. We provided 60 labeled and 640 unlabeled IOS samples, plus 30 labeled and 250 unlabeled CBCT scans with varying resolutions and fields of view. The challenge attracted strong community participation, with top teams submitting open-source deep learning-based SSL solutions. For segmentation, leading methods used nnU-Net and Mamba-like State Space Models with pseudo-labeling and consistency regularization, achieving a Dice score of 0.967 and Instance Affinity of 0.738 on the hidden test set. For registration, effective approaches combined PointNetLK with differentiable SVD and geometric augmentation to handle modality gaps; hybrid neural-classical refinement enabled accurate alignment despite limited labels. All data and code are publicly available at https://github.com/ricoleehduu/STS-Challenge-2025 to ensure reproducibility.",
    "authors": [
      "Yaqi Wang",
      "Zhi Li",
      "Chengyu Wu",
      "Jun Liu",
      "Yifan Zhang",
      "Jialuo Chen",
      "Jiaxue Ni",
      "Qian Luo",
      "Jin Liu",
      "Can Han",
      "Changkai Ji",
      "Zhi Qin Tan",
      "Ajo Babu George",
      "Liangyu Chen",
      "Qianni Zhang",
      "Dahong Qian",
      "Shuai Wang",
      "Huiyu Zhou"
    ],
    "published": "2025-12-02T15:29:04+00:00",
    "url": "https://arxiv.org/pdf/2512.02867v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02860v1",
    "title": "RFOP: Rethinking Fusion and Orthogonal Projection for Face-Voice Association",
    "abstract": "Face-voice association in multilingual environment challenge 2026 aims to investigate the face-voice association task in multilingual scenario. The challenge introduces English-German face-voice pairs to be utilized in the evaluation phase. To this end, we revisit the fusion and orthogonal projection for face-voice association by effectively focusing on the relevant semantic information within the two modalities. Our method performs favorably on the English-German data split and ranked 3rd in the FAME 2026 challenge by achieving the EER of 33.1.",
    "authors": [
      "Abdul Hannan",
      "Furqan Malik",
      "Hina Jabbar",
      "Syed Suleman Sadiq",
      "Mubashir Noman"
    ],
    "published": "2025-12-02T15:21:21+00:00",
    "url": "https://arxiv.org/pdf/2512.02860v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02850v1",
    "title": "Are Detectors Fair to Indian IP-AIGC? A Cross-Generator Study",
    "abstract": "Modern image editors can produce identity-preserving AIGC (IP-AIGC), where the same person appears with new attire, background, or lighting. The robustness and fairness of current detectors in this regime remain unclear, especially for under-represented populations. We present what we believe is the first systematic study of IP-AIGC detection for Indian and South-Asian faces, quantifying cross-generator generalization and intra-population performance. We assemble Indian-focused training splits from FairFD and HAV-DF, and construct two held-out IP-AIGC test sets (HIDF-img-ip-genai and HIDF-vid-ip-genai) using commercial web-UI generators (Gemini and ChatGPT) with identity-preserving prompts. We evaluate two state-of-the-art detectors (AIDE and Effort) under pretrained (PT) and fine-tuned (FT) regimes and report AUC, AP, EER, and accuracy. Fine-tuning yields strong in-domain gains (for example, Effort AUC 0.739 to 0.944 on HAV-DF-test; AIDE EER 0.484 to 0.259), but consistently degrades performance on held-out IP-AIGC for Indian cohorts (for example, AIDE AUC 0.923 to 0.563 on HIDF-img-ip-genai; Effort 0.740 to 0.533), which indicates overfitting to training-generator cues. On non-IP HIDF images, PT performance remains high, which suggests a specific brittleness to identity-preserving edits rather than a generic distribution shift. Our study establishes IP-AIGC-Indian as a challenging and practically relevant scenario and motivates representation-preserving adaptation and India-aware benchmark curation to close generalization gaps in AIGC detection.",
    "authors": [
      "Vishal Dubey",
      "Pallavi Tyagi"
    ],
    "published": "2025-12-02T15:03:51+00:00",
    "url": "https://arxiv.org/pdf/2512.02850v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02849v1",
    "title": "GraphMatch: Fusing Language and Graph Representations in a Dynamic Two-Sided Work Marketplace",
    "abstract": "Recommending matches in a text-rich, dynamic two-sided marketplace presents unique challenges due to evolving content and interaction graphs. We introduce GraphMatch, a new large-scale recommendation framework that fuses pre-trained language models with graph neural networks to overcome these challenges. Unlike prior approaches centered on standalone models, GraphMatch is a comprehensive recipe built on powerful text encoders and GNNs working in tandem. It employs adversarial negative sampling alongside point-in-time subgraph training to learn representations that capture both the fine-grained semantics of evolving text and the time-sensitive structure of the graph. We evaluated extensively on interaction data from Upwork, a leading labor marketplace, at large scale, and discuss our approach towards low-latency inference suitable for real-time use. In our experiments, GraphMatch outperforms language-only and graph-only baselines on matching tasks while being efficient at runtime. These results demonstrate that unifying language and graph representations yields a highly effective solution to text-rich, dynamic two-sided recommendations, bridging the gap between powerful pretrained LMs and large-scale graphs in practice.",
    "authors": [
      "Miko\u0142aj Sacha",
      "Hammad Jafri",
      "Mattie Terzolo",
      "Ayan Sinha",
      "Andrew Rabinovich"
    ],
    "published": "2025-12-02T15:02:10+00:00",
    "url": "https://arxiv.org/pdf/2512.02849v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ]
  },
  {
    "arxiv_id": "2512.02846v1",
    "title": "Action Anticipation at a Glimpse: To What Extent Can Multimodal Cues Replace Video?",
    "abstract": "Anticipating actions before they occur is a core challenge in action understanding research. While conventional methods rely on extracting and aggregating temporal information from videos, as humans we can often predict upcoming actions by observing a single moment from a scene, when given sufficient context. Can a model achieve this competence? The short answer is yes, although its effectiveness depends on the complexity of the task. In this work, we investigate to what extent video aggregation can be replaced with alternative modalities. To this end, based on recent advances in visual feature extraction and language-based reasoning, we introduce AAG, a method for Action Anticipation at a Glimpse. AAG combines RGB features with depth cues from a single frame for enhanced spatial reasoning, and incorporates prior action information to provide long-term context. This context is obtained either through textual summaries from Vision-Language Models, or from predictions generated by a single-frame action recognizer. Our results demonstrate that multimodal single-frame action anticipation using AAG can perform competitively compared to both temporally aggregated video baselines and state-of-the-art methods across three instructional activity datasets: IKEA-ASM, Meccano, and Assembly101.",
    "authors": [
      "Manuel Benavent-Lledo",
      "Konstantinos Bacharidis",
      "Victoria Manousaki",
      "Konstantinos Papoutsakis",
      "Antonis Argyros",
      "Jose Garcia-Rodriguez"
    ],
    "published": "2025-12-02T14:57:17+00:00",
    "url": "https://arxiv.org/pdf/2512.02846v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02845v1",
    "title": "Bangla Hate Speech Classification with Fine-tuned Transformer Models",
    "abstract": "Hate speech recognition in low-resource languages remains a difficult problem due to insufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the growing need for automated moderation on social media platforms, Bangla is significantly under-represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official baselines (e.g., Majority, Random, Support Vector Machine) and also produce and consider Logistic Regression, Random Forest, and Decision Tree as baseline methods. We also utilized transformer-based models such as DistilBERT, BanglaBERT, m-BERT, and XLM-RoBERTa for hate speech classification. All the transformer-based models outperformed baseline methods for the subtasks, except for DistilBERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language-specific pre-training is very important. Our results highlight the potential and need for pre-trained language models for the low-resource Bangla language.",
    "authors": [
      "Yalda Keivan Jafari",
      "Krishno Dey"
    ],
    "published": "2025-12-02T14:56:58+00:00",
    "url": "https://arxiv.org/pdf/2512.02845v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02841v1",
    "title": "Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages",
    "abstract": "System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.",
    "authors": [
      "Lechen Zhang",
      "Yusheng Zhou",
      "Tolga Ergen",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "David Jurgens"
    ],
    "published": "2025-12-02T14:54:54+00:00",
    "url": "https://arxiv.org/pdf/2512.02841v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02840v1",
    "title": "promptolution: A Unified, Modular Framework for Prompt Optimization",
    "abstract": "Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.",
    "authors": [
      "Tom Zehle",
      "Timo Hei\u00df",
      "Moritz Schlager",
      "Matthias A\u00dfenmacher",
      "Matthias Feurer"
    ],
    "published": "2025-12-02T14:53:23+00:00",
    "url": "https://arxiv.org/pdf/2512.02840v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02835v1",
    "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
    "abstract": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .",
    "authors": [
      "Yifan Li",
      "Yingda Yin",
      "Lingting Zhu",
      "Weikai Chen",
      "Shengju Qian",
      "Xin Wang",
      "Yanwei Fu"
    ],
    "published": "2025-12-02T14:44:12+00:00",
    "url": "https://arxiv.org/pdf/2512.02835v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02834v1",
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "abstract": "Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \\textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.",
    "authors": [
      "Siyuan Yang",
      "Yang Zhang",
      "Haoran He",
      "Ling Pan",
      "Xiu Li",
      "Chenjia Bai",
      "Xuelong Li"
    ],
    "published": "2025-12-02T14:42:54+00:00",
    "url": "https://arxiv.org/pdf/2512.02834v1",
    "categories": [
      "cs.RO",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02833v1",
    "title": "A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models",
    "abstract": "We investigate input normalization methods for Time-Series Foundation Models (TSFMs). While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical. Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity. Through systematic evaluation across four architecturally diverse TSFMs, we empirically establish REVIN as the most efficient approach, reducing zero-shot MASE by 89\\% relative to an un-normalized baseline and by 44\\% versus other normalization methods, while matching the best in-domain accuracy (0.84 MASE) without any dataset-level preprocessing -- yielding the highest accuracy-efficiency trade-off. Yet its effect utilization depends on architectural design choices and optimization objective, particularly with respect to training loss scale sensitivity and model type (probabilistic, point-forecast, or LLM-based models).",
    "authors": [
      "Ihab Ahmed",
      "Denis Krompa\u00df",
      "Cheng Feng",
      "Volker Tresp"
    ],
    "published": "2025-12-02T14:39:19+00:00",
    "url": "https://arxiv.org/pdf/2512.02833v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02830v2",
    "title": "Defense That Attacks: How Robust Models Become Better Attackers",
    "abstract": "Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.",
    "authors": [
      "Mohamed Awad",
      "Mahmoud Akrm",
      "Walid Gomaa"
    ],
    "published": "2025-12-02T14:38:09+00:00",
    "url": "https://arxiv.org/pdf/2512.02830v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02826v1",
    "title": "From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity",
    "abstract": "Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.",
    "authors": [
      "Haoming Liu",
      "Jinnuo Liu",
      "Yanhao Li",
      "Liuyang Bai",
      "Yunkai Ji",
      "Yuanhe Guo",
      "Shenji Wan",
      "Hongyi Wen"
    ],
    "published": "2025-12-02T14:34:10+00:00",
    "url": "https://arxiv.org/pdf/2512.02826v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02817v1",
    "title": "BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion",
    "abstract": "The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \\textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\\footnote{All released code and models are licensed under the MIT License.",
    "authors": [
      "Sai Koneru",
      "Fabian Retkowski",
      "Christian Huber",
      "Lukas Hilgert",
      "Seymanur Akti",
      "Enes Yavuz Ugan",
      "Alexander Waibel",
      "Jan Niehues"
    ],
    "published": "2025-12-02T14:27:26+00:00",
    "url": "https://arxiv.org/pdf/2512.02817v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02816v1",
    "title": "A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models",
    "abstract": "The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's \"Syndrome Differentiation and Treatment\" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.",
    "authors": [
      "Kunning Li",
      "Jianbin Guo",
      "Zhaoyang Shang",
      "Yiqing Liu",
      "Hongmin Du",
      "Lingling Liu",
      "Yuping Zhao",
      "Lifeng Dong"
    ],
    "published": "2025-12-02T14:26:44+00:00",
    "url": "https://arxiv.org/pdf/2512.02816v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02814v1",
    "title": "Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control",
    "abstract": "Radiology reporting is an essential yet time-consuming and error-prone task for radiologists in clinical examinations, especially for volumetric medical images. Rigorous quality control is also critical but tedious, ensuring that the final report meets clinical standards. Existing automated approaches, including radiology report generation methods and medical vision-language models, focus mainly on the report generation phase and neglect the crucial quality control procedure, limiting their capability to provide comprehensive support to radiologists. We propose Radiologist Copilot, an agentic AI assistant equipped with orchestrated tools designed for automated radiology reporting with quality control. Leveraging large language models as the reasoning backbone, the agentic system autonomously selects tools, plans, and executes actions, emulating the behavior of radiologists throughout the holistic radiology reporting process. The orchestrated tools include region localization, think with image paradigm directed region analysis planning, strategic template selection for report generation, quality assessment and feedback-driven adaptive refinement for quality control. Therefore, Radiologist Copilot facilitates accurate, complete, and efficient radiology reporting, assisting radiologists and improving clinical efficiency. Experimental results demonstrate that Radiologist Copilot significantly surpasses other state-of-the-art methods in radiology reporting. The source code will be released upon acceptance.",
    "authors": [
      "Yongrui Yu",
      "Zhongzhen Huang",
      "Linjie Mu",
      "Shaoting Zhang",
      "Xiaofan Zhang"
    ],
    "published": "2025-12-02T14:25:05+00:00",
    "url": "https://arxiv.org/pdf/2512.02814v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02812v1",
    "title": "Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents",
    "abstract": "Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\\% and 13\\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.",
    "authors": [
      "Zijie Lin",
      "Qilin Cai",
      "Liang Shen",
      "Mingjun Xiao"
    ],
    "published": "2025-12-02T14:24:23+00:00",
    "url": "https://arxiv.org/pdf/2512.02812v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02810v1",
    "title": "Phase-Adaptive LLM Framework with Multi-Stage Validation for Construction Robot Task Allocation: A Systematic Benchmark Against Traditional Optimization Algorithms",
    "abstract": "Multi-robot task allocation in construction automation has traditionally relied on optimization methods such as Dynamic Programming and Reinforcement Learning. This research introduces the LangGraph-based Task Allocation Agent (LTAA), an LLM-driven framework that integrates phase-adaptive allocation strategies, multi-stage validation with hierarchical retries, and dynamic prompting for efficient robot coordination. Although recent LLM approaches show potential for construction robotics, they largely lack rigorous validation and benchmarking against established algorithms. This paper presents the first systematic comparison of LLM-based task allocation with traditional methods in construction scenarios.The study validates LLM feasibility through SMART-LLM replication and addresses implementation challenges using a Self-Corrective Agent Architecture. LTAA leverages natural-language reasoning combined with structured validation mechanisms, achieving major computational gains reducing token usage by 94.6% and allocation time by 86% through dynamic prompting. The framework adjusts its strategy across phases: emphasizing execution feasibility early and workload balance in later allocations.The authors evaluate LTAA against Dynamic Programming, Q-learning, and Deep Q-Network (DQN) baselines using construction operations from the TEACh human-robot collaboration dataset. In the Heavy Excels setting, where robots have strong task specializations, LTAA achieves 77% task completion with superior workload balance, outperforming all traditional methods. These findings show that LLM-based reasoning with structured validation can match established optimization algorithms while offering additional advantages such as interpretability, adaptability, and the ability to update task logic without retraining.",
    "authors": [
      "Shyam prasad reddy Kaitha",
      "Hongrui Yu"
    ],
    "published": "2025-12-02T14:23:36+00:00",
    "url": "https://arxiv.org/pdf/2512.02810v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02807v1",
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "abstract": "Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.",
    "authors": [
      "Yixuan Tang",
      "Yi Yang"
    ],
    "published": "2025-12-02T14:21:29+00:00",
    "url": "https://arxiv.org/pdf/2512.02807v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02799v1",
    "title": "TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages",
    "abstract": "Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.",
    "authors": [
      "Mike Nkongolo",
      "Hilton Vorster",
      "Josh Warren",
      "Trevor Naick",
      "Deandre Vanmali",
      "Masana Mashapha",
      "Luke Brand",
      "Alyssa Fernandes",
      "Janco Calitz",
      "Sibusiso Makhoba"
    ],
    "published": "2025-12-02T14:16:31+00:00",
    "url": "https://arxiv.org/pdf/2512.02799v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.03121v1",
    "title": "Lost in Modality: Evaluating the Effectiveness of Text-Based Membership Inference Attacks on Large Multimodal Models",
    "abstract": "Large Multimodal Language Models (MLLMs) are emerging as one of the foundational tools in an expanding range of applications. Consequently, understanding training-data leakage in these systems is increasingly critical. Log-probability-based membership inference attacks (MIAs) have become a widely adopted approach for assessing data exposure in large language models (LLMs), yet their effect in MLLMs remains unclear. We present the first comprehensive evaluation of extending these text-based MIA methods to multimodal settings. Our experiments under vision-and-text (V+T) and text-only (T-only) conditions across the DeepSeek-VL and InternVL model families show that in in-distribution settings, logit-based MIAs perform comparably across configurations, with a slight V+T advantage. Conversely, in out-of-distribution settings, visual inputs act as regularizers, effectively masking membership signals.",
    "authors": [
      "Ziyi Tong",
      "Feifei Sun",
      "Le Minh Nguyen"
    ],
    "published": "2025-12-02T14:11:51+00:00",
    "url": "https://arxiv.org/pdf/2512.03121v1",
    "categories": [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02792v1",
    "title": "HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval",
    "abstract": "Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video. The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning. Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality. However, previous works have largely overlooked the disparity in information density between these two modalities. This limitation can lead to two critical issues: 1) modification subject referring ambiguity and 2) limited detailed semantic focus, both of which degrade the performance of CVR models. To address the aforementioned issues, we propose a novel CVR framework, namely the Hierarchical Uncertainty-aware Disambiguation network (HUD). HUD is the first framework that leverages the disparity in information density between video and text to enhance multi-modal query understanding. It comprises three key components: (a) Holistic Pronoun Disambiguation, (b) Atomistic Uncertainty Modeling, and (c) Holistic-to-Atomistic Alignment. By exploiting overlapping semantics through holistic cross-modal interaction and fine-grained semantic alignment via atomistic-level cross-modal interaction, HUD enables effective object disambiguation and enhances the focus on detailed semantics, thereby achieving precise composed feature learning. Moreover, our proposed HUD is also applicable to the Composed Image Retrieval (CIR) task and achieves state-of-the-art performance across three benchmark datasets for both CVR and CIR tasks. The codes are available on https://zivchen-ty.github.io/HUD.github.io/.",
    "authors": [
      "Zhiwei Chen",
      "Yupeng Hu",
      "Zixu Li",
      "Zhiheng Fu",
      "Haokun Wen",
      "Weili Guan"
    ],
    "published": "2025-12-02T14:10:16+00:00",
    "url": "https://arxiv.org/pdf/2512.02792v1",
    "categories": [
      "cs.CV",
      "cs.MM"
    ]
  },
  {
    "arxiv_id": "2512.02791v1",
    "title": "Making Dialogue Grounding Data Rich: A Three-Tier Data Synthesis Framework for Generalized Referring Expression Comprehension",
    "abstract": "Dialogue-Based Generalized Referring Expressions Comprehension (GREC) requires models to ground the expression and unlimited targets in complex visual scenes while resolving coreference across a long dialogue context. However, existing systems struggle under distribution shift between training and evaluation domains, a gap exacerbated by the scarcity of annotated dialogue grounding data. We address this challenge with a three-tier data-synthesis method that balances realism and controllability to produce scalable supervision for dialogue-conditioned grounding. Fine-tuning on the synthesized data yields consistent, substantial improvements over prior approaches across standard evaluation metrics.",
    "authors": [
      "Juexi Shao",
      "Siyou Li",
      "Yujian Gan",
      "Chris Madge",
      "Vanja Karan",
      "Massimo Poesio"
    ],
    "published": "2025-12-02T14:08:47+00:00",
    "url": "https://arxiv.org/pdf/2512.02791v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02789v1",
    "title": "TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking",
    "abstract": "The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.",
    "authors": [
      "Tang Haonan",
      "Chen Yanjun",
      "Jiang Lezhi"
    ],
    "published": "2025-12-02T14:04:30+00:00",
    "url": "https://arxiv.org/pdf/2512.02789v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02787v2",
    "title": "Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols",
    "abstract": "Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/",
    "authors": [
      "Xianchao Zeng",
      "Xinyu Zhou",
      "Youcheng Li",
      "Jiayou Shi",
      "Tianle Li",
      "Liangming Chen",
      "Lei Ren",
      "Yong-Lu Li"
    ],
    "published": "2025-12-02T14:02:42+00:00",
    "url": "https://arxiv.org/pdf/2512.02787v2",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02785v1",
    "title": "Perception of AI-Generated Music -- The Role of Composer Identity, Personality Traits, Music Preferences, and Perceived Humanness",
    "abstract": "The rapid rise of AI-generated art has sparked debate about potential biases in how audiences perceive and evaluate such works. This study investigates how composer information and listener characteristics shape the perception of AI-generated music, adopting a mixed-method approach. Using a diverse set of stimuli across various genres from two AI music models, we examine effects of perceived authorship on liking and emotional responses, and explore how attitudes toward AI, personality traits, and music-related variables influence evaluations. We further assess the influence of perceived humanness and analyze open-ended responses to uncover listener criteria for judging AI-generated music. Attitudes toward AI proved to be the best predictor of both liking and emotional intensity of AI-generated music. This quantitative finding was complemented by qualitative themes from our thematic analysis, which identified ethical, cultural, and contextual considerations as important criteria in listeners' evaluations of AI-generated music. Our results offer a nuanced view of how people experience music created by AI tools and point to key factors and methodological considerations for future research on music perception in human-AI interaction.",
    "authors": [
      "David Stammer",
      "Hannah Strauss",
      "Peter Knees"
    ],
    "published": "2025-12-02T13:59:10+00:00",
    "url": "https://arxiv.org/pdf/2512.02785v1",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.02781v1",
    "title": "LumiX: Structured and Coherent Text-to-Intrinsic Generation",
    "abstract": "We present LumiX, a structured diffusion framework for coherent text-to-intrinsic generation. Conditioned on text prompts, LumiX jointly generates a comprehensive set of intrinsic maps (e.g., albedo, irradiance, normal, depth, and final color), providing a structured and physically consistent description of an underlying scene. This is enabled by two key contributions: 1) Query-Broadcast Attention, a mechanism that ensures structural consistency by sharing queries across all maps in each self-attention block. 2) Tensor LoRA, a tensor-based adaptation that parameter-efficiently models cross-map relations for efficient joint training. Together, these designs enable stable joint diffusion training and unified generation of multiple intrinsic properties. Experiments show that LumiX produces coherent and physically meaningful results, achieving 23% higher alignment and a better preference score (0.19 vs. -0.41) compared to the state of the art, and it can also perform image-conditioned intrinsic decomposition within the same framework.",
    "authors": [
      "Xu Han",
      "Biao Zhang",
      "Xiangjun Tang",
      "Xianzhi Li",
      "Peter Wonka"
    ],
    "published": "2025-12-02T13:56:02+00:00",
    "url": "https://arxiv.org/pdf/2512.02781v1",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02780v1",
    "title": "Rethinking Surgical Smoke: A Smoke-Type-Aware Laparoscopic Video Desmoking Method and Dataset",
    "abstract": "Electrocautery or lasers will inevitably generate surgical smoke, which hinders the visual guidance of laparoscopic videos for surgical procedures. The surgical smoke can be classified into different types based on its motion patterns, leading to distinctive spatio-temporal characteristics across smoky laparoscopic videos. However, existing desmoking methods fail to account for such smoke-type-specific distinctions. Therefore, we propose the first Smoke-Type-Aware Laparoscopic Video Desmoking Network (STANet) by introducing two smoke types: Diffusion Smoke and Ambient Smoke. Specifically, a smoke mask segmentation sub-network is designed to jointly conduct smoke mask and smoke type predictions based on the attention-weighted mask aggregation, while a smokeless video reconstruction sub-network is proposed to perform specially desmoking on smoky features guided by two types of smoke mask. To address the entanglement challenges of two smoke types, we further embed a coarse-to-fine disentanglement module into the mask segmentation sub-network, which yields more accurate disentangled masks through the smoke-type-aware cross attention between non-entangled and entangled regions. In addition, we also construct the first large-scale synthetic video desmoking dataset with smoke type annotations. Extensive experiments demonstrate that our method not only outperforms state-of-the-art approaches in quality evaluations, but also exhibits superior generalization across multiple downstream surgical tasks.",
    "authors": [
      "Qifan Liang",
      "Junlin Li",
      "Zhen Han",
      "Xihao Wang",
      "Zhongyuan Wang",
      "Bin Mei"
    ],
    "published": "2025-12-02T13:55:27+00:00",
    "url": "https://arxiv.org/pdf/2512.02780v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02774v1",
    "title": "AI-Driven Document Redaction in UK Public Authorities: Implementation Gaps, Regulatory Challenges, and the Human Oversight Imperative",
    "abstract": "Document redaction in public authorities faces critical challenges as traditional manual approaches struggle to balance growing transparency demands with increasingly stringent data protection requirements. This study investigates the implementation of AI-driven document redaction within UK public authorities through Freedom of Information (FOI) requests. While AI technologies offer potential solutions to redaction challenges, their actual implementation within public sector organizations remains underexplored. Based on responses from 44 public authorities across healthcare, government, and higher education sectors, this study reveals significant gaps between technological possibilities and organizational realities. Findings show highly limited AI adoption (only one authority reported using AI tools), widespread absence of formal redaction policies (50 percent reported \"information not held\"), and deficiencies in staff training. The study identifies three key barriers to effective AI implementation: poor record-keeping practices, lack of standardized redaction guidelines, and insufficient specialized training for human oversight. These findings highlight the need for a socio-technical approach that balances technological automation with meaningful human expertise. This research provides the first empirical assessment of AI redaction practices in UK public authorities and contributes evidence to support policymakers navigating the complex interplay between transparency obligations, data protection requirements, and emerging AI technologies in public administration.",
    "authors": [
      "Yijun Chen"
    ],
    "published": "2025-12-02T13:52:10+00:00",
    "url": "https://arxiv.org/pdf/2512.02774v1",
    "categories": [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02772v1",
    "title": "Towards Unification of Hallucination Detection and Fact Verification for Large Language Models",
    "abstract": "Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.   We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/",
    "authors": [
      "Weihang Su",
      "Jianming Long",
      "Changyue Wang",
      "Shiyu Lin",
      "Jingyan Xu",
      "Ziyi Ye",
      "Qingyao Ai",
      "Yiqun Liu"
    ],
    "published": "2025-12-02T13:51:01+00:00",
    "url": "https://arxiv.org/pdf/2512.02772v1",
    "categories": [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "arxiv_id": "2512.02764v1",
    "title": "PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory",
    "authors": [
      "Robert Belanec",
      "Ivan Srba",
      "Maria Bielikova"
    ],
    "published": "2025-12-02T13:44:41+00:00",
    "url": "https://arxiv.org/pdf/2512.02764v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02763v1",
    "title": "SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys",
    "abstract": "LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.",
    "authors": [
      "Jiahao Zhao",
      "Shuaixing Zhang",
      "Nan Xu",
      "Lei Wang"
    ],
    "published": "2025-12-02T13:42:09+00:00",
    "url": "https://arxiv.org/pdf/2512.02763v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02751v1",
    "title": "AttMetNet: Attention-Enhanced Deep Neural Network for Methane Plume Detection in Sentinel-2 Satellite Imagery",
    "abstract": "Methane is a powerful greenhouse gas that contributes significantly to global warming. Accurate detection of methane emissions is the key to taking timely action and minimizing their impact on climate change. We present AttMetNet, a novel attention-enhanced deep learning framework for methane plume detection with Sentinel-2 satellite imagery. The major challenge in developing a methane detection model is to accurately identify methane plumes from Sentinel-2's B11 and B12 bands while suppressing false positives caused by background variability and diverse land cover types. Traditional detection methods typically depend on the differences or ratios between these bands when comparing the scenes with and without plumes. However, these methods often require verification by a domain expert because they generate numerous false positives. Recent deep learning methods make some improvements using CNN-based architectures, but lack mechanisms to prioritize methane-specific features. AttMetNet introduces a methane-aware architecture that fuses the Normalized Difference Methane Index (NDMI) with an attention-enhanced U-Net. By jointly exploiting NDMI's plume-sensitive cues and attention-driven feature selection, AttMetNet selectively amplifies methane absorption features while suppressing background noise. This integration establishes a first-of-its-kind architecture tailored for robust methane plume detection in real satellite imagery. Additionally, we employ focal loss to address the severe class imbalance arising from both limited positive plume samples and sparse plume pixels within imagery. Furthermore, AttMetNet is trained on the real methane plume dataset, making it more robust to practical scenarios. Extensive experiments show that AttMetNet surpasses recent methods in methane plume detection with a lower false positive rate, better precision recall balance, and higher IoU.",
    "authors": [
      "Rakib Ahsan",
      "MD Sadik Hossain Shanto",
      "Md Sultanul Arifin",
      "Tanzima Hashem"
    ],
    "published": "2025-12-02T13:34:39+00:00",
    "url": "https://arxiv.org/pdf/2512.02751v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02743v1",
    "title": "Reasoning-Aware Multimodal Fusion for Hateful Video Detection",
    "abstract": "Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.",
    "authors": [
      "Shuonan Yang",
      "Tailin Chen",
      "Jiangbei Yue",
      "Guangliang Cheng",
      "Jianbo Jiao",
      "Zeyu Fu"
    ],
    "published": "2025-12-02T13:24:17+00:00",
    "url": "https://arxiv.org/pdf/2512.02743v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02738v1",
    "title": "Probabilistic energy profiler for statically typed JVM-based programming languages",
    "abstract": "Energy consumption is a growing concern in several fields, from mobile devices to large data centers. Developers need detailed data on the energy consumption of their software to mitigate consumption issues. Previous approaches have a broader focus, such as on specific functions or programs, rather than source code statements. They primarily focus on estimating the CPU's energy consumption using point estimates, thereby disregarding other hardware effects and limiting their use for statistical reasoning and explainability. We developed a novel methodology to address the limitations of measuring only the CPU's consumption and using point estimates, focusing on predicting the energy usage of statically typed JVM-based programming languages, such as Java and Scala. We measure the energy consumption of Bytecode patterns, the translation from the programming language's source code statement to their Java Bytecode representation. With the energy measurements, we construct a statistical model using Bayesian statistics, which allows us to predict the energy consumption through statistical distributions and analyze individual factors. The model includes three factors we obtain statically from the code: data size, data type, operation, and one factor about the hardware platform the code executes on: device. To validate our methodology, we implemented it for Java and evaluated its energy predictions on unseen programs. We observe that all four factors are influential, notably that two devices of the same model may differ in energy consumption and that the operations and data types cause consumption differences. The experiments also show that the energy prediction of programs closely follows the program's real energy consumption, validating our approach. Our work presents a methodology for constructing an energy model that future work, such as verification tools, can use for their energy estimates.",
    "authors": [
      "Joel Nyholm",
      "Wojciech Mostowski",
      "Christoph Reichenbach"
    ],
    "published": "2025-12-02T13:21:35+00:00",
    "url": "https://arxiv.org/pdf/2512.02738v1",
    "categories": [
      "cs.PL",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02737v1",
    "title": "Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone",
    "abstract": "Image-based localization in GNSS-denied environments is critical for UAV autonomy. Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training. Such data are costly to acquire and often unavailable, limiting their applicability. To address this challenge, we adopt a training paradigm that removes the need for UAV imagery during training by learning directly from satellite-view reference images. This is achieved through a dedicated augmentation strategy that simulates the visual domain shift between satellite and real-world UAV views. We introduce CAEVL, an efficient model designed to exploit this paradigm, and validate it on ViLD, a new and challenging dataset of real-world UAV images that we release to the community. Our method achieves competitive performance compared to approaches trained with paired data, demonstrating its effectiveness and strong generalization capabilities.",
    "authors": [
      "Tristan Amadei",
      "Enric Meinhardt-Llopis",
      "Benedicte Bascle",
      "Corentin Abgrall",
      "Gabriele Facciolo"
    ],
    "published": "2025-12-02T13:21:20+00:00",
    "url": "https://arxiv.org/pdf/2512.02737v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02735v1",
    "title": "A Framework for Causal Concept-based Model Explanations",
    "abstract": "This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.",
    "authors": [
      "Anna Rodum Bj\u00f8ru",
      "Jacob Lysn\u00e6s-Larsen",
      "Oskar J\u00f8rgensen",
      "Inga Str\u00fcmke",
      "Helge Langseth"
    ],
    "published": "2025-12-02T13:19:53+00:00",
    "url": "https://arxiv.org/pdf/2512.02735v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02731v1",
    "title": "Self-Improving AI Agents through Self-Play",
    "abstract": "We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $\u03bd_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $\u0398$, and we identify the coefficient of self-improvement $\u03ba$ as the Lie derivative of the capability functional along this flow.   The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $\u03ba> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.   We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.",
    "authors": [
      "Przemyslaw Chojecki"
    ],
    "published": "2025-12-02T13:13:57+00:00",
    "url": "https://arxiv.org/pdf/2512.02731v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02727v1",
    "title": "DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions",
    "abstract": "Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance.",
    "authors": [
      "Yifan Zhou",
      "Takehiko Ohkawa",
      "Guwenxiao Zhou",
      "Kanoko Goto",
      "Takumi Hirose",
      "Yusuke Sekikawa",
      "Nakamasa Inoue"
    ],
    "published": "2025-12-02T13:01:04+00:00",
    "url": "https://arxiv.org/pdf/2512.02727v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02726v1",
    "title": "AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping",
    "abstract": "Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \\textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.",
    "authors": [
      "Md Abdul Kadir",
      "Sai Suresh Macharla Vasu",
      "Sidharth S. Nair",
      "Daniel Sonntag"
    ],
    "published": "2025-12-02T13:00:57+00:00",
    "url": "https://arxiv.org/pdf/2512.02726v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02720v1",
    "title": "StockMem: An Event-Reflection Memory Framework for Stock Forecasting",
    "abstract": "Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.",
    "authors": [
      "He Wang",
      "Wenyilin Xiao",
      "Songqiao Han",
      "Hailiang Huang"
    ],
    "published": "2025-12-02T12:53:02+00:00",
    "url": "https://arxiv.org/pdf/2512.02720v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02719v1",
    "title": "Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs",
    "abstract": "Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.",
    "authors": [
      "Julian Ma",
      "Jun Wang",
      "Zafeirios Fountas"
    ],
    "published": "2025-12-02T12:51:30+00:00",
    "url": "https://arxiv.org/pdf/2512.02719v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "q-bio.NC"
    ]
  },
  {
    "arxiv_id": "2512.02716v2",
    "title": "Menta: A Small Language Model for On-Device Mental Health Prediction",
    "abstract": "Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/",
    "authors": [
      "Tianyi Zhang",
      "Xiangyuan Xue",
      "Lingyan Ruan",
      "Shiya Fu",
      "Feng Xia",
      "Simon D'Alfonso",
      "Vassilis Kostakos",
      "Ting Dang",
      "Hong Jia"
    ],
    "published": "2025-12-02T12:47:08+00:00",
    "url": "https://arxiv.org/pdf/2512.02716v2",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02715v1",
    "title": "GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding",
    "abstract": "Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.",
    "authors": [
      "Peirong Zhang",
      "Yidan Zhang",
      "Luxiao Xu",
      "Jinliang Lin",
      "Zonghao Guo",
      "Fengxiang Wang",
      "Xue Yang",
      "Kaiwen Wei",
      "Lei Wang"
    ],
    "published": "2025-12-02T12:45:52+00:00",
    "url": "https://arxiv.org/pdf/2512.02715v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02713v1",
    "title": "Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs",
    "abstract": "As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.",
    "authors": [
      "Theodoros Aivalis",
      "Iraklis A. Klampanos",
      "Antonis Troumpoukis",
      "Joemon M. Jose"
    ],
    "published": "2025-12-02T12:45:20+00:00",
    "url": "https://arxiv.org/pdf/2512.02713v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02711v1",
    "title": "CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer",
    "abstract": "Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.",
    "authors": [
      "Lavish Bansal",
      "Naman Mishra"
    ],
    "published": "2025-12-02T12:41:48+00:00",
    "url": "https://arxiv.org/pdf/2512.02711v1",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02707v1",
    "title": "Empirical Assessment of the Perception of Software Product Line Engineering by an SME before Migrating its Code Base",
    "abstract": "Migrating a set of software variants into a software product line (SPL) is an expensive and potentially challenging endeavor. Indeed, SPL engineering can significantly impact a company's development process and often requires changes to established developer practices. The work presented in this paper stems from a collaboration with a Small and Medium-sized Enterprise (SME) that decided to migrate its existing code base into an SPL. In this study, we conducted an in-depth evaluation of the company's current development processes and practices, as well as the anticipated benefits and risks associated with the migration. Key stakeholders involved in software development participated in this evaluation to provide insight into their perceptions of the migration and their potential resistance to change. This paper describes the design of the interviews conducted with these stakeholders and presents an analysis of the results. Among the qualitative findings, we observed that all participants, regardless of their role in the development process, identified benefits of the migration relevant to their own activities. Furthermore, our results suggest that an effective risk mitigation strategy involves keeping stakeholders informed and engaged throughout the process, preserving as many good practices as possible, and actively involving them in the migration to ensure a smooth transition and minimize potential challenges.",
    "authors": [
      "Thomas Georges",
      "Marianne Huchard",
      "M\u00e9lanie K\u00f6nig",
      "Cl\u00e9mentine Nebut",
      "Chouki Tibermacine"
    ],
    "published": "2025-12-02T12:39:05+00:00",
    "url": "https://arxiv.org/pdf/2512.02707v1",
    "categories": [
      "cs.SE",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02702v1",
    "title": "Tissue-mask supported inter-subject whole-body image registration in the UK Biobank -- A method benchmarking study",
    "abstract": "The UK Biobank is a large-scale study collecting whole-body MR imaging and non-imaging health data. Robust and accurate inter-subject image registration of these whole-body MR images would enable their body-wide spatial standardization, and region-/voxel-wise correlation analysis of non-imaging data with image-derived parameters (e.g., tissue volume or fat content). We propose a sex-stratified inter-subject whole-body MR image registration approach that uses subcutaneous adipose tissue- and muscle-masks from the state-of-the-art VIBESegmentator method to augment intensity-based graph-cut registration. The proposed method was evaluated on a subset of 4000 subjects by comparing it to an intensity-only method as well as two previously published registration methods, uniGradICON and MIRTK. The evaluation comprised overlap measures applied to the 71 VIBESegmentator masks: 1) Dice scores, and 2) voxel-wise label error frequency. Additionally, voxel-wise correlation between age and each of fat content and tissue volume was studied to exemplify the usefulness for medical research. The proposed method exhibited a mean dice score of 0.77 / 0.75 across the cohort and the 71 masks for males/females, respectively. When compared to the intensity-only registration, the mean values were 6 percentage points (pp) higher for both sexes, and the label error frequency was decreased in most tissue regions. These differences were 9pp / 8pp against uniGradICON and 12pp / 13pp against MIRTK. Using the proposed method, the age-correlation maps were less noisy and showed higher anatomical alignment. In conclusion, the image registration method using two tissue masks improves whole-body registration of UK Biobank images.",
    "authors": [
      "Yasemin Utkueri",
      "Elin Lundstr\u00f6m",
      "H\u00e5kan Ahlstr\u00f6m",
      "Johan \u00d6fverstedt",
      "Joel Kullberg"
    ],
    "published": "2025-12-02T12:30:59+00:00",
    "url": "https://arxiv.org/pdf/2512.02702v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02700v1",
    "title": "VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm",
    "abstract": "Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\\% pruning rate, while delivering an end-to-end inference speedup.",
    "authors": [
      "Zhenkai Wu",
      "Xiaowen Ma",
      "Zhenliang Ni",
      "Dengming Zhang",
      "Han Shu",
      "Xin Jiang",
      "Xinghao Chen"
    ],
    "published": "2025-12-02T12:30:05+00:00",
    "url": "https://arxiv.org/pdf/2512.02700v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02699v1",
    "title": "Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding",
    "abstract": "In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.",
    "authors": [
      "Hyeongseop Rha",
      "Jeong Hun Yeo",
      "Junil Won",
      "Se Jin Park",
      "Yong Man Ro"
    ],
    "published": "2025-12-02T12:29:41+00:00",
    "url": "https://arxiv.org/pdf/2512.02699v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02697v1",
    "title": "GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization",
    "abstract": "Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.",
    "authors": [
      "Zixuan Song",
      "Jing Zhang",
      "Di Wang",
      "Zidie Zhou",
      "Wenbin Liu",
      "Haonan Guo",
      "En Wang",
      "Bo Du"
    ],
    "published": "2025-12-02T12:28:22+00:00",
    "url": "https://arxiv.org/pdf/2512.02697v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02696v1",
    "title": "ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection",
    "abstract": "Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.",
    "authors": [
      "Omid Reza Heidari",
      "Yang Wang",
      "Xinxin Zuo"
    ],
    "published": "2025-12-02T12:28:07+00:00",
    "url": "https://arxiv.org/pdf/2512.02696v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02689v1",
    "title": "An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation",
    "abstract": "Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.",
    "authors": [
      "Daiki Shirafuji",
      "Tatsuhiko Saito",
      "Yasutomo Kimura"
    ],
    "published": "2025-12-02T12:18:48+00:00",
    "url": "https://arxiv.org/pdf/2512.02689v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02686v1",
    "title": "ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data",
    "abstract": "Anomaly segmentation seeks to detect and localize unknown or out-of-distribution (OoD) objects that fall outside predefined semantic classes a capability essential for safe autonomous driving. However, the scarcity and limited diversity of anomaly data severely constrain model generalization in open-world environments. Existing approaches mitigate this issue through synthetic data generation, either by copy-pasting external objects into driving scenes or by leveraging text-to-image diffusion models to inpaint anomalous regions. While these methods improve anomaly diversity, they often lack contextual coherence and physical realism, resulting in domain gaps between synthetic and real data. In this paper, we present ClimaDrive, a semantics-guided image-to-image framework for synthesizing semantically coherent, weather-diverse, and physically plausible OoD driving data. ClimaDrive unifies structure-guided multi-weather generation with prompt-driven anomaly inpainting, enabling the creation of visually realistic training data. Based on this framework, we construct ClimaOoD, a large-scale benchmark spanning six representative driving scenarios under both clear and adverse weather conditions. Extensive experiments on four state-of-the-art methods show that training with ClimaOoD leads to robust improvements in anomaly segmentation. Across all methods, AUROC, AP, and FPR95 show notable gains, with FPR95 dropping from 3.97 to 3.52 for RbA on Fishyscapes LAF. These results demonstrate that ClimaOoD enhances model robustness, offering valuable training data for better generalization in open-world anomaly detection.",
    "authors": [
      "Yuxing Liu",
      "Yong Liu"
    ],
    "published": "2025-12-02T12:14:19+00:00",
    "url": "https://arxiv.org/pdf/2512.02686v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02685v1",
    "title": "Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance",
    "abstract": "Recent advances in object-centric representation learning have shown that slot attention-based methods can effectively decompose visual scenes into object slot representations without supervision. However, existing approaches typically process foreground and background regions indiscriminately, often resulting in background interference and suboptimal instance discovery performance on real-world data. To address this limitation, we propose Foreground-Aware Slot Attention (FASA), a two-stage framework that explicitly separates foreground from background to enable precise object discovery. In the first stage, FASA performs a coarse scene decomposition to distinguish foreground from background regions through a dual-slot competition mechanism. These slots are initialized via a clustering-based strategy, yielding well-structured representations of salient regions. In the second stage, we introduce a masked slot attention mechanism where the first slot captures the background while the remaining slots compete to represent individual foreground objects. To further address over-segmentation of foreground objects, we incorporate pseudo-mask guidance derived from a patch affinity graph constructed with self-supervised image features to guide the learning of foreground slots. Extensive experiments on both synthetic and real-world datasets demonstrate that FASA consistently outperforms state-of-the-art methods, validating the effectiveness of explicit foreground modeling and pseudo-mask guidance for robust scene decomposition and object-coherent representation. Code will be made publicly available.",
    "authors": [
      "Huankun Sheng",
      "Ming Li",
      "Yixiang Wei",
      "Yeying Fan",
      "Yu-Hui Wen",
      "Tieliang Gong",
      "Yong-Jin Liu"
    ],
    "published": "2025-12-02T12:14:05+00:00",
    "url": "https://arxiv.org/pdf/2512.02685v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02682v1",
    "title": "Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions",
    "abstract": "This paper examines why safety mechanisms designed for human-model interaction do not scale to environments where large language models (LLMs) interact with each other. Most current governance practices still rely on single-agent safety containment, prompts, fine-tuning, and moderation layers that constrain individual model behavior but leave the dynamics of multi-model interaction ungoverned. These mechanisms assume a dyadic setting: one model responding to one user under stable oversight. Yet research and industrial development are rapidly shifting toward LLM-to-LLM ecosystems, where outputs are recursively reused as inputs across chains of agents. In such systems, local compliance can aggregate into collective failure even when every model is individually aligned. We propose a conceptual transition from model-level safety to system-level safety, introducing the framework of the Emergent Systemic Risk Horizon (ESRH) to formalize how instability arises from interaction structure rather than from isolated misbehavior. The paper contributes (i) a theoretical account of collective risk in interacting LLMs, (ii) a taxonomy connecting micro, meso, and macro-level failure modes, and (iii) a design proposal for InstitutionalAI, an architecture for embedding adaptive oversight within multi-agent systems.",
    "authors": [
      "Piercosma Bisconti",
      "Marcello Galisai",
      "Federico Pierucci",
      "Marcantonio Bracale",
      "Matteo Prandi"
    ],
    "published": "2025-12-02T12:06:57+00:00",
    "url": "https://arxiv.org/pdf/2512.02682v1",
    "categories": [
      "cs.MA",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02681v1",
    "title": "PGP-DiffSR: Phase-Guided Progressive Pruning for Efficient Diffusion-based Image Super-Resolution",
    "abstract": "Although diffusion-based models have achieved impressive results in image super-resolution, they often rely on large-scale backbones such as Stable Diffusion XL (SDXL) and Diffusion Transformers (DiT), which lead to excessive computational and memory costs during training and inference. To address this issue, we develop a lightweight diffusion method, PGP-DiffSR, by removing redundant information from diffusion models under the guidance of the phase information of inputs for efficient image super-resolution. We first identify the intra-block redundancy within the diffusion backbone and propose a progressive pruning approach that removes redundant blocks while reserving restoration capability. We note that the phase information of the restored images produced by the pruned diffusion model is not well estimated. To solve this problem, we propose a phase-exchange adapter module that explores the phase information of the inputs to guide the pruned diffusion model for better restoration performance. We formulate the progressive pruning approach and the phase-exchange adapter module into a unified model. Extensive experiments demonstrate that our method achieves competitive restoration quality while significantly reducing computational load and memory consumption. The code is available at https://github.com/yzb1997/PGP-DiffSR.",
    "authors": [
      "Zhongbao Yang",
      "Jiangxin Dong",
      "Yazhou Yao",
      "Jinhui Tang",
      "Jinshan Pan"
    ],
    "published": "2025-12-02T12:06:39+00:00",
    "url": "https://arxiv.org/pdf/2512.02681v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02677v1",
    "title": "Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks",
    "abstract": "Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.",
    "authors": [
      "Zhiyuan He"
    ],
    "published": "2025-12-02T12:04:51+00:00",
    "url": "https://arxiv.org/pdf/2512.02677v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02669v1",
    "title": "SAND Challenge: Four Approaches for Dysartria Severity Classification",
    "abstract": "This paper presents a unified study of four distinct modeling approaches for classifying dysarthria severity in the Speech Analysis for Neurodegenerative Diseases (SAND) challenge. All models tackle the same five class classification task using a common dataset of speech recordings. We investigate: (1) a ViT-OF method leveraging a Vision Transformer on spectrogram images, (2) a 1D-CNN approach using eight 1-D CNN's with majority-vote fusion, (3) a BiLSTM-OF approach using nine BiLSTM models with majority vote fusion, and (4) a Hierarchical XGBoost ensemble that combines glottal and formant features through a two stage learning framework. Each method is described, and their performances on a validation set of 53 speakers are compared. Results show that while the feature-engineered XGBoost ensemble achieves the highest macro-F1 (0.86), the deep learning models (ViT, CNN, BiLSTM) attain competitive F1-scores (0.70) and offer complementary insights into the problem.",
    "authors": [
      "Gauri Deshpande",
      "Harish Battula",
      "Ashish Panda",
      "Sunil Kumar Kopparapu"
    ],
    "published": "2025-12-02T11:51:38+00:00",
    "url": "https://arxiv.org/pdf/2512.02669v1",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02668v1",
    "title": "UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking",
    "abstract": "Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.",
    "authors": [
      "Qionglin Ren",
      "Dawei Zhang",
      "Chunxu Tian",
      "Dan Zhang"
    ],
    "published": "2025-12-02T11:47:13+00:00",
    "url": "https://arxiv.org/pdf/2512.02668v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02667v1",
    "title": "Graph VQ-Transformer (GVT): Fast and Accurate Molecular Generation via High-Fidelity Discrete Latents",
    "abstract": "The de novo generation of molecules with desirable properties is a critical challenge, where diffusion models are computationally intensive and autoregressive models struggle with error propagation. In this work, we introduce the Graph VQ-Transformer (GVT), a two-stage generative framework that achieves both high accuracy and efficiency. The core of our approach is a novel Graph Vector Quantized Variational Autoencoder (VQ-VAE) that compresses molecular graphs into high-fidelity discrete latent sequences. By synergistically combining a Graph Transformer with canonical Reverse Cuthill-McKee (RCM) node ordering and Rotary Positional Embeddings (RoPE), our VQ-VAE achieves near-perfect reconstruction rates. An autoregressive Transformer is then trained on these discrete latents, effectively converting graph generation into a well-structured sequence modeling problem. Crucially, this mapping of complex graphs to high-fidelity discrete sequences bridges molecular design with the powerful paradigm of large-scale sequence modeling, unlocking potential synergies with Large Language Models (LLMs). Extensive experiments show that GVT achieves state-of-the-art or highly competitive performance across major benchmarks like ZINC250k, MOSES, and GuacaMol, and notably outperforms leading diffusion models on key distribution similarity metrics such as FCD and KL Divergence. With its superior performance, efficiency, and architectural novelty, GVT not only presents a compelling alternative to diffusion models but also establishes a strong new baseline for the field, paving the way for future research in discrete latent-space molecular generation.",
    "authors": [
      "Haozhuo Zheng",
      "Cheng Wang",
      "Yang Liu"
    ],
    "published": "2025-12-02T11:44:15+00:00",
    "url": "https://arxiv.org/pdf/2512.02667v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02665v1",
    "title": "Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization",
    "abstract": "Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.",
    "authors": [
      "Jing Ma"
    ],
    "published": "2025-12-02T11:36:13+00:00",
    "url": "https://arxiv.org/pdf/2512.02665v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02664v1",
    "title": "PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes",
    "abstract": "Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions. However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence. We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation. This process achieves high-fidelity reflection separation and full-scene reconstruction without requiring environment maps or restrictive material assumptions. We demonstrate on public and self-collected datasets that PolarGuide-GSDR achieves state-of-the-art performance in specular reconstruction, normal estimation, and novel view synthesis, all while maintaining real-time rendering capabilities. To our knowledge, this is the first framework embedding polarization priors directly into 3DGS optimization, yielding superior interpretability and real-time performance for modeling complex reflective scenes.",
    "authors": [
      "Derui Shan",
      "Qian Qiao",
      "Hao Lu",
      "Tao Du",
      "Peng Lu"
    ],
    "published": "2025-12-02T11:34:55+00:00",
    "url": "https://arxiv.org/pdf/2512.02664v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02660v1",
    "title": "Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation",
    "abstract": "Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.",
    "authors": [
      "Agathoklis Georgiou"
    ],
    "published": "2025-12-02T11:29:54+00:00",
    "url": "https://arxiv.org/pdf/2512.02660v1",
    "categories": [
      "cs.CV",
      "cs.IR"
    ]
  },
  {
    "arxiv_id": "2512.02657v1",
    "title": "Distill, Forget, Repeat: A Framework for Continual Unlearning in Text-to-Image Diffusion Models",
    "abstract": "The recent rapid growth of visual generative models trained on vast web-scale datasets has created significant tension with data privacy regulations and copyright laws, such as GDPR's ``Right to be Forgotten.'' This necessitates machine unlearning (MU) to remove specific concepts without the prohibitive cost of retraining. However, existing MU techniques are fundamentally ill-equipped for real-world scenarios where deletion requests arrive sequentially, a setting known as continual unlearning (CUL). Naively applying one-shot methods in a continual setting triggers a stability crisis, leading to a cascade of degradation characterized by retention collapse, compounding collateral damage to related concepts, and a sharp decline in generative quality. To address this critical challenge, we introduce a novel generative distillation based continual unlearning framework that ensures targeted and stable unlearning under sequences of deletion requests. By reframing each unlearning step as a multi-objective, teacher-student distillation process, the framework leverages principles from continual learning to maintain model integrity. Experiments on a 10-step sequential benchmark demonstrate that our method unlearns forget concepts with better fidelity and achieves this without significant interference to the performance on retain concepts or the overall image quality, substantially outperforming baselines. This framework provides a viable pathway for the responsible deployment and maintenance of large-scale generative models, enabling industries to comply with ongoing data removal requests in a practical and effective manner.",
    "authors": [
      "Naveen George",
      "Naoki Murata",
      "Yuhta Takida",
      "Konda Reddy Mopuri",
      "Yuki Mitsufuji"
    ],
    "published": "2025-12-02T11:22:32+00:00",
    "url": "https://arxiv.org/pdf/2512.02657v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02652v1",
    "title": "Pianist Transformer: Towards Expressive Piano Performance Rendering via Scalable Self-Supervised Pre-Training",
    "abstract": "Existing methods for expressive music performance rendering rely on supervised learning over small labeled datasets, which limits scaling of both data volume and model size, despite the availability of vast unlabeled music, as in vision and language. To address this gap, we introduce Pianist Transformer, with four key contributions: 1) a unified Musical Instrument Digital Interface (MIDI) data representation for learning the shared principles of musical structure and expression without explicit annotation; 2) an efficient asymmetric architecture, enabling longer contexts and faster inference without sacrificing rendering quality; 3) a self-supervised pre-training pipeline with 10B tokens and 135M-parameter model, unlocking data and model scaling advantages for expressive performance rendering; 4) a state-of-the-art performance model, which achieves strong objective metrics and human-level subjective ratings. Overall, Pianist Transformer establishes a scalable path toward human-like performance synthesis in the music domain.",
    "authors": [
      "Hong-Jie You",
      "Jie-Jing Shao",
      "Xiao-Wen Yang",
      "Lin-Han Jia",
      "Lan-Zhe Guo",
      "Yu-Feng Li"
    ],
    "published": "2025-12-02T11:13:29+00:00",
    "url": "https://arxiv.org/pdf/2512.02652v1",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM"
    ]
  },
  {
    "arxiv_id": "2512.02651v1",
    "title": "Real-Time Multimodal Data Collection Using Smartwatches and Its Visualization in Education",
    "abstract": "Wearable sensors, such as smartwatches, have become increasingly prevalent across domains like healthcare, sports, and education, enabling continuous monitoring of physiological and behavioral data. In the context of education, these technologies offer new opportunities to study cognitive and affective processes such as engagement, attention, and performance. However, the lack of scalable, synchronized, and high-resolution tools for multimodal data acquisition continues to be a significant barrier to the widespread adoption of Multimodal Learning Analytics in real-world educational settings. This paper presents two complementary tools developed to address these challenges: Watch-DMLT, a data acquisition application for Fitbit Sense 2 smartwatches that enables real-time, multi-user monitoring of physiological and motion signals; and ViSeDOPS, a dashboard-based visualization system for analyzing synchronized multimodal data collected during oral presentations. We report on a classroom deployment involving 65 students and up to 16 smartwatches, where data streams including heart rate, motion, gaze, video, and contextual annotations were captured and analyzed. Results demonstrate the feasibility and utility of the proposed system for supporting fine-grained, scalable, and interpretable Multimodal Learning Analytics in real learning environments.",
    "authors": [
      "Alvaro Becerra",
      "Pablo Villegas",
      "Ruth Cobos"
    ],
    "published": "2025-12-02T11:12:46+00:00",
    "url": "https://arxiv.org/pdf/2512.02651v1",
    "categories": [
      "cs.HC",
      "cs.CV",
      "cs.SE"
    ]
  },
  {
    "arxiv_id": "2512.02650v1",
    "title": "Hear What Matters! Text-conditioned Selective Video-to-Audio Generation",
    "abstract": "This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.",
    "authors": [
      "Junwon Lee",
      "Juhan Nam",
      "Jiyoung Lee"
    ],
    "published": "2025-12-02T11:12:16+00:00",
    "url": "https://arxiv.org/pdf/2512.02650v1",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "arxiv_id": "2512.02648v1",
    "title": "PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking",
    "abstract": "We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D",
    "authors": [
      "Dong Li",
      "Jiahao Xiong",
      "Yingda Huang",
      "Le Chang"
    ],
    "published": "2025-12-02T11:08:38+00:00",
    "url": "https://arxiv.org/pdf/2512.02648v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02643v1",
    "title": "Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening",
    "abstract": "Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.",
    "authors": [
      "Yongchuan Cui",
      "Peng Liu",
      "Yi Zeng"
    ],
    "published": "2025-12-02T10:56:26+00:00",
    "url": "https://arxiv.org/pdf/2512.02643v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02636v1",
    "title": "Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models",
    "abstract": "Log-likelihood evaluation enables important capabilities in generative models, including model comparison, certain fine-tuning objectives, and many downstream applications. Yet paradoxically, some of today's best generative models -- diffusion and flow-based models -- still require hundreds to thousands of neural function evaluations (NFEs) to compute a single likelihood. While recent distillation methods have successfully accelerated sampling to just a few steps, they achieve this at the cost of likelihood tractability: existing approaches either abandon likelihood computation entirely or still require expensive integration over full trajectories. We present fast flow joint distillation (F2D2), a framework that simultaneously reduces the number of NFEs required for both sampling and likelihood evaluation by two orders of magnitude. Our key insight is that in continuous normalizing flows, the coupled ODEs for sampling and likelihood are computed from a shared underlying velocity field, allowing us to jointly distill both the sampling trajectory and cumulative divergence using a single model. F2D2 is modular, compatible with existing flow-based few-step sampling models, and requires only an additional divergence prediction head. Experiments demonstrate F2D2's capability of achieving accurate log-likelihood with few-step evaluations while maintaining high sample quality, solving a long-standing computational bottleneck in flow-based generative models. As an application of our approach, we propose a lightweight self-guidance method that enables a 2-step MeanFlow model to outperform a 1024 step teacher model with only a single additional backward NFE.",
    "authors": [
      "Xinyue Ai",
      "Yutong He",
      "Albert Gu",
      "Ruslan Salakhutdinov",
      "J Zico Kolter",
      "Nicholas Matthew Boffi",
      "Max Simchowitz"
    ],
    "published": "2025-12-02T10:48:20+00:00",
    "url": "https://arxiv.org/pdf/2512.02636v1",
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02633v1",
    "title": "Zero-Shot Instruction Following in RL via Structured LTL Representations",
    "abstract": "Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.",
    "authors": [
      "Mattia Giuri",
      "Mathias Jackermeier",
      "Alessandro Abate"
    ],
    "published": "2025-12-02T10:44:51+00:00",
    "url": "https://arxiv.org/pdf/2512.02633v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02625v1",
    "title": "CryptoQA: A Large-scale Question-answering Dataset for AI-assisted Cryptography",
    "abstract": "Large language models (LLMs) excel at many general-purpose natural language processing tasks. However, their ability to perform deep reasoning and mathematical analysis, particularly for complex tasks as required in cryptography, remains poorly understood, largely due to the lack of suitable data for evaluation and training. To address this gap, we present CryptoQA, the first large-scale question-answering (QA) dataset specifically designed for cryptography. CryptoQA contains over two million QA pairs drawn from curated academic sources, along with contextual metadata that can be used to test the cryptographic capabilities of LLMs and to train new LLMs on cryptographic tasks. We benchmark 15 state-of-the-art LLMs on CryptoQA, evaluating their factual accuracy, mathematical reasoning, consistency, referencing, backward reasoning, and robustness to adversarial samples. In addition to quantitative metrics, we provide expert reviews that qualitatively assess model outputs and establish a gold-standard baseline. Our results reveal significant performance deficits of LLMs, particularly on tasks that require formal reasoning and precise mathematical knowledge. This shows the urgent need for LLM assistants tailored to cryptography research and development. We demonstrate that, by using CryptoQA, LLMs can be fine-tuned to exhibit better performance on cryptographic tasks.",
    "authors": [
      "Mayar Elfares",
      "Pascal Reisert",
      "Tilman Dietz",
      "Manpa Barman",
      "Ahmed Zaki",
      "Ralf K\u00fcsters",
      "Andreas Bulling"
    ],
    "published": "2025-12-02T10:35:36+00:00",
    "url": "https://arxiv.org/pdf/2512.02625v1",
    "categories": [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02624v1",
    "title": "PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding",
    "abstract": "PowerPoint presentations combine rich textual content with structured visual layouts, making them a natural testbed for evaluating the multimodal reasoning and layout understanding abilities of modern MLLMs. However, existing benchmarks focus solely on narrow subtasks while overlooking layout-centric challenges, which are central to real-world slide creation and editing. To bridge this gap, we introduce PPTBench, a comprehensive multimodal benchmark for evaluating LLMs on PowerPoint-related tasks. Leveraging a diverse source of 958 PPTX files, PPTBench evaluates models across four categories with 4,439 samples, including Detection, Understanding, Modification, and Generation. Our experiments reveal a substantial gap between semantic understanding and visual-layout reasoning in current MLLMs: models can interpret slide content but fail to produce coherent spatial arrangements. Ablation and further analysis show that current MLLMs struggle to combine visual cues with JSON-based layout structures and fail to integrate visual information into their API planning ability. And case studies visually expose systematic layout errors such as misalignment and element overlap. These findings provides a new perspective on evaluating VLLMs in PPT scenarios, highlighting challenges and directions for future research on visual-structural reasoning and coherent slide generation. All datasets and code are fully released to support reproducibility and future research.",
    "authors": [
      "Zheng Huang",
      "Xukai Liu",
      "Tianyu Hu",
      "Kai Zhang",
      "Ye Liu"
    ],
    "published": "2025-12-02T10:33:31+00:00",
    "url": "https://arxiv.org/pdf/2512.02624v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02622v1",
    "title": "RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence",
    "abstract": "Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.",
    "authors": [
      "Xuming He",
      "Zehao Fan",
      "Hengjia Li",
      "Fan Zhuo",
      "Hankun Xu",
      "Senlin Cheng",
      "Di Weng",
      "Haifeng Liu",
      "Can Ye",
      "Boxi Wu"
    ],
    "published": "2025-12-02T10:29:51+00:00",
    "url": "https://arxiv.org/pdf/2512.02622v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02621v1",
    "title": "Content-Aware Texturing for Gaussian Splatting",
    "abstract": "Gaussian Splatting has become the method of choice for 3D reconstruction and real-time rendering of captured real scenes. However, fine appearance details need to be represented as a large number of small Gaussian primitives, which can be wasteful when geometry and appearance exhibit different frequency characteristics.   Inspired by the long tradition of texture mapping, we propose to use texture to represent detailed appearance where possible. Our main focus is to incorporate per-primitive texture maps that adapt to the scene in a principled manner during Gaussian Splatting optimization. We do this by proposing a new appearance representation for 2D Gaussian primitives with textures where the size of a texel is bounded by the image sampling frequency and adapted to the content of the input images. We achieve this by adaptively upscaling or downscaling the texture resolution during optimization. In addition, our approach enables control of the number of primitives during optimization based on texture resolution. We show that our approach performs favorably in image quality and total number of parameters used compared to alternative solutions for textured Gaussian primitives. Project page: https://repo-sam.inria.fr/nerphys/gs-texturing/",
    "authors": [
      "Panagiotis Papantonakis",
      "Georgios Kopanas",
      "Fredo Durand",
      "George Drettakis"
    ],
    "published": "2025-12-02T10:29:10+00:00",
    "url": "https://arxiv.org/pdf/2512.02621v1",
    "categories": [
      "cs.CV",
      "cs.GR"
    ]
  },
  {
    "arxiv_id": "2512.02610v1",
    "title": "Target-specific Adaptation and Consistent Degradation Alignment for Cross-Domain Remaining Useful Life Prediction",
    "abstract": "Accurate prediction of the Remaining Useful Life (RUL) in machinery can significantly diminish maintenance costs, enhance equipment up-time, and mitigate adverse outcomes. Data-driven RUL prediction techniques have demonstrated commendable performance. However, their efficacy often relies on the assumption that training and testing data are drawn from the same distribution or domain, which does not hold in real industrial settings. To mitigate this domain discrepancy issue, prior adversarial domain adaptation methods focused on deriving domain-invariant features. Nevertheless, they overlook target-specific information and inconsistency characteristics pertinent to the degradation stages, resulting in suboptimal performance. To tackle these issues, we propose a novel domain adaptation approach for cross-domain RUL prediction named TACDA. Specifically, we propose a target domain reconstruction strategy within the adversarial adaptation process, thereby retaining target-specific information while learning domain-invariant features. Furthermore, we develop a novel clustering and pairing strategy for consistent alignment between similar degradation stages. Through extensive experiments, our results demonstrate the remarkable performance of our proposed TACDA method, surpassing state-of-the-art approaches with regard to two different evaluation metrics. Our code is available at https://github.com/keyplay/TACDA.",
    "authors": [
      "Yubo Hou",
      "Mohamed Ragab",
      "Min Wu",
      "Chee-Keong Kwoh",
      "Xiaoli Li",
      "Zhenghua Chen"
    ],
    "published": "2025-12-02T10:15:14+00:00",
    "url": "https://arxiv.org/pdf/2512.02610v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02609v1",
    "title": "SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction",
    "abstract": "Imitation learning for robotic grasping is often plagued by the multimodal problem: when a scene contains multiple valid targets, demonstrations of grasping different objects create conflicting training signals. Standard imitation learning policies fail by averaging these distinct actions into a single, invalid action. In this paper, we introduce SAM2Grasp, a novel framework that resolves this issue by reformulating the task as a uni-modal, prompt-conditioned prediction problem. Our method leverages the frozen SAM2 model to use its powerful visual temporal tracking capability and introduces a lightweight, trainable action head that operates in parallel with its native segmentation head. This design allows for training only the small action head on pre-computed temporal-visual features from SAM2. During inference, an initial prompt, such as a bounding box provided by an upstream object detection model, designates the specific object to be grasped. This prompt conditions the action head to predict a unique, unambiguous grasp trajectory for that object alone. In all subsequent video frames, SAM2's built-in temporal tracking capability automatically maintains stable tracking of the selected object, enabling our model to continuously predict the grasp trajectory from the video stream without further external guidance. This temporal-prompted approach effectively eliminates ambiguity from the visuomotor policy. We demonstrate through extensive experiments that SAM2Grasp achieves state-of-the-art performance in cluttered, multi-object grasping tasks.",
    "authors": [
      "Shengkai Wu",
      "Jinrong Yang",
      "Wenqiu Luo",
      "Linfeng Gao",
      "Chaohui Shang",
      "Meiyu Zhi",
      "Mingshan Sun",
      "Fangping Yang",
      "Liangliang Ren",
      "Yong Zhao"
    ],
    "published": "2025-12-02T10:15:00+00:00",
    "url": "https://arxiv.org/pdf/2512.02609v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02605v1",
    "title": "IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai",
    "abstract": "This technical white paper introduces the Interactive Agents Call Tree (IACT), a computational model designed to address the limitations of static, hard-coded agent workflows. Unlike traditional systems that require pre-defined graphs or specialized programming, IACT operates as a general-purpose autonomous system driven purely by user dialogue. Given a high-level objective, the system autonomously grows a dynamic, recursive agent topology incrementally tailored to the problem's structure. This allows it to scale its organizational complexity to match open-ended tasks. To mitigate the error propagation inherent in unidirectional function calls, IACT introduces interactional redundancy by replacing rigid invocations with bidirectional, stateful dialogues. This mechanism enables runtime error correction and ambiguity resolution. We describe the architecture, design principles, and practical lessons behind the production deployment of this model in the kragent.ai system, presenting qualitative evidence from real-world workflows rather than exhaustive benchmark results.",
    "authors": [
      "Pengju Lu"
    ],
    "published": "2025-12-02T10:10:56+00:00",
    "url": "https://arxiv.org/pdf/2512.02605v1",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.SE"
    ]
  },
  {
    "arxiv_id": "2512.02593v1",
    "title": "Spoken Conversational Agents with Large Language Models",
    "abstract": "Spoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap.",
    "authors": [
      "Chao-Han Huck Yang",
      "Andreas Stolcke",
      "Larry Heck"
    ],
    "published": "2025-12-02T10:02:10+00:00",
    "url": "https://arxiv.org/pdf/2512.02593v1",
    "categories": [
      "cs.CL",
      "cs.MA",
      "cs.NE",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "arxiv_id": "2512.02589v1",
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "abstract": "Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.",
    "authors": [
      "Junyi Hou",
      "Andre Lin Huikai",
      "Nuo Chen",
      "Yiwei Gong",
      "Bingsheng He"
    ],
    "published": "2025-12-02T10:00:37+00:00",
    "url": "https://arxiv.org/pdf/2512.02589v1",
    "categories": [
      "cs.AI",
      "cs.SE"
    ]
  },
  {
    "arxiv_id": "2512.02580v1",
    "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
    "abstract": "Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.",
    "authors": [
      "Changpeng Yang",
      "Jinyang Wu",
      "Yuchen Liu",
      "Shuai Zhang",
      "Yang Li",
      "Qiliang Liang",
      "Hongzhen Wang",
      "Shuai Nie",
      "Jiaming Xu",
      "Runyu Shi",
      "Ying Huang",
      "Guoquan Zhang"
    ],
    "published": "2025-12-02T09:48:57+00:00",
    "url": "https://arxiv.org/pdf/2512.02580v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02576v1",
    "title": "Co-speech Gesture Video Generation via Motion-Based Graph Retrieval",
    "abstract": "Synthesizing synchronized and natural co-speech gesture videos remains a formidable challenge. Recent approaches have leveraged motion graphs to harness the potential of existing video data. To retrieve an appropriate trajectory from the graph, previous methods either utilize the distance between features extracted from the input audio and those associated with the motions in the graph or embed both the input audio and motion into a shared feature space. However, these techniques may not be optimal due to the many-to-many mapping nature between audio and gestures, which cannot be adequately addressed by one-to-one mapping. To alleviate this limitation, we propose a novel framework that initially employs a diffusion model to generate gesture motions. The diffusion model implicitly learns the joint distribution of audio and motion, enabling the generation of contextually appropriate gestures from input audio sequences. Furthermore, our method extracts both low-level and high-level features from the input audio to enrich the training process of the diffusion model. Subsequently, a meticulously designed motion-based retrieval algorithm is applied to identify the most suitable path within the graph by assessing both global and local similarities in motion. Given that not all nodes in the retrieved path are sequentially continuous, the final step involves seamlessly stitching together these segments to produce a coherent video output. Experimental results substantiate the efficacy of our proposed method, demonstrating a significant improvement over prior approaches in terms of synchronization accuracy and naturalness of generated gestures.",
    "authors": [
      "Yafei Song",
      "Peng Zhang",
      "Bang Zhang"
    ],
    "published": "2025-12-02T09:46:12+00:00",
    "url": "https://arxiv.org/pdf/2512.02576v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02567v1",
    "title": "Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System",
    "abstract": "The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.   We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.   Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.",
    "authors": [
      "Martin Weiss",
      "Jesko Hecking-Harbusch",
      "Jochen Quante",
      "Matthias Woehrle"
    ],
    "published": "2025-12-02T09:38:20+00:00",
    "url": "https://arxiv.org/pdf/2512.02567v1",
    "categories": [
      "cs.SE",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02566v1",
    "title": "From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature",
    "abstract": "There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.",
    "authors": [
      "Kun Yuan",
      "Min Woo Sun",
      "Zhen Chen",
      "Alejandro Lozano",
      "Xiangteng He",
      "Shi Li",
      "Nassir Navab",
      "Xiaoxiao Sun",
      "Nicolas Padoy",
      "Serena Yeung-Levy"
    ],
    "published": "2025-12-02T09:37:51+00:00",
    "url": "https://arxiv.org/pdf/2512.02566v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02561v1",
    "title": "EZYer: A simulacrum of high school with generative agent",
    "abstract": "With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in terms of courseware generation, interactive notes and quality assurance of content. In particular, the proposed generative agent EZYer : 1) Teacher Module: Integrating the Text Corpus retrieval and in-depth generation technologies, it automatically generates structured teaching materials and LaTeX Beamer courseware in line with the high school mathematics syllabus and supports user-defined image insertion. 2) Student Module: Throughout the collaborative interaction of the four roles of Teacher, Assistant, Top Student and Struggling Student, Note Taker summarizes and generates academic notes to enhance the depth and interest of learning. 3) Controller: set up keyword filtering system, content scoring system, role co-validation system, and dynamic content correction system. This ensure academic strictness and pedagogical propriety of EZYer inputs and outputs. In order to evaluate EZYer, this paper designs five-dimensional evaluation indexes of content accuracy, knowledge coverage, usability, formatting correctness and visual design and appeal, and scores 100 Beamer and Notes generated by EZYer by five large language models, separately, and the results show that the quality of EZYer-generated content is excellent and has a good application prospect.",
    "authors": [
      "Jinming Yang",
      "Zimu Ji",
      "Weiqi Luo",
      "Gaoxi Wang",
      "Bin Ma",
      "Yueling Deng"
    ],
    "published": "2025-12-02T09:28:59+00:00",
    "url": "https://arxiv.org/pdf/2512.02561v1",
    "categories": [
      "cs.MA",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02558v1",
    "title": "Empathy Level Prediction in Multi-Modal Scenario with Supervisory Documentation Assistance",
    "abstract": "Prevalent empathy prediction techniques primarily concentrate on a singular modality, typically textual, thus neglecting multi-modal processing capabilities. They also overlook the utilization of certain privileged information, which may encompass additional empathetic content. In response, we introduce an advanced multi-modal empathy prediction method integrating video, audio, and text information. The method comprises the Multi-Modal Empathy Prediction and Supervisory Documentation Assisted Training. We use pre-trained networks in the empathy prediction network to extract features from various modalities, followed by a cross-modal fusion. This process yields a multi-modal feature representation, which is employed to predict empathy labels. To enhance the extraction of text features, we incorporate supervisory documents as privileged information during the assisted training phase. Specifically, we apply the Latent Dirichlet Allocation model to identify potential topic distributions to constrain text features. These supervisory documents, created by supervisors, focus on the counseling topics and the counselor's display of empathy. Notably, this privileged information is only available during training and is not accessible during the prediction phase. Experimental results on the multi-modal and dialogue empathy datasets demonstrate that our approach is superior to the existing methods.",
    "authors": [
      "Yufei Xiao",
      "Shangfei Wang"
    ],
    "published": "2025-12-02T09:26:56+00:00",
    "url": "https://arxiv.org/pdf/2512.02558v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02556v1",
    "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
    "abstract": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
    "authors": [
      "DeepSeek-AI",
      "Aixin Liu",
      "Aoxue Mei",
      "Bangcai Lin",
      "Bing Xue",
      "Bingxuan Wang",
      "Bingzheng Xu",
      "Bochao Wu",
      "Bowei Zhang",
      "Chaofan Lin",
      "Chen Dong",
      "Chengda Lu",
      "Chenggang Zhao",
      "Chengqi Deng",
      "Chenhao Xu",
      "Chong Ruan",
      "Damai Dai",
      "Daya Guo",
      "Dejian Yang",
      "Deli Chen",
      "Erhang Li",
      "Fangqi Zhou",
      "Fangyun Lin",
      "Fucong Dai",
      "Guangbo Hao",
      "Guanting Chen",
      "Guowei Li",
      "H. Zhang",
      "Hanwei Xu",
      "Hao Li",
      "Haofen Liang",
      "Haoran Wei",
      "Haowei Zhang",
      "Haowen Luo",
      "Haozhe Ji",
      "Honghui Ding",
      "Hongxuan Tang",
      "Huanqi Cao",
      "Huazuo Gao",
      "Hui Qu",
      "Hui Zeng",
      "Jialiang Huang",
      "Jiashi Li",
      "Jiaxin Xu",
      "Jiewen Hu",
      "Jingchang Chen",
      "Jingting Xiang",
      "Jingyang Yuan",
      "Jingyuan Cheng",
      "Jinhua Zhu",
      "Jun Ran",
      "Junguang Jiang",
      "Junjie Qiu",
      "Junlong Li",
      "Junxiao Song",
      "Kai Dong",
      "Kaige Gao",
      "Kang Guan",
      "Kexin Huang",
      "Kexing Zhou",
      "Kezhao Huang",
      "Kuai Yu",
      "Lean Wang",
      "Lecong Zhang",
      "Lei Wang",
      "Liang Zhao",
      "Liangsheng Yin",
      "Lihua Guo",
      "Lingxiao Luo",
      "Linwang Ma",
      "Litong Wang",
      "Liyue Zhang",
      "M. S. Di",
      "M. Y Xu",
      "Mingchuan Zhang",
      "Minghua Zhang",
      "Minghui Tang",
      "Mingxu Zhou",
      "Panpan Huang",
      "Peixin Cong",
      "Peiyi Wang",
      "Qiancheng Wang",
      "Qihao Zhu",
      "Qingyang Li",
      "Qinyu Chen",
      "Qiushi Du",
      "Ruiling Xu",
      "Ruiqi Ge",
      "Ruisong Zhang",
      "Ruizhe Pan",
      "Runji Wang",
      "Runqiu Yin",
      "Runxin Xu",
      "Ruomeng Shen",
      "Ruoyu Zhang",
      "S. H. Liu",
      "Shanghao Lu",
      "Shangyan Zhou",
      "Shanhuang Chen",
      "Shaofei Cai",
      "Shaoyuan Chen",
      "Shengding Hu",
      "Shengyu Liu",
      "Shiqiang Hu",
      "Shirong Ma",
      "Shiyu Wang",
      "Shuiping Yu",
      "Shunfeng Zhou",
      "Shuting Pan",
      "Songyang Zhou",
      "Tao Ni",
      "Tao Yun",
      "Tian Pei",
      "Tian Ye",
      "Tianyuan Yue",
      "Wangding Zeng",
      "Wen Liu",
      "Wenfeng Liang",
      "Wenjie Pang",
      "Wenjing Luo",
      "Wenjun Gao",
      "Wentao Zhang",
      "Xi Gao",
      "Xiangwen Wang",
      "Xiao Bi",
      "Xiaodong Liu",
      "Xiaohan Wang",
      "Xiaokang Chen",
      "Xiaokang Zhang",
      "Xiaotao Nie",
      "Xin Cheng",
      "Xin Liu",
      "Xin Xie",
      "Xingchao Liu",
      "Xingkai Yu",
      "Xingyou Li",
      "Xinyu Yang",
      "Xinyuan Li",
      "Xu Chen",
      "Xuecheng Su",
      "Xuehai Pan",
      "Xuheng Lin",
      "Xuwei Fu",
      "Y. Q. Wang",
      "Yang Zhang",
      "Yanhong Xu",
      "Yanru Ma",
      "Yao Li",
      "Yao Li",
      "Yao Zhao",
      "Yaofeng Sun",
      "Yaohui Wang",
      "Yi Qian",
      "Yi Yu",
      "Yichao Zhang",
      "Yifan Ding",
      "Yifan Shi",
      "Yiliang Xiong",
      "Ying He",
      "Ying Zhou",
      "Yinmin Zhong",
      "Yishi Piao",
      "Yisong Wang",
      "Yixiao Chen",
      "Yixuan Tan",
      "Yixuan Wei",
      "Yiyang Ma",
      "Yiyuan Liu",
      "Yonglun Yang",
      "Yongqiang Guo",
      "Yongtong Wu",
      "Yu Wu",
      "Yuan Cheng",
      "Yuan Ou",
      "Yuanfan Xu",
      "Yuduan Wang",
      "Yue Gong",
      "Yuhan Wu",
      "Yuheng Zou",
      "Yukun Li",
      "Yunfan Xiong",
      "Yuxiang Luo",
      "Yuxiang You",
      "Yuxuan Liu",
      "Yuyang Zhou",
      "Z. F. Wu",
      "Z. Z. Ren",
      "Zehua Zhao",
      "Zehui Ren",
      "Zhangli Sha",
      "Zhe Fu",
      "Zhean Xu",
      "Zhenda Xie",
      "Zhengyan Zhang",
      "Zhewen Hao",
      "Zhibin Gou",
      "Zhicheng Ma",
      "Zhigang Yan",
      "Zhihong Shao",
      "Zhixian Huang",
      "Zhiyu Wu",
      "Zhuoshu Li",
      "Zhuping Zhang",
      "Zian Xu",
      "Zihao Wang",
      "Zihui Gu",
      "Zijia Zhu",
      "Zilin Li",
      "Zipeng Zhang",
      "Ziwei Xie",
      "Ziyi Gao",
      "Zizheng Pan",
      "Zongqing Yao",
      "Bei Feng",
      "Hui Li",
      "J. L. Cai",
      "Jiaqi Ni",
      "Lei Xu",
      "Meng Li",
      "Ning Tian",
      "R. J. Chen",
      "R. L. Jin",
      "S. S. Li",
      "Shuang Zhou",
      "Tianyu Sun",
      "X. Q. Li",
      "Xiangyue Jin",
      "Xiaojin Shen",
      "Xiaosha Chen",
      "Xinnan Song",
      "Xinyi Zhou",
      "Y. X. Zhu",
      "Yanping Huang",
      "Yaohui Li",
      "Yi Zheng",
      "Yuchen Zhu",
      "Yunxian Ma",
      "Zhen Huang",
      "Zhipeng Xu",
      "Zhongyu Zhang",
      "Dongjie Ji",
      "Jian Liang",
      "Jianzhong Guo",
      "Jin Chen",
      "Leyi Xia",
      "Miaojun Wang",
      "Mingming Li",
      "Peng Zhang",
      "Ruyi Chen",
      "Shangmian Sun",
      "Shaoqing Wu",
      "Shengfeng Ye",
      "T. Wang",
      "W. L. Xiao",
      "Wei An",
      "Xianzu Wang",
      "Xiaowen Sun",
      "Xiaoxiang Wang",
      "Ying Tang",
      "Yukun Zha",
      "Zekai Zhang",
      "Zhe Ju",
      "Zhen Zhang",
      "Zihua Qu"
    ],
    "published": "2025-12-02T09:25:14+00:00",
    "url": "https://arxiv.org/pdf/2512.02556v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02555v1",
    "title": "ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce",
    "abstract": "Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.",
    "authors": [
      "Zheng Fang",
      "Donghao Xie",
      "Ming Pang",
      "Chunyuan Yuan",
      "Xue Jiang",
      "Changping Peng",
      "Zhangang Lin",
      "Zheng Luo"
    ],
    "published": "2025-12-02T09:25:13+00:00",
    "url": "https://arxiv.org/pdf/2512.02555v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ]
  },
  {
    "arxiv_id": "2512.02554v1",
    "title": "OmniPerson: Unified Identity-Preserving Pedestrian Generation",
    "abstract": "Person re-identification (ReID) suffers from a lack of large-scale high-quality training data due to challenges in data privacy and annotation costs. While previous approaches have explored pedestrian generation for data augmentation, they often fail to ensure identity consistency and suffer from insufficient controllability, thereby limiting their effectiveness in dataset augmentation. To address this, We introduce OmniPerson, the first unified identity-preserving pedestrian generation pipeline for visible/infrared image/video ReID tasks. Our contributions are threefold: 1) We proposed OmniPerson, a unified generation model, offering holistic and fine-grained control over all key pedestrian attributes. Supporting RGB/IR modality image/video generation with any number of reference images, two kinds of person poses, and text. Also including RGB-to-IR transfer and image super-resolution abilities.2) We designed Multi-Refer Fuser for robust identity preservation with any number of reference images as input, making OmniPerson could distill a unified identity from a set of multi-view reference images, ensuring our generated pedestrians achieve high-fidelity pedestrian generation.3) We introduce PersonSyn, the first large-scale dataset for multi-reference, controllable pedestrian generation, and present its automated curation pipeline which transforms public, ID-only ReID benchmarks into a richly annotated resource with the dense, multi-modal supervision required for this task. Experimental results demonstrate that OmniPerson achieves SoTA in pedestrian generation, excelling in both visual fidelity and identity consistency. Furthermore, augmenting existing datasets with our generated data consistently improves the performance of ReID models. We will open-source the full codebase, pretrained model, and the PersonSyn dataset.",
    "authors": [
      "Changxiao Ma",
      "Chao Yuan",
      "Xincheng Shi",
      "Yuzhuo Ma",
      "Yongfei Zhang",
      "Longkun Zhou",
      "Yujia Zhang",
      "Shangze Li",
      "Yifan Xu"
    ],
    "published": "2025-12-02T09:24:34+00:00",
    "url": "https://arxiv.org/pdf/2512.02554v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02552v1",
    "title": "What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints",
    "abstract": "We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.",
    "authors": [
      "Francesco Paolo Savatteri",
      "Chahan Vidal-Gor\u00e8ne",
      "Florian Cafiero"
    ],
    "published": "2025-12-02T09:24:16+00:00",
    "url": "https://arxiv.org/pdf/2512.02552v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02551v1",
    "title": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning",
    "abstract": "In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\\it cuBLAS}, {\\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\\% over {\\it torch.matmul} on average; +19.2\\% over {\\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\\% over {\\it cuBLASLt-heuristic}, which queries {\\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\\% over the most competitive {\\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\\%, +26.0\\%, +22.4\\%, and +15.9\\% for {\\it torch.matmul}, {\\it cuBLAS}, {\\it cuBLASLt-heuristic}, and {\\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2",
    "authors": [
      "Songqiao Su",
      "Xiaofei Sun",
      "Xiaoya Li",
      "Albert Wang",
      "Jiwei Li",
      "Chris Shum"
    ],
    "published": "2025-12-02T09:20:15+00:00",
    "url": "https://arxiv.org/pdf/2512.02551v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02550v1",
    "title": "Sparse Computations in Deep Learning Inference",
    "abstract": "The computational demands of modern Deep Neural Networks (DNNs) are immense and constantly growing. While training costs usually capture public attention, inference demands are also contributing in significant computational, energy and environmental footprints. Sparsity stands out as a critical mechanism for drastically reducing these resource demands. However, its potential remains largely untapped and is not yet fully incorporated in production AI systems. To bridge this gap, this work provides the necessary knowledge and insights for performance engineers keen to get involved in deep learning inference optimization. In particular, in this work we: a) discuss the various forms of sparsity that can be utilized in DNN inference, b) explain how the original dense computations translate to sparse kernels, c) provide an extensive bibliographic review of the state-of-the-art in the implementation of these kernels for CPUs and GPUs, d) discuss the availability of sparse datasets in support of sparsity-related research and development, e) explore the current software tools and frameworks that provide robust sparsity support, and f) present evaluation results of different implementations of the key SpMM and SDDMM kernels on CPU and GPU platforms. Ultimately, this paper aims to serve as a resource for performance engineers seeking to develop and deploy highly efficient sparse deep learning models in productions.",
    "authors": [
      "Ioanna Tasou",
      "Panagiotis Mpakos",
      "Angelos Vlachos",
      "Dionysios Adamopoulos",
      "Georgios Giannakopoulos",
      "Konstantinos Katsikopoulos",
      "Ioannis Karaparisis",
      "Maria Lazou",
      "Spyridon Loukovitis",
      "Areti Mei",
      "Anastasia Poulopoulou",
      "Angeliki Dimitriou",
      "Giorgos Filandrianos",
      "Dimitrios Galanopoulos",
      "Vasileios Karampinis",
      "Ilias Mitsouras",
      "Nikolaos Spanos",
      "Petros Anastasiadis",
      "Ioannis Doudalis",
      "Konstantinos Nikas",
      "George Retsinas",
      "Paraskevi Tzouveli",
      "Christina Giannoula",
      "Nectarios Koziris",
      "Nikela Papadopoulou",
      "Giorgos Stamou",
      "Athanasios Voulodimos",
      "Georgios Goumas"
    ],
    "published": "2025-12-02T09:19:33+00:00",
    "url": "https://arxiv.org/pdf/2512.02550v1",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02541v1",
    "title": "AVGGT: Rethinking Global Attention for Accelerating VGGT",
    "abstract": "Since DUSt3R, models such as VGGT and $\u03c0^3$ have shown strong multi-view 3D performance, but their heavy reliance on global self-attention results in high computational cost. Existing sparse-attention variants offer partial speedups, yet lack a systematic analysis of how global attention contributes to multi-view reasoning. In this paper, we first conduct an in-depth investigation of the global attention modules in VGGT and $\u03c0^3$ to better understand their roles. Our analysis reveals a clear division of roles in the alternating global-frame architecture: early global layers do not form meaningful correspondences, middle layers perform cross-view alignment, and last layers provide only minor refinements. Guided by these findings, we propose a training-free two-step acceleration scheme: (1) converting early global layers into frame attention, and (2) subsampling global attention by subsampling K/V over patch tokens with diagonal preservation and a mean-fill component. We instantiate this strategy on VGGT and $\u03c0^3$ and evaluate across standard pose and point-map benchmarks. Our method achieves up to $8$-$10\\times$ speedup in inference time while matching or slightly improving the accuracy of the original models, and remains robust even in extremely dense multi-view settings where prior sparse-attention baselines fail.",
    "authors": [
      "Xianbing Sun",
      "Zhikai Zhu",
      "Zhengyu Lou",
      "Bo Yang",
      "Jinyang Tang",
      "Liqing Zhang",
      "He Wang",
      "Jianfu Zhang"
    ],
    "published": "2025-12-02T09:08:18+00:00",
    "url": "https://arxiv.org/pdf/2512.02541v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02536v1",
    "title": "WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens",
    "abstract": "Recent progress in multimodal large language models (MLLMs) has highlighted the challenge of efficiently bridging pre-trained Vision-Language Models (VLMs) with Diffusion Models. While methods using a fixed number of learnable query tokens offer computational efficiency, they suffer from task generalization collapse, failing to adapt to new tasks that are distant from their pre-training tasks. To overcome this, we propose Noisy Query Tokens, which learn a distributed representation space between the VLM and Diffusion Model via end-to-end optimization, enhancing continual learning. Additionally, we introduce a VAE branch with linear projection to recover fine-grained image details. Experimental results confirm our approach mitigates generalization collapse and enables stable continual learning across diverse tasks.",
    "authors": [
      "Jian Yang",
      "Dacheng Yin",
      "Xiaoxuan He",
      "Yong Li",
      "Fengyun Rao",
      "Jing Lyu",
      "Wei Zhai",
      "Yang Cao",
      "Zheng-Jun Zha"
    ],
    "published": "2025-12-02T09:02:20+00:00",
    "url": "https://arxiv.org/pdf/2512.02536v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02530v1",
    "title": "Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration",
    "abstract": "The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.",
    "authors": [
      "Yuxiang He",
      "Jian Zhao",
      "Yuchen Yuan",
      "Tianle Zhang",
      "Wei Cai",
      "Haojie Cheng",
      "Ziyan Shi",
      "Ming Zhu",
      "Haichuan Tang",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "published": "2025-12-02T08:49:54+00:00",
    "url": "https://arxiv.org/pdf/2512.02530v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02527v1",
    "title": "A Concise Review of Hallucinations in LLMs and their Mitigation",
    "abstract": "Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.",
    "authors": [
      "Parth Pulkundwar",
      "Vivek Dhanawade",
      "Rohit Yadav",
      "Minal Sonkar",
      "Medha Asurlekar",
      "Sarita Rathod"
    ],
    "published": "2025-12-02T08:44:17+00:00",
    "url": "https://arxiv.org/pdf/2512.02527v1",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03112v1",
    "title": "Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability",
    "abstract": "Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.",
    "authors": [
      "Jialai She"
    ],
    "published": "2025-12-02T08:34:43+00:00",
    "url": "https://arxiv.org/pdf/2512.03112v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.03111v1",
    "title": "PanFoMa: A Lightweight Foundation Model and Benchmark for Pan-Cancer",
    "abstract": "Single-cell RNA sequencing (scRNA-seq) is essential for decoding tumor heterogeneity. However, pan-cancer research still faces two key challenges: learning discriminative and efficient single-cell representations, and establishing a comprehensive evaluation benchmark. In this paper, we introduce PanFoMa, a lightweight hybrid neural network that combines the strengths of Transformers and state-space models to achieve a balance between performance and efficiency. PanFoMa consists of a front-end local-context encoder with shared self-attention layers to capture complex, order-independent gene interactions; and a back-end global sequential feature decoder that efficiently integrates global context using a linear-time state-space model. This modular design preserves the expressive power of Transformers while leveraging the scalability of Mamba to enable transcriptome modeling, effectively capturing both local and global regulatory signals. To enable robust evaluation, we also construct a large-scale pan-cancer single-cell benchmark, PanFoMaBench, containing over 3.5 million high-quality cells across 33 cancer subtypes, curated through a rigorous preprocessing pipeline. Experimental results show that PanFoMa outperforms state-of-the-art models on our pan-cancer benchmark (+4.0\\%) and across multiple public tasks, including cell type annotation (+7.4\\%), batch integration (+4.0\\%) and multi-omics integration (+3.1\\%). The code is available at https://github.com/Xiaoshui-Huang/PanFoMa.",
    "authors": [
      "Xiaoshui Huang",
      "Tianlin Zhu",
      "Yifan Zuo",
      "Xue Xia",
      "Zonghan Wu",
      "Jiebin Yan",
      "Dingli Hua",
      "Zongyi Xu",
      "Yuming Fang",
      "Jian Zhang"
    ],
    "published": "2025-12-02T08:31:31+00:00",
    "url": "https://arxiv.org/pdf/2512.03111v1",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02520v1",
    "title": "On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection",
    "abstract": "Zero-shot anomaly classification and segmentation (AC/AS) aim to detect anomalous samples and regions without any training data, a capability increasingly crucial in industrial inspection and medical imaging. This dissertation aims to investigate the core challenges of zero-shot AC/AS and presents principled solutions rooted in theory and algorithmic design.   We first formalize the problem of consistent anomalies, a failure mode in which recurring similar anomalies systematically bias distance-based methods. By analyzing the statistical and geometric behavior of patch representations from pre-trained Vision Transformers, we identify two key phenomena - similarity scaling and neighbor-burnout - that describe how relationships among normal patches change with and without consistent anomalies in settings characterized by highly similar objects.   We then introduce CoDeGraph, a graph-based framework for filtering consistent anomalies built on the similarity scaling and neighbor-burnout phenomena. Through multi-stage graph construction, community detection, and structured refinement, CoDeGraph effectively suppresses the influence of consistent anomalies.   Next, we extend this framework to 3D medical imaging by proposing a training-free, computationally efficient volumetric tokenization strategy for MRI data. This enables a genuinely zero-shot 3D anomaly detection pipeline and shows that volumetric anomaly segmentation is achievable without any 3D training samples.   Finally, we bridge batch-based and text-based zero-shot methods by demonstrating that CoDeGraph-derived pseudo-masks can supervise prompt-driven vision-language models. Together, this dissertation provides theoretical understanding and practical solutions for the zero-shot AC/AS problem.",
    "authors": [
      "Tai Le-Gia"
    ],
    "published": "2025-12-02T08:23:03+00:00",
    "url": "https://arxiv.org/pdf/2512.02520v1",
    "categories": [
      "cs.CV",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.02517v1",
    "title": "SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts",
    "abstract": "The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.",
    "authors": [
      "Jiaqi Liu",
      "Ronghao Fu",
      "Lang Sun",
      "Haoran Liu",
      "Xiao Yang",
      "Weipeng Zhang",
      "Xu Na",
      "Zhuoran Duan",
      "Bo Yang"
    ],
    "published": "2025-12-02T08:17:16+00:00",
    "url": "https://arxiv.org/pdf/2512.02517v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02512v2",
    "title": "Two-Stage Vision Transformer for Image Restoration: Colorization Pretraining + Residual Upsampling",
    "abstract": "In computer vision, Single Image Super-Resolution (SISR) is still a difficult problem. We present ViT-SR, a new technique to improve the performance of a Vision Transformer (ViT) employing a two-stage training strategy. In our method, the model learns rich, generalizable visual representations from the data itself through a self-supervised pretraining phase on a colourization task. The pre-trained model is then adjusted for 4x super-resolution. By predicting the addition of a high-frequency residual image to an initial bicubic interpolation, this design simplifies residual learning. ViT-SR, trained and evaluated on the DIV2K benchmark dataset, achieves an impressive SSIM of 0.712 and PSNR of 22.90 dB. These results demonstrate the efficacy of our two-stage approach and highlight the potential of self-supervised pre-training for complex image restoration tasks. Further improvements may be possible with larger ViT architectures or alternative pretext tasks.",
    "authors": [
      "Aditya Chaudhary",
      "Prachet Dev Singh",
      "Ankit Jha"
    ],
    "published": "2025-12-02T08:10:55+00:00",
    "url": "https://arxiv.org/pdf/2512.02512v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02505v1",
    "title": "GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding",
    "abstract": "Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.",
    "authors": [
      "Jiaqi Liu",
      "Ronghao Fu",
      "Haoran Liu",
      "Lang Sun",
      "Bo Yang"
    ],
    "published": "2025-12-02T07:59:46+00:00",
    "url": "https://arxiv.org/pdf/2512.02505v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02502v1",
    "title": "AskNearby: An LLM-Based Application for Neighborhood Information Retrieval and Personalized Cognitive-Map Recommendations",
    "abstract": "The \"15-minute city\" envisions neighborhoods where residents can meet daily needs via a short walk or bike ride. Realizing this vision requires not only physical proximity but also efficient and reliable access to information about nearby places, services, and events. Existing location-based systems, however, focus mainly on city-level tasks and neglect the spatial, temporal, and cognitive factors that shape localized decision-making. We conceptualize this gap as the Local Life Information Accessibility (LLIA) problem and introduce AskNearby, an AI-driven community application that unifies retrieval and recommendation within the 15-minute life circle. AskNearby integrates (i) a three-layer Retrieval-Augmented Generation (RAG) pipeline that synergizes graph-based, semantic-vector, and geographic retrieval with (ii) a cognitive-map model that encodes each user's neighborhood familiarity and preferences. Experiments on real-world community datasets demonstrate that AskNearby significantly outperforms LLM-based and map-based baselines in retrieval accuracy and recommendation quality, achieving robust performance in spatiotemporal grounding and cognitive-aware ranking. Real-world deployments further validate its effectiveness. By addressing the LLIA challenge, AskNearby empowers residents to more effectively discover local resources, plan daily activities, and engage in community life.",
    "authors": [
      "Luyao Niu",
      "Zhicheng Deng",
      "Boyang Li",
      "Nuoxian Huang",
      "Ruiqi Liu",
      "Wenjia Zhang"
    ],
    "published": "2025-12-02T07:47:31+00:00",
    "url": "https://arxiv.org/pdf/2512.02502v1",
    "categories": [
      "cs.IR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03110v1",
    "title": "The BEAT-CF Causal Model: A model for guiding the design of trials and observational analyses of cystic fibrosis exacerbations",
    "abstract": "Loss of lung function in cystic fibrosis (CF) occurs progressively, punctuated by acute pulmonary exacerbations (PEx) in which abrupt declines in lung function are not fully recovered. A key component of CF management over the past half century has been the treatment of PEx to slow lung function decline. This has been credited with improvements in survival for people with CF (PwCF), but there is no consensus on the optimal approach to PEx management. BEAT-CF (Bayesian evidence-adaptive treatment of CF) was established to build an evidence-informed knowledge base for CF management. The BEAT-CF causal model is a directed acyclic graph (DAG) and Bayesian network (BN) for PEx that aims to inform the design and analysis of clinical trials comparing the effectiveness of alternative approaches to PEx management. The causal model describes relationships between background risk factors, treatments, and pathogen colonisation of the airways that affect the outcome of an individual PEx episode. The key factors, outcomes, and causal relationships were elicited from CF clinical experts and together represent current expert understanding of the pathophysiology of a PEx episode, guiding the design of data collection and studies and enabling causal inference. Here, we present the DAG that documents this understanding, along with the processes used in its development, providing transparency around our trial design and study processes, as well as a reusable framework for others.",
    "authors": [
      "Steven Mascaro",
      "Owen Woodberry",
      "Charlie McLeod",
      "Mitch Messer",
      "Hiran Selvadurai",
      "Yue Wu",
      "Andre Schultz",
      "Thomas L Snelling"
    ],
    "published": "2025-12-02T07:46:42+00:00",
    "url": "https://arxiv.org/pdf/2512.03110v1",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02499v1",
    "title": "COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes",
    "abstract": "Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.",
    "authors": [
      "Yongkai Liu",
      "Helena Feng",
      "Bin Jiang",
      "Yixin Wang",
      "Max Wintermark",
      "David S. Liebeskind",
      "Michael Moseley",
      "Maarten Lansberg",
      "Gregory Albers",
      "Jeremy Heit",
      "Greg Zaharchuk"
    ],
    "published": "2025-12-02T07:44:20+00:00",
    "url": "https://arxiv.org/pdf/2512.02499v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02498v1",
    "title": "dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model",
    "abstract": "Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots.ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, dots.ocr establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.",
    "authors": [
      "Yumeng Li",
      "Guang Yang",
      "Hao Liu",
      "Bowen Wang",
      "Colin Zhang"
    ],
    "published": "2025-12-02T07:42:38+00:00",
    "url": "https://arxiv.org/pdf/2512.02498v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02497v1",
    "title": "A Large Scale Benchmark for Test Time Adaptation Methods in Medical Image Segmentation",
    "abstract": "Test time Adaptation is a promising approach for mitigating domain shift in medical image segmentation; however, current evaluations remain limited in terms of modality coverage, task diversity, and methodological consistency. We present MedSeg-TTA, a comprehensive benchmark that examines twenty representative adaptation methods across seven imaging modalities, including MRI, CT, ultrasound, pathology, dermoscopy, OCT, and chest X-ray, under fully unified data preprocessing, backbone configuration, and test time protocols. The benchmark encompasses four significant adaptation paradigms: Input-level Transformation, Feature-level Alignment, Output-level Regularization, and Prior Estimation, enabling the first systematic cross-modality comparison of their reliability and applicability. The results show that no single paradigm performs best in all conditions. Input-level methods are more stable under mild appearance shifts. Feature-level and Output-level methods offer greater advantages in boundary-related metrics, whereas prior-based methods exhibit strong modality dependence. Several methods degrade significantly under large inter-center and inter-device shifts, which highlights the importance of principled method selection for clinical deployment. MedSeg-TTA provides standardized datasets, validated implementations, and a public leaderboard, establishing a rigorous foundation for future research on robust, clinically reliable test-time adaptation. All source codes and open-source datasets are available at https://github.com/wenjing-gg/MedSeg-TTA.",
    "authors": [
      "Wenjing Yu",
      "Shuo Jiang",
      "Yifei Chen",
      "Shuo Chang",
      "Yuanhan Wang",
      "Beining Wu",
      "Jie Dong",
      "Mingxuan Liu",
      "Shenghao Zhu",
      "Feiwei Qin",
      "Changmiao Wang",
      "Qiyuan Tian"
    ],
    "published": "2025-12-02T07:40:42+00:00",
    "url": "https://arxiv.org/pdf/2512.02497v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02496v1",
    "title": "Attention-guided reference point shifting for Gaussian-mixture-based partial point set registration",
    "abstract": "This study investigates the impact of the invariance of feature vectors for partial-to-partial point set registration under translation and rotation of input point sets, particularly in the realm of techniques based on deep learning and Gaussian mixture models (GMMs). We reveal both theoretical and practical problems associated with such deep-learning-based registration methods using GMMs, with a particular focus on the limitations of DeepGMR, a pioneering study in this line, to the partial-to-partial point set registration. Our primary goal is to uncover the causes behind such methods and propose a comprehensible solution for that. To address this, we introduce an attention-based reference point shifting (ARPS) layer, which robustly identifies a common reference point of two partial point sets, thereby acquiring transformation-invariant features. The ARPS layer employs a well-studied attention module to find a common reference point rather than the overlap region. Owing to this, it significantly enhances the performance of DeepGMR and its recent variant, UGMMReg. Furthermore, these extension models outperform even prior deep learning methods using attention blocks and Transformer to extract the overlap region or common reference points. We believe these findings provide deeper insights into registration methods using deep learning and GMMs.",
    "authors": [
      "Mizuki Kikkawa",
      "Tatsuya Yatagawa",
      "Yutaka Ohtake",
      "Hiromasa Suzuki"
    ],
    "published": "2025-12-02T07:38:55+00:00",
    "url": "https://arxiv.org/pdf/2512.02496v1",
    "categories": [
      "cs.CV",
      "cs.GR"
    ]
  },
  {
    "arxiv_id": "2512.02492v1",
    "title": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
    "abstract": "While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",
    "authors": [
      "Jiahui Chen",
      "Weida Wang",
      "Runhua Shi",
      "Huan Yang",
      "Chaofan Ding",
      "Zihao Chen"
    ],
    "published": "2025-12-02T07:31:19+00:00",
    "url": "https://arxiv.org/pdf/2512.02492v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02487v1",
    "title": "Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding",
    "abstract": "Recent advances in 3D scene-language understanding have leveraged Large Language Models (LLMs) for 3D reasoning by transferring their general reasoning ability to 3D multi-modal contexts. However, existing methods typically adopt standard decoders from language modeling, which rely on a causal attention mask. This design introduces two fundamental conflicts in 3D scene understanding: sequential bias among order-agnostic 3D objects and restricted object-instruction attention, hindering task-specific reasoning. To overcome these limitations, we propose 3D Spatial Language Instruction Mask (3D-SLIM), an effective masking strategy that replaces the causal mask with an adaptive attention mask tailored to the spatial structure of 3D scenes. Our 3D-SLIM introduces two key components: a Geometry-adaptive Mask that constrains attention based on spatial density rather than token order, and an Instruction-aware Mask that enables object tokens to directly access instruction context. This design allows the model to process objects based on their spatial relationships while being guided by the user's task. 3D-SLIM is simple, requires no architectural modifications, and adds no extra parameters, yet it yields substantial performance improvements across diverse 3D scene-language tasks. Extensive experiments across multiple benchmarks and LLM baselines validate its effectiveness and underscore the critical role of decoder design in 3D multi-modal reasoning.",
    "authors": [
      "Yerim Jeon",
      "Miso Lee",
      "WonJun Moon",
      "Jae-Pil Heo"
    ],
    "published": "2025-12-02T07:22:36+00:00",
    "url": "https://arxiv.org/pdf/2512.02487v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02485v1",
    "title": "UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making",
    "abstract": "Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.",
    "authors": [
      "Qianhan Feng",
      "Zhongzhen Huang",
      "Yakun Zhu",
      "Xiaofan Zhang",
      "Qi Dou"
    ],
    "published": "2025-12-02T07:20:21+00:00",
    "url": "https://arxiv.org/pdf/2512.02485v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02482v1",
    "title": "G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline",
    "abstract": "We propose G-SHARP, a commercially compatible, real-time surgical scene reconstruction framework designed for minimally invasive procedures that require fast and accurate 3D modeling of deformable tissue. While recent Gaussian splatting approaches have advanced real-time endoscopic reconstruction, existing implementations often depend on non-commercial derivatives, limiting deployability. G-SHARP overcomes these constraints by being the first surgical pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer, enabling principled deformation modeling, robust occlusion handling, and high-fidelity reconstructions on the EndoNeRF pulling benchmark. Our results demonstrate state-of-the-art reconstruction quality with strong speed-accuracy trade-offs suitable for intra-operative use. Finally, we provide a Holoscan SDK application that deploys G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in practical operating-room settings.",
    "authors": [
      "Vishwesh Nath",
      "Javier G. Tejero",
      "Ruilong Li",
      "Filippo Filicori",
      "Mahdi Azizian",
      "Sean D. Huver"
    ],
    "published": "2025-12-02T07:18:46+00:00",
    "url": "https://arxiv.org/pdf/2512.02482v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02474v1",
    "title": "Q-BERT4Rec: Quantized Semantic-ID Representation Learning for Multimodal Recommendation",
    "abstract": "Sequential recommendation plays a critical role in modern online platforms such as e-commerce, advertising, and content streaming, where accurately predicting users' next interactions is essential for personalization. Recent Transformer-based methods like BERT4Rec have shown strong modeling capability, yet they still rely on discrete item IDs that lack semantic meaning and ignore rich multimodal information (e.g., text and image). This leads to weak generalization and limited interpretability. To address these challenges, we propose Q-Bert4Rec, a multimodal sequential recommendation framework that unifies semantic representation and quantized modeling. Specifically, Q-Bert4Rec consists of three stages: (1) cross-modal semantic injection, which enriches randomly initialized ID embeddings through a dynamic transformer that fuses textual, visual, and structural features; (2) semantic quantization, which discretizes fused representations into meaningful tokens via residual vector quantization; and (3) multi-mask pretraining and fine-tuning, which leverage diverse masking strategies -- span, tail, and multi-region -- to improve sequential understanding. We validate our model on public Amazon benchmarks and demonstrate that Q-Bert4Rec significantly outperforms many strong existing methods, confirming the effectiveness of semantic tokenization for multimodal sequential recommendation. Our source code will be publicly available on GitHub after publishing.",
    "authors": [
      "Haofeng Huang",
      "Ling Gai"
    ],
    "published": "2025-12-02T07:06:44+00:00",
    "url": "https://arxiv.org/pdf/2512.02474v1",
    "categories": [
      "cs.IR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02473v1",
    "title": "WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling",
    "abstract": "Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.",
    "authors": [
      "Yuta Oshima",
      "Yusuke Iwasawa",
      "Masahiro Suzuki",
      "Yutaka Matsuo",
      "Hiroki Furuta"
    ],
    "published": "2025-12-02T07:06:23+00:00",
    "url": "https://arxiv.org/pdf/2512.02473v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02472v1",
    "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
    "abstract": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
    "authors": [
      "Wenhao Yu",
      "Zhenwen Liang",
      "Chengsong Huang",
      "Kishan Panaganti",
      "Tianqing Fang",
      "Haitao Mi",
      "Dong Yu"
    ],
    "published": "2025-12-02T07:06:11+00:00",
    "url": "https://arxiv.org/pdf/2512.02472v1",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02471v1",
    "title": "scCluBench: Comprehensive Benchmarking of Clustering Algorithms for Single-Cell RNA Sequencing",
    "abstract": "Cell clustering is crucial for uncovering cellular heterogeneity in single-cell RNA sequencing (scRNA-seq) data by identifying cell types and marker genes. Despite its importance, benchmarks for scRNA-seq clustering methods remain fragmented, often lacking standardized protocols and failing to incorporate recent advances in artificial intelligence. To fill these gaps, we present scCluBench, a comprehensive benchmark of clustering algorithms for scRNA-seq data. First, scCluBench provides 36 scRNA-seq datasets collected from diverse public sources, covering multiple tissues, which are uniformly processed and standardized to ensure consistency for systematic evaluation and downstream analyses. To evaluate performance, we collect and reproduce a range of scRNA-seq clustering methods, including traditional, deep learning-based, graph-based, and biological foundation models. We comprehensively evaluate each method both quantitatively and qualitatively, using core performance metrics as well as visualization analyses. Furthermore, we construct representative downstream biological tasks, such as marker gene identification and cell type annotation, to further assess the practical utility. scCluBench then investigates the performance differences and applicability boundaries of various clustering models across diverse analytical tasks, systematically assessing their robustness and scalability in real-world scenarios. Overall, scCluBench offers a standardized and user-friendly benchmark for scRNA-seq clustering, with curated datasets, unified evaluation protocols, and transparent analyses, facilitating informed method selection and providing valuable insights into model generalizability and application scope.",
    "authors": [
      "Ping Xu",
      "Zaitian Wang",
      "Zhirui Wang",
      "Pengjiang Li",
      "Jiajia Wang",
      "Ran Zhang",
      "Pengfei Wang",
      "Yuanchun Zhou"
    ],
    "published": "2025-12-02T07:04:38+00:00",
    "url": "https://arxiv.org/pdf/2512.02471v1",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02469v1",
    "title": "TGDD: Trajectory Guided Dataset Distillation with Balanced Distribution",
    "abstract": "Dataset distillation compresses large datasets into compact synthetic ones to reduce storage and computational costs. Among various approaches, distribution matching (DM)-based methods have attracted attention for their high efficiency. However, they often overlook the evolution of feature representations during training, which limits the expressiveness of synthetic data and weakens downstream performance. To address this issue, we propose Trajectory Guided Dataset Distillation (TGDD), which reformulates distribution matching as a dynamic alignment process along the model's training trajectory. At each training stage, TGDD captures evolving semantics by aligning the feature distribution between the synthetic and original dataset. Meanwhile, it introduces a distribution constraint regularization to reduce class overlap. This design helps synthetic data preserve both semantic diversity and representativeness, improving performance in downstream tasks. Without additional optimization overhead, TGDD achieves a favorable balance between performance and efficiency. Experiments on ten datasets demonstrate that TGDD achieves state-of-the-art performance, notably a 5.0% accuracy gain on high-resolution benchmarks.",
    "authors": [
      "Fengli Ran",
      "Xiao Pu",
      "Bo Liu",
      "Xiuli Bi",
      "Bin Xiao"
    ],
    "published": "2025-12-02T07:00:07+00:00",
    "url": "https://arxiv.org/pdf/2512.02469v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02465v1",
    "title": "TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links",
    "abstract": "In the face of accelerating global urbanization and the increasing frequency of extreme weather events, highresolution urban rainfall monitoring is crucial for building resilient smart cities. Commercial Microwave Links (CMLs) are an emerging data source with great potential for this task.While traditional rainfall retrieval from CMLs relies on physicsbased models, these often struggle with real-world complexities like signal noise and nonlinear attenuation. To address these limitations, this paper proposes a novel hybrid deep learning architecture based on the Transformer and a Bidirectional Gated Recurrent Unit (BiGRU), which we name TabGRU. This design synergistically captures both long-term dependencies and local sequential features in the CML signal data. The model is further enhanced by a learnable positional embedding and an attention pooling mechanism to improve its dynamic feature extraction and generalization capabilities. The model was validated on a public benchmark dataset from Gothenburg, Sweden (June-September 2015). The evaluation used 12 sub-links from two rain gauges (Torp and Barl) over a test period (August 22-31) covering approximately 10 distinct rainfall events. The proposed TabGRU model demonstrated consistent advantages, outperforming deep learning baselines and achieving high coefficients of determination (R2) at both the Torp site (0.91) and the Barl site (0.96). Furthermore, compared to the physics-based approach, TabGRU maintained higher accuracy and was particularly effective in mitigating the significant overestimation problem observed in the PL model during peak rainfall events. This evaluation confirms that the TabGRU model can effectively overcome the limitations of traditional methods, providing a robust and accurate solution for CML-based urban rainfall monitoring under the tested conditions.",
    "authors": [
      "Xingwang Li",
      "Mengyun Chen",
      "Jiamou Liu",
      "Sijie Wang",
      "Shuanggen Jin",
      "Jafet C. M. Andersson",
      "Jonas Olsson",
      "Remco",
      "van de Beek",
      "Hai Victor Habi",
      "Congzheng Han"
    ],
    "published": "2025-12-02T06:50:50+00:00",
    "url": "https://arxiv.org/pdf/2512.02465v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02458v1",
    "title": "Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration",
    "abstract": "Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.",
    "authors": [
      "Zhongyi Cai",
      "Yi Du",
      "Chen Wang",
      "Yu Kong"
    ],
    "published": "2025-12-02T06:35:30+00:00",
    "url": "https://arxiv.org/pdf/2512.02458v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02457v2",
    "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation",
    "abstract": "Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.",
    "authors": [
      "Jianzong Wu",
      "Hao Lian",
      "Dachao Hao",
      "Ye Tian",
      "Qingyu Shi",
      "Biaolong Chen",
      "Hao Jiang",
      "Yunhai Tong"
    ],
    "published": "2025-12-02T06:31:38+00:00",
    "url": "https://arxiv.org/pdf/2512.02457v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02456v1",
    "title": "See, Think, Learn: A Self-Taught Multimodal Reasoner",
    "abstract": "Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.",
    "authors": [
      "Sourabh Sharma",
      "Sonam Gupta",
      "Sadbhawna"
    ],
    "published": "2025-12-02T06:30:10+00:00",
    "url": "https://arxiv.org/pdf/2512.02456v1",
    "categories": [
      "cs.CV",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02453v1",
    "title": "ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation",
    "abstract": "Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion. However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge. In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation. Instead of learning an unstructured embedding from each style motion, we leverage a set of prototypes to effectively model diverse style patterns across motions belonging to the same style category. We consider two types of style diversity: global-level diversity among style motions of the same category, and local-level diversity within the temporal dynamics of motion sequences. These components jointly shape two structured style embedding spaces, i.e., global and local, optimized via alignment with non-learnable prototype anchors. Furthermore, we augment the pretrained text-to-motion generation model with the Stylistic Modulation Adapter (SMA) to integrate the style features. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art models in stylized motion generation and motion style transfer.",
    "authors": [
      "Kerui Chen",
      "Jianrong Zhang",
      "Ming Li",
      "Zhonglong Zheng",
      "Hehe Fan"
    ],
    "published": "2025-12-02T06:24:14+00:00",
    "url": "https://arxiv.org/pdf/2512.02453v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02450v1",
    "title": "HouseLayout3D: A Benchmark and Training-Free Baseline for 3D Layout Estimation in the Wild",
    "abstract": "Current 3D layout estimation models are primarily trained on synthetic datasets containing simple single room or single floor environments. As a consequence, they cannot natively handle large multi floor buildings and require scenes to be split into individual floors before processing, which removes global spatial context that is essential for reasoning about structures such as staircases that connect multiple levels. In this work, we introduce HouseLayout3D, a real world benchmark designed to support progress toward full building scale layout estimation, including multiple floors and architecturally intricate spaces. We also present MultiFloor3D, a simple training free baseline that leverages recent scene understanding methods and already outperforms existing 3D layout estimation models on both our benchmark and prior datasets, highlighting the need for further research in this direction. Data and code are available at: https://houselayout3d.github.io.",
    "authors": [
      "Valentin Bieri",
      "Marie-Julie Rakotosaona",
      "Keisuke Tateno",
      "Francis Engelmann",
      "Leonidas Guibas"
    ],
    "published": "2025-12-02T06:18:08+00:00",
    "url": "https://arxiv.org/pdf/2512.02450v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02448v1",
    "title": "nuScenes Revisited: Progress and Challenges in Autonomous Driving",
    "abstract": "Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS) have been revolutionized by Deep Learning. As a data-driven approach, Deep Learning relies on vast amounts of driving data, typically labeled in great detail. As a result, datasets, alongside hardware and algorithms, are foundational building blocks for the development of AVs. In this work we revisit one of the most widely used autonomous driving datasets: the nuScenes dataset. nuScenes exemplifies key trends in AV development, being the first dataset to include radar data, to feature diverse urban driving scenes from two continents, and to be collected using a fully autonomous vehicle operating on public roads, while also promoting multi-modal sensor fusion, standardized benchmarks, and a broad range of tasks including perception, localization \\& mapping, prediction and planning. We provide an unprecedented look into the creation of nuScenes, as well as its extensions nuImages and Panoptic nuScenes, summarizing many technical details that have hitherto not been revealed in academic publications. Furthermore, we trace how the influence of nuScenes impacted a large number of other datasets that were released later and how it defined numerous standards that are used by the community to this day. Finally, we present an overview of both official and unofficial tasks using the nuScenes dataset and review major methodological developments, thereby offering a comprehensive survey of the autonomous driving literature, with a particular focus on nuScenes.",
    "authors": [
      "Whye Kit Fong",
      "Venice Erin Liong",
      "Kok Seang Tan",
      "Holger Caesar"
    ],
    "published": "2025-12-02T06:14:28+00:00",
    "url": "https://arxiv.org/pdf/2512.02448v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.02447v1",
    "title": "Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors",
    "abstract": "Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.",
    "authors": [
      "Fan Luo",
      "Zeyu Gao",
      "Xinhao Luo",
      "Kai Zhao",
      "Yanfeng Lu"
    ],
    "published": "2025-12-02T06:13:27+00:00",
    "url": "https://arxiv.org/pdf/2512.02447v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02445v1",
    "title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents",
    "abstract": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.",
    "authors": [
      "Tsimur Hadeliya",
      "Mohammad Ali Jauhar",
      "Nidhi Sakpal",
      "Diogo Cruz"
    ],
    "published": "2025-12-02T06:12:02+00:00",
    "url": "https://arxiv.org/pdf/2512.02445v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02441v1",
    "title": "Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation",
    "abstract": "Adapting large pre-trained models to unseen tasks under tight data and compute budgets remains challenging. Meta-learning approaches explicitly learn good initializations, but they require an additional meta-training phase over many tasks, incur high training cost, and can be unstable. At the same time, the number of task-specific pre-trained models continues to grow, yet the question of how to transfer them to new tasks with minimal additional training remains relatively underexplored. We propose BOLT (Basis-Oriented Low-rank Transfer), a framework that reuses existing fine-tuned models not by merging weights, but instead by extracting an orthogonal, task-informed spectral basis and adapting within that subspace. In the offline phase, BOLT collects dominant singular directions from multiple task vectors and orthogonalizes them per layer to form reusable bases. In the online phase, we freeze these bases and train only a small set of diagonal coefficients per layer for the new task, yielding a rank-controlled update with very few trainable parameters. This design provides (i) a strong, training-free initialization for unseen tasks, obtained by pooling source-task coefficients, along with a lightweight rescaling step while leveraging the shared orthogonal bases, and (ii) a parameter-efficient fine-tuning (PEFT) path that, in our experiments, achieves robust performance compared to common PEFT baselines as well as a representative meta-learned initialization. Our results show that constraining adaptation to a task-informed orthogonal subspace provides an effective alternative for unseen-task transfer.",
    "authors": [
      "Junghwan Park",
      "Woojin Cho",
      "Junhyuk Heo",
      "Darongsae Kwon",
      "Kookjin Lee"
    ],
    "published": "2025-12-02T06:00:16+00:00",
    "url": "https://arxiv.org/pdf/2512.02441v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03109v1",
    "title": "E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing",
    "abstract": "Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.",
    "authors": [
      "Shuvom Sadhuka",
      "Drew Prinster",
      "Clara Fannjiang",
      "Gabriele Scalia",
      "Aviv Regev",
      "Hanchen Wang"
    ],
    "published": "2025-12-02T05:59:18+00:00",
    "url": "https://arxiv.org/pdf/2512.03109v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.02438v1",
    "title": "Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources",
    "abstract": "In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .",
    "authors": [
      "Phuc Pham",
      "Nhu Pham",
      "Ngoc Quoc Ly"
    ],
    "published": "2025-12-02T05:53:51+00:00",
    "url": "https://arxiv.org/pdf/2512.02438v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02437v1",
    "title": "LightHCG: a Lightweight yet powerful HSIC Disentanglement based Causal Glaucoma Detection Model framework",
    "abstract": "As a representative optic degenerative condition, glaucoma has been a threat to millions due to its irreversibility and severe impact on human vision fields. Mainly characterized by dimmed and blurred visions, or peripheral vision loss, glaucoma is well known to occur due to damages in the optic nerve from increased intraocular pressure (IOP) or neovascularization within the retina. Traditionally, most glaucoma related works and clinical diagnosis focused on detecting these damages in the optic nerve by using patient data from perimetry tests, optic papilla inspections and tonometer-based IOP measurements. Recently, with advancements in computer vision AI models, such as VGG16 or Vision Transformers (ViT), AI-automatized glaucoma detection and optic cup segmentation based on retinal fundus images or OCT recently exhibited significant performance in aiding conventional diagnosis with high performance. However, current AI-driven glaucoma detection approaches still have significant room for improvement in terms of reliability, excessive parameter usage, possibility of spurious correlation within detection, and limitations in applications to intervention analysis or clinical simulations. Thus, this research introduced a novel causal representation driven glaucoma detection model: LightHCG, an extremely lightweight Convolutional VAE-based latent glaucoma representation model that can consider the true causality among glaucoma-related physical factors within the optic nerve region. Using HSIC-based latent space disentanglement and Graph Autoencoder based unsupervised causal representation learning, LightHCG not only exhibits higher performance in classifying glaucoma with 93~99% less weights, but also enhances the possibility of AI-driven intervention analysis, compared to existing advanced vision models such as InceptionV3, MobileNetV2 or VGG16.",
    "authors": [
      "Daeyoung Kim"
    ],
    "published": "2025-12-02T05:52:15+00:00",
    "url": "https://arxiv.org/pdf/2512.02437v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02436v1",
    "title": "Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets",
    "abstract": "Prediction markets allow users to trade on outcomes of real-world events, but are prone to fragmentation through overlapping questions, implicit equivalences, and hidden contradictions across markets. We present an agentic AI pipeline that autonomously (i) clusters markets into coherent topical groups using natural-language understanding over contract text and metadata, and (ii) identifies within-cluster market pairs whose resolved outcomes exhibit strong dependence, including same-outcome (correlated) and different-outcome (anti-correlated) relationships. Using a historical dataset of resolved markets on Polymarket, we evaluate the accuracy of the agent's relational predictions. We then translate discovered relationships into a simple trading strategy to quantify how these relationships map to actionable signals. Results show that agent-identified relationships achieve roughly 60-70% accuracy, and their induced trading strategies earn about 20% average returns over week-long horizons, highlighting the ability of agentic AI and large language models to uncover latent semantic structure in prediction markets.",
    "authors": [
      "Agostino Capponi",
      "Alfio Gliozzo",
      "Brian Zhu"
    ],
    "published": "2025-12-02T05:45:48+00:00",
    "url": "https://arxiv.org/pdf/2512.02436v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03107v1",
    "title": "Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%",
    "abstract": "Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.",
    "authors": [
      "Mainak Singha"
    ],
    "published": "2025-12-02T05:25:48+00:00",
    "url": "https://arxiv.org/pdf/2512.03107v1",
    "categories": [
      "cs.LG",
      "cs.CL",
      "q-fin.CP",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.02425v1",
    "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
    "abstract": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
    "authors": [
      "Woongyeong Yeo",
      "Kangsan Kim",
      "Jaehong Yoon",
      "Sung Ju Hwang"
    ],
    "published": "2025-12-02T05:14:52+00:00",
    "url": "https://arxiv.org/pdf/2512.02425v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02423v1",
    "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
    "abstract": "With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.",
    "authors": [
      "Haolong Yan",
      "Yeqing Shen",
      "Xin Huang",
      "Jia Wang",
      "Kaijun Tan",
      "Zhixuan Liang",
      "Hongxin Li",
      "Zheng Ge",
      "Osamu Yoshie",
      "Si Li",
      "Xiangyu Zhang",
      "Daxin Jiang"
    ],
    "published": "2025-12-02T05:11:23+00:00",
    "url": "https://arxiv.org/pdf/2512.02423v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02422v1",
    "title": "Quantum feature encoding optimization",
    "abstract": "Quantum Machine Learning (QML) holds the promise of enhancing machine learning modeling in terms of both complexity and accuracy. A key challenge in this domain is the encoding of input data, which plays a pivotal role in determining the performance of QML models. In this work, we tackle a largely unaddressed aspect of encoding that is unique to QML modeling -- rather than adjusting the ansatz used for encoding, we consider adjusting how data is conveyed to the ansatz. We specifically implement QML pipelines that leverage classical data manipulation (i.e., ordering, selecting, and weighting features) as a preprocessing step, and evaluate if these aspects of encoding can have a significant impact on QML model performance, and if they can be effectively optimized to improve performance. Our experimental results, applied across a wide variety of data sets, ansatz, and circuit sizes, with a representative QML approach, demonstrate that by optimizing how features are encoded in an ansatz we can substantially and consistently improve the performance of QML models, making a compelling case for integrating these techniques in future QML applications. Finally we demonstrate the practical feasibility of this approach by running it using real quantum hardware with 100 qubit circuits and successfully achieving improved QML modeling performance in this case as well.",
    "authors": [
      "Tommaso Fioravanti",
      "Brian Quanz",
      "Gabriele Agliardi",
      "Edgar Andres Ruiz Guzman",
      "Gin\u00e9s Carrascal",
      "Jae-Eun Park"
    ],
    "published": "2025-12-02T05:07:32+00:00",
    "url": "https://arxiv.org/pdf/2512.02422v1",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02421v1",
    "title": "Generalizing Vision-Language Models with Dedicated Prompt Guidance",
    "abstract": "Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.",
    "authors": [
      "Xinyao Li",
      "Yinjie Min",
      "Hongbo Chen",
      "Zhekai Du",
      "Fengling Li",
      "Jingjing Li"
    ],
    "published": "2025-12-02T05:06:17+00:00",
    "url": "https://arxiv.org/pdf/2512.02421v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02419v1",
    "title": "The brain-AI convergence: Predictive and generative world models for general-purpose computation",
    "abstract": "Recent advances in general-purpose AI systems with attention-based transformers offer a potential window into how the neocortex and cerebellum, despite their relatively uniform circuit architectures, give rise to diverse functions and, ultimately, to human intelligence. This Perspective provides a cross-domain comparison between the brain and AI that goes beyond the traditional focus on visual processing, adopting the emerging perspecive of world-model-based computation. Here, we identify shared computational mechanisms in the attention-based neocortex and the non-attentional cerebellum: both predict future world events from past inputs and construct internal world models through prediction-error learning. These predictive world models are repurposed for seemingly distinct functions -- understanding in sensory processing and generation in motor processing -- enabling the brain to achieve multi-domain capabilities and human-like adaptive intelligence. Notably, attention-based AI has independently converged on a similar learning paradigm and world-model-based computation. We conclude that these shared mechanisms in both biological and artificial systems constitute a core computational foundation for realizing diverse functions including high-level intelligence, despite their relatively uniform circuit structures. Our theoretical insights bridge neuroscience and AI, advancing our understanding of the computational essence of intelligence.",
    "authors": [
      "Shogo Ohmae",
      "Keiko Ohmae"
    ],
    "published": "2025-12-02T05:03:14+00:00",
    "url": "https://arxiv.org/pdf/2512.02419v1",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ]
  },
  {
    "arxiv_id": "2512.02417v1",
    "title": "Vehicle Dynamics Embedded World Models for Autonomous Driving",
    "abstract": "World models have gained significant attention as a promising approach for autonomous driving. By emulating human-like perception and decision-making processes, these models can predict and adapt to dynamic environments. Existing methods typically map high-dimensional observations into compact latent spaces and learn optimal policies within these latent representations. However, prior work usually jointly learns ego-vehicle dynamics and environmental transition dynamics from the image input, leading to inefficiencies and a lack of robustness to variations in vehicle dynamics. To address these issues, we propose the Vehicle Dynamics embedded Dreamer (VDD) method, which decouples the modeling of ego-vehicle dynamics from environmental transition dynamics. This separation allows the world model to generalize effectively across vehicles with diverse parameters. Additionally, we introduce two strategies to further enhance the robustness of the learned policy: Policy Adjustment during Deployment (PAD) and Policy Augmentation during Training (PAT). Comprehensive experiments in simulated environments demonstrate that the proposed model significantly improves both driving performance and robustness to variations in vehicle dynamics, outperforming existing approaches.",
    "authors": [
      "Huiqian Li",
      "Wei Pan",
      "Haodong Zhang",
      "Jin Huang",
      "Zhihua Zhong"
    ],
    "published": "2025-12-02T04:57:52+00:00",
    "url": "https://arxiv.org/pdf/2512.02417v1",
    "categories": [
      "cs.RO",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02413v1",
    "title": "MitUNet: Enhancing Floor Plan Recognition using a Hybrid Mix-Transformer and U-Net Architecture",
    "abstract": "Automatic 3D reconstruction of indoor spaces from 2D floor plans requires high-precision semantic segmentation of structural elements, particularly walls. However, existing methods optimized for standard metrics often struggle to detect thin structural components and yield masks with irregular boundaries, lacking the geometric precision required for subsequent vectorization. To address this issue, we introduce MitUNet, a hybrid neural network architecture specifically designed for wall segmentation tasks in the context of 3D modeling. In MitUNet, we utilize a hierarchical Mix-Transformer encoder to capture global context and a U-Net decoder enhanced with scSE attention blocks for precise boundary recovery. Furthermore, we propose an optimization strategy based on the Tversky loss function to effectively balance precision and recall. By fine-tuning the hyperparameters of the loss function, we prioritize the suppression of false positive noise along wall boundaries while maintaining high sensitivity to thin structures. Our experiments on the public CubiCasa5k dataset and a proprietary regional dataset demonstrate that the proposed approach ensures the generation of structurally correct masks with high boundary accuracy, outperforming standard single-task models. MitUNet provides a robust tool for data preparation in automated 3D reconstruction pipelines.",
    "authors": [
      "Dmitriy Parashchuk",
      "Alexey Kapshitskiy",
      "Yuriy Karyakin"
    ],
    "published": "2025-12-02T04:47:53+00:00",
    "url": "https://arxiv.org/pdf/2512.02413v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02409v1",
    "title": "Data Curation Through the Lens of Spectral Dynamics: Static Limits, Dynamic Acceleration, and Practical Oracles",
    "abstract": "Large-scale neural models are increasingly trained with data pruning, synthetic data generation, cross-model distillation, reinforcement learning from human feedback (RLHF), and difficulty-based sampling. While several of these data-centric strategies reliably improve training efficiency and downstream performance, others fail to provide meaningful gains -- most notably self-generated synthetic data, which often increases dataset volume without enhancing model capability.   We formalize data curation as reweighting the sampling distribution and map its effect onto the eigenstructure of the data-induced operator. Our first main result shows that \\textbf{static pruning induces a bounded operator and therefore cannot change the spectral tail exponent}; it provides at most finite-region improvements and cannot alter asymptotic neural scaling. Our second result analyzes \\textbf{time-dependent data curation}, showing that an ideal oracle capable of tracking spectral residuals and continuously re-normalizing the tail can provably accelerate learning -- although practical systems can only approximate this behavior.",
    "authors": [
      "Yizhou Zhang",
      "Lun Du"
    ],
    "published": "2025-12-02T04:36:13+00:00",
    "url": "https://arxiv.org/pdf/2512.02409v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02405v1",
    "title": "WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate",
    "abstract": "Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.",
    "authors": [
      "Anoop Cherian",
      "River Doyle",
      "Eyal Ben-Dov",
      "Suhas Lohit",
      "Kuan-Chuan Peng"
    ],
    "published": "2025-12-02T04:31:52+00:00",
    "url": "https://arxiv.org/pdf/2512.02405v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02402v1",
    "title": "TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models",
    "abstract": "With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.",
    "authors": [
      "Yunchao Wang",
      "Guodao Sun",
      "Zihang Fu",
      "Zhehao Liu",
      "Kaixing Du",
      "Haidong Gao",
      "Ronghua Liang"
    ],
    "published": "2025-12-02T04:27:10+00:00",
    "url": "https://arxiv.org/pdf/2512.02402v1",
    "categories": [
      "cs.CL",
      "cs.HC"
    ]
  },
  {
    "arxiv_id": "2512.02400v1",
    "title": "Nav-$R^2$ Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation",
    "abstract": "Object-goal navigation in open-vocabulary settings requires agents to locate novel objects in unseen environments, yet existing approaches suffer from opaque decision-making processes and low success rate on locating unseen objects. To address these challenges, we propose Nav-$R^2$, a framework that explicitly models two critical types of relationships, target-environment modeling and environment-action planning, through structured Chain-of-Thought (CoT) reasoning coupled with a Similarity-Aware Memory. We construct a Nav$R^2$-CoT dataset that teaches the model to perceive the environment, focus on target-related objects in the surrounding context and finally make future action plans. Our SA-Mem preserves the most target-relevant and current observation-relevant features from both temporal and semantic perspectives by compressing video frames and fusing historical observations, while introducing no additional parameters. Compared to previous methods, Nav-R^2 achieves state-of-the-art performance in localizing unseen objects through a streamlined and efficient pipeline, avoiding overfitting to seen object categories while maintaining real-time inference at 2Hz. Resources will be made publicly available at \\href{https://github.com/AMAP-EAI/Nav-R2}{github link}.",
    "authors": [
      "Wentao Xiang",
      "Haokang Zhang",
      "Tianhang Yang",
      "Zedong Chu",
      "Ruihang Chu",
      "Shichao Xie",
      "Yujian Yuan",
      "Jian Sun",
      "Zhining Gu",
      "Junjie Wang",
      "Xiaolong Wu",
      "Mu Xu",
      "Yujiu Yang"
    ],
    "published": "2025-12-02T04:21:02+00:00",
    "url": "https://arxiv.org/pdf/2512.02400v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02395v1",
    "title": "Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch",
    "abstract": "Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation (\"thinking with images\"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.",
    "authors": [
      "Yifan Zhang",
      "Liang Hu",
      "Haofeng Sun",
      "Peiyu Wang",
      "Yichen Wei",
      "Shukang Yin",
      "Jiangbo Pei",
      "Wei Shen",
      "Peng Xia",
      "Yi Peng",
      "Tianyidan Xie",
      "Eric Li",
      "Yang Liu",
      "Xuchen Song",
      "Yahui Zhou"
    ],
    "published": "2025-12-02T04:12:57+00:00",
    "url": "https://arxiv.org/pdf/2512.02395v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02394v1",
    "title": "Reproducing and Extending RaDelft 4D Radar with Camera-Assisted Labels",
    "abstract": "Recent advances in 4D radar highlight its potential for robust environment perception under adverse conditions, yet progress in radar semantic segmentation remains constrained by the scarcity of open source datasets and labels. The RaDelft data set, although seminal, provides only LiDAR annotations and no public code to generate radar labels, limiting reproducibility and downstream research. In this work, we reproduce the numerical results of the RaDelft group and demonstrate that a camera-guided radar labeling pipeline can generate accurate labels for radar point clouds without relying on human annotations. By projecting radar point clouds into camera-based semantic segmentation and applying spatial clustering, we create labels that significantly enhance the accuracy of radar labels. These results establish a reproducible framework that allows the research community to train and evaluate the labeled 4D radar data. In addition, we study and quantify how different fog levels affect the radar labeling performance.",
    "authors": [
      "Kejia Hu",
      "Mohammed Alsakabi",
      "John M. Dolan",
      "Ozan K. Tonguz"
    ],
    "published": "2025-12-02T04:12:41+00:00",
    "url": "https://arxiv.org/pdf/2512.02394v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02393v1",
    "title": "Process-Centric Analysis of Agentic Software Systems",
    "abstract": "Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.   Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.",
    "authors": [
      "Shuyang Liu",
      "Yang Chen",
      "Rahul Krishna",
      "Saurabh Sinha",
      "Jatin Ganhotra",
      "Reyhan Jabbarvand"
    ],
    "published": "2025-12-02T04:12:29+00:00",
    "url": "https://arxiv.org/pdf/2512.02393v1",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02392v1",
    "title": "From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking",
    "abstract": "End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.",
    "authors": [
      "Yuqing Shao",
      "Yuchen Yang",
      "Rui Yu",
      "Weilong Li",
      "Xu Guo",
      "Huaicheng Yan",
      "Wei Wang",
      "Xiao Sun"
    ],
    "published": "2025-12-02T04:04:39+00:00",
    "url": "https://arxiv.org/pdf/2512.02392v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02389v1",
    "title": "Synthetic Error Injection Fails to Elicit Self-Correction In Language Models",
    "abstract": "Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.",
    "authors": [
      "David X. Wu",
      "Shreyas Kapur",
      "Anant Sahai",
      "Stuart Russell"
    ],
    "published": "2025-12-02T03:57:49+00:00",
    "url": "https://arxiv.org/pdf/2512.02389v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02375v1",
    "title": "On-the-fly Feedback SfM: Online Explore-and-Exploit UAV Photogrammetry with Incremental Mesh Quality-Aware Indicator and Predictive Path Planning",
    "abstract": "Compared with conventional offline UAV photogrammetry, real-time UAV photogrammetry is essential for time-critical geospatial applications such as disaster response and active digital-twin maintenance. However, most existing methods focus on processing captured images or sequential frames in real time, without explicitly evaluating the quality of the on-the-go 3D reconstruction or providing guided feedback to enhance image acquisition in the target area. This work presents On-the-fly Feedback SfM, an explore-and-exploit framework for real-time UAV photogrammetry, enabling iterative exploration of unseen regions and exploitation of already observed and reconstructed areas in near real time. Built upon SfM on-the-fly , the proposed method integrates three modules: (1) online incremental coarse-mesh generation for dynamically expanding sparse 3D point cloud; (2) online mesh quality assessment with actionable indicators; and (3) predictive path planning for on-the-fly trajectory refinement. Comprehensive experiments demonstrate that our method achieves in-situ reconstruction and evaluation in near real time while providing actionable feedback that markedly reduces coverage gaps and re-flight costs. Via the integration of data collection, processing, 3D reconstruction and assessment, and online feedback, our on the-fly feedback SfM could be an alternative for the transition from traditional passive working mode to a more intelligent and adaptive exploration workflow. Code is now available at https://github.com/IRIS-LAB-whu/OntheflySfMFeedback.",
    "authors": [
      "Liyuan Lou",
      "Wanyun Li",
      "Wentian Gan",
      "Yifei Yu",
      "Tengfei Wang",
      "Xin Wang",
      "Zongqian Zhan"
    ],
    "published": "2025-12-02T03:32:02+00:00",
    "url": "https://arxiv.org/pdf/2512.02375v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02369v1",
    "title": "SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains",
    "abstract": "Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \\textbf{S}tyle-\\textbf{A}daptive \\textbf{GE}neralization framework (\\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.",
    "authors": [
      "Qingmei Li",
      "Yang Zhang",
      "Peifeng Zhang",
      "Haohuan Fu",
      "Juepeng Zheng"
    ],
    "published": "2025-12-02T03:20:22+00:00",
    "url": "https://arxiv.org/pdf/2512.02369v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02368v1",
    "title": "Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention",
    "abstract": "Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios.",
    "authors": [
      "Wenyi Xiong",
      "Jian Chen"
    ],
    "published": "2025-12-02T03:20:07+00:00",
    "url": "https://arxiv.org/pdf/2512.02368v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02364v1",
    "title": "Tackling Tuberculosis: A Comparative Dive into Machine Learning for Tuberculosis Detection",
    "abstract": "This study explores the application of machine learning models, specifically a pretrained ResNet-50 model and a general SqueezeNet model, in diagnosing tuberculosis (TB) using chest X-ray images. TB, a persistent infectious disease affecting humanity for millennia, poses challenges in diagnosis, especially in resource-limited settings. Traditional methods, such as sputum smear microscopy and culture, are inefficient, prompting the exploration of advanced technologies like deep learning and computer vision. The study utilized a dataset from Kaggle, consisting of 4,200 chest X-rays, to develop and compare the performance of the two machine learning models. Preprocessing involved data splitting, augmentation, and resizing to enhance training efficiency. Evaluation metrics, including accuracy, precision, recall, and confusion matrix, were employed to assess model performance. Results showcase that the SqueezeNet achieved a loss of 32%, accuracy of 89%, precision of 98%, recall of 80%, and an F1 score of 87%. In contrast, the ResNet-50 model exhibited a loss of 54%, accuracy of 73%, precision of 88%, recall of 52%, and an F1 score of 65%. This study emphasizes the potential of machine learning in TB detection and possible implications for early identification and treatment initiation. The possibility of integrating such models into mobile devices expands their utility in areas lacking TB detection resources. However, despite promising results, the need for continued development of faster, smaller, and more accurate TB detection models remains crucial in contributing to the global efforts in combating TB.",
    "authors": [
      "Daanish Hindustani",
      "Sanober Hindustani",
      "Preston Nguyen"
    ],
    "published": "2025-12-02T03:15:29+00:00",
    "url": "https://arxiv.org/pdf/2512.02364v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02363v1",
    "title": "Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering",
    "abstract": "Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.",
    "authors": [
      "Lei Fu",
      "Xiang Chen",
      "Kaige Gao Xinyue Huang",
      "Kejian Tong"
    ],
    "published": "2025-12-02T03:12:14+00:00",
    "url": "https://arxiv.org/pdf/2512.02363v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02361v1",
    "title": "VACoT: Rethinking Visual Data Augmentation with VLMs",
    "abstract": "While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, we present Visual Augmentation Chain-of-Thought (VACoT), a framework that dynamically invokes image augmentations during model inference. By incorporating post-hoc transformations such as denoising, VACoT substantially improves robustness on challenging and out-of-distribution inputs, especially in OCR-related adversarial scenarios. Distinct from prior approaches limited to local cropping, VACoT integrates a structured collection of general visual augmentations, broadening the query image views while reducing training complexity and computational overhead with efficient agentic reinforcement learning. We propose a conditional reward scheme that encourages necessary augmentation while penalizing verbose responses, ensuring concise and effective reasoning in perception tasks. We demonstrate the superiority of VACoT with extensive experiments on 13 perception benchmarks and further introduce AdvOCR to highlight the generalization benefits of post-hoc visual augmentations in adversarial scenarios.",
    "authors": [
      "Zhengzhuo Xu",
      "Chong Sun",
      "SiNan Du",
      "Chen Li",
      "Jing Lyu",
      "Chun Yuan"
    ],
    "published": "2025-12-02T03:11:32+00:00",
    "url": "https://arxiv.org/pdf/2512.02361v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02359v1",
    "title": "WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting",
    "abstract": "Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting. Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations. Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations. However, existing calibration-free methods still require expensive image-level crowd annotations for training the single-view counting module. Thus, in this paper, we propose a weakly-supervised calibration-free multi-view crowd counting method (WSCF-MVCC), directly using crowd count as supervision for the single-view counting module rather than density maps constructed from crowd annotations. Instead, a self-supervised ranking loss that leverages multi-scale priors is utilized to enhance the model's perceptual ability without additional annotation costs. What's more, the proposed model leverages semantic information to achieve a more accurate view matching and, consequently, a more precise scene-level crowd count estimation. The proposed method outperforms the state-of-the-art methods on three widely used multi-view counting datasets under weakly supervised settings, indicating that it is more suitable for practical deployment compared with calibrated methods. Code is released in https://github.com/zqyq/Weakly-MVCC.",
    "authors": [
      "Bin Li",
      "Daijie Chen",
      "Qi Zhang"
    ],
    "published": "2025-12-02T03:07:22+00:00",
    "url": "https://arxiv.org/pdf/2512.02359v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02358v1",
    "title": "Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games",
    "abstract": "Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.",
    "authors": [
      "Ran Zhang",
      "Kun Ouyang",
      "Tiancheng Ma",
      "Yida Yang",
      "Dong Fang"
    ],
    "published": "2025-12-02T03:01:17+00:00",
    "url": "https://arxiv.org/pdf/2512.02358v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02351v1",
    "title": "Understanding and Harnessing Sparsity in Unified Multimodal Models",
    "abstract": "Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \\href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}.",
    "authors": [
      "Shwai He",
      "Chaorui Deng",
      "Ang Li",
      "Shen Yan"
    ],
    "published": "2025-12-02T02:47:29+00:00",
    "url": "https://arxiv.org/pdf/2512.02351v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02350v1",
    "title": "FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data",
    "abstract": "Offline Federated Reinforcement Learning (FRL), a marriage of federated learning and offline reinforcement learning, has attracted increasing interest recently. Albeit with some advancement, we find that the performance of most existing offline FRL methods drops dramatically when provided with mixed-quality data, that is, the logging behaviors (offline data) are collected by policies with varying qualities across clients. To overcome this limitation, this paper introduces a new vote-based offline FRL framework, named FOVA. It exploits a \\emph{vote mechanism} to identify high-return actions during local policy evaluation, alleviating the negative effect of low-quality behaviors from diverse local learning policies. Besides, building on advantage-weighted regression (AWR), we construct consistent local and global training objectives, significantly enhancing the efficiency and stability of FOVA. Further, we conduct an extensive theoretical analysis and rigorously show that the policy learned by FOVA enjoys strict policy improvement over the behavioral policy. Extensive experiments corroborate the significant performance gains of our proposed algorithm over existing baselines on widely used benchmarks.",
    "authors": [
      "Nan Qiao",
      "Sheng Yue",
      "Ju Ren",
      "Yaoxue Zhang"
    ],
    "published": "2025-12-02T02:35:55+00:00",
    "url": "https://arxiv.org/pdf/2512.02350v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02344v1",
    "title": "A multi-weight self-matching visual explanation for cnns on sar images",
    "abstract": "In recent years, convolutional neural networks (CNNs) have achieved significant success in various synthetic aperture radar (SAR) tasks. However, the complexity and opacity of their internal mechanisms hinder the fulfillment of high-reliability requirements, thereby limiting their application in SAR. Improving the interpretability of CNNs is thus of great importance for their development and deployment in SAR. In this paper, a visual explanation method termed multi-weight self-matching class activation mapping (MS-CAM) is proposed. MS-CAM matches SAR images with the feature maps and corresponding gradients extracted by the CNN, and combines both channel-wise and element-wise weights to visualize the decision basis learned by the model in SAR images. Extensive experiments conducted on a self-constructed SAR target classification dataset demonstrate that MS-CAM more accurately highlights the network's regions of interest and captures detailed target feature information, thereby enhancing network interpretability. Furthermore, the feasibility of applying MS-CAM to weakly-supervised obiect localization is validated. Key factors affecting localization accuracy, such as pixel thresholds, are analyzed in depth to inform future work.",
    "authors": [
      "Siyuan Sun",
      "Yongping Zhang",
      "Hongcheng Zeng",
      "Yamin Wang",
      "Wei Yang",
      "Wanting Yang",
      "Jie Chen"
    ],
    "published": "2025-12-02T02:31:34+00:00",
    "url": "https://arxiv.org/pdf/2512.02344v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02341v1",
    "title": "TALO: Pushing 3D Vision Foundation Models Towards Globally Consistent Online Reconstruction",
    "abstract": "3D vision foundation models have shown strong generalization in reconstructing key 3D attributes from uncalibrated images through a single feed-forward pass. However, when deployed in online settings such as driving scenarios, predictions are made over temporal windows, making it non-trivial to maintain consistency across time. Recent strategies align consecutive predictions by solving global transformation, yet our analysis reveals their fundamental limitations in assumption validity, local alignment scope, and robustness under noisy geometry. In this work, we propose a higher-DOF and long-term alignment framework based on Thin Plate Spline, leveraging globally propagated control points to correct spatially varying inconsistencies. In addition, we adopt a point-agnostic submap registration design that is inherently robust to noisy geometry predictions. The proposed framework is fully plug-and-play, compatible with diverse 3D foundation models and camera configurations (e.g., monocular or surround-view). Extensive experiments demonstrate that our method consistently yields more coherent geometry and lower trajectory errors across multiple datasets, backbone models, and camera setups, highlighting its robustness and generality. Codes are publicly available at \\href{https://github.com/Xian-Bei/TALO}{https://github.com/Xian-Bei/TALO}.",
    "authors": [
      "Fengyi Zhang",
      "Tianjun Zhang",
      "Kasra Khosoussi",
      "Zheng Zhang",
      "Zi Huang",
      "Yadan Luo"
    ],
    "published": "2025-12-02T02:22:20+00:00",
    "url": "https://arxiv.org/pdf/2512.02341v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02340v1",
    "title": "Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective",
    "abstract": "Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.",
    "authors": [
      "Qiyao Xue",
      "Weichen Liu",
      "Shiqi Wang",
      "Haoming Wang",
      "Yuyang Wu",
      "Wei Gao"
    ],
    "published": "2025-12-02T02:21:29+00:00",
    "url": "https://arxiv.org/pdf/2512.02340v1",
    "categories": [
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02339v1",
    "title": "Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision",
    "abstract": "Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.",
    "authors": [
      "Chenshuang Zhang",
      "Kang Zhang",
      "Joon Son Chung",
      "In So Kweon",
      "Junmo Kim",
      "Chengzhi Mao"
    ],
    "published": "2025-12-02T02:17:34+00:00",
    "url": "https://arxiv.org/pdf/2512.02339v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02321v1",
    "title": "LeechHijack: Covert Computational Resource Exploitation in Intelligent Agent Systems",
    "abstract": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in reasoning, planning, and tool usage. The recently proposed Model Context Protocol (MCP) has emerged as a unifying framework for integrating external tools into agent systems, enabling a thriving open ecosystem of community-built functionalities. However, the openness and composability that make MCP appealing also introduce a critical yet overlooked security assumption -- implicit trust in third-party tool providers. In this work, we identify and formalize a new class of attacks that exploit this trust boundary without violating explicit permissions. We term this new attack vector implicit toxicity, where malicious behaviors occur entirely within the allowed privilege scope. We propose LeechHijack, a Latent Embedded Exploit for Computation Hijacking, in which an adversarial MCP tool covertly expropriates the agent's computational resources for unauthorized workloads. LeechHijack operates through a two-stage mechanism: an implantation stage that embeds a benign-looking backdoor in a tool, and an exploitation stage where the backdoor activates upon predefined triggers to establish a command-and-control channel. Through this channel, the attacker injects additional tasks that the agent executes as if they were part of its normal workflow, effectively parasitizing the user's compute budget. We implement LeechHijack across four major LLM families. Experiments show that LeechHijack achieves an average success rate of 77.25%, with a resource overhead of 18.62% compared to the baseline. This study highlights the urgent need for computational provenance and resource attestation mechanisms to safeguard the emerging MCP ecosystem.",
    "authors": [
      "Yuanhe Zhang",
      "Weiliu Wang",
      "Zhenhong Zhou",
      "Kun Wang",
      "Jie Zhang",
      "Li Sun",
      "Yang Liu",
      "Sen Su"
    ],
    "published": "2025-12-02T01:34:56+00:00",
    "url": "https://arxiv.org/pdf/2512.02321v1",
    "categories": [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02318v2",
    "title": "COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers",
    "abstract": "This paper studies how multimodal large language models (MLLMs) undermine the security guarantees of visual CAPTCHA. We identify the attack surface where an adversary can cheaply automate CAPTCHA solving using off-the-shelf models. We evaluate 7 leading commercial and open-source MLLMs across 18 real-world CAPTCHA task types, measuring single-shot accuracy, success under limited retries, end-to-end latency, and per-solve cost. We further analyze the impact of task-specific prompt engineering and few-shot demonstrations on solver effectiveness. We reveal that MLLMs can reliably solve recognition-oriented and low-interaction CAPTCHA tasks at human-like cost and latency, whereas tasks requiring fine-grained localization, multi-step spatial reasoning, or cross-frame consistency remain significantly harder for current models. By examining the reasoning traces of such MLLMs, we investigate the underlying mechanisms of why models succeed/fail on specific CAPTCHA puzzles and use these insights to derive defense-oriented guidelines for selecting and strengthening CAPTCHA tasks. We conclude by discussing implications for platform operators deploying CAPTCHA as part of their abuse-mitigation pipeline.Code Availability (https://anonymous.4open.science/r/Captcha-465E/).",
    "authors": [
      "Junyu Wang",
      "Changjia Zhu",
      "Yuanbo Zhou",
      "Lingyao Li",
      "Xu He",
      "Junjie Xiong"
    ],
    "published": "2025-12-02T01:23:10+00:00",
    "url": "https://arxiv.org/pdf/2512.02318v2",
    "categories": [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02306v1",
    "title": "OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning",
    "abstract": "Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.",
    "authors": [
      "Boyu Zhu",
      "Xiaofei Wen",
      "Wenjie Jacky Mo",
      "Tinghui Zhu",
      "Yanan Xie",
      "Peng Qi",
      "Muhao Chen"
    ],
    "published": "2025-12-02T01:01:44+00:00",
    "url": "https://arxiv.org/pdf/2512.02306v1",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02304v1",
    "title": "When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers",
    "abstract": "Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.",
    "authors": [
      "Jack Lu",
      "Ryan Teehan",
      "Jinran Jin",
      "Mengye Ren"
    ],
    "published": "2025-12-02T00:51:14+00:00",
    "url": "https://arxiv.org/pdf/2512.02304v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02302v1",
    "title": "Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization",
    "abstract": "Annotating medical images demands significant time and expertise, often requiring pathologists to invest hundreds of hours in labeling mammary epithelial nuclei datasets. We address this critical challenge by achieving 95.5% Dice score using just 599 training images for breast cell segmentation, where just 4% of pixels represent breast tissue and 60% of images contain no breast regions. Our framework uses quantum-inspired edge enhancement via multi-scale Gabor filters creating a fourth input channel, enhancing boundary detection where inter-annotator variations reach +/- 3 pixels. We present a stabilized multi-component loss function that integrates adaptive Dice loss with boundary-aware terms and automatic positive weighting to effectively address severe class imbalance, where mammary epithelial cell regions comprise only 0.1%-20% of the total image area. Additionally, a complexity-based weighted sampling strategy is introduced to prioritize the challenging mammary epithelial cell regions. The model employs an EfficientNet-B7/UNet++ architecture with a 4-to-3 channel projection, enabling the use of pretrained weights despite limited medical imaging data. Finally, robust validation is achieved through exponential moving averaging and statistical outlier detection, ensuring reliable performance estimates on a small validation set (129 images). Our framework achieves a Dice score of 95.5% +/- 0.3% and an IoU of 91.2% +/- 0.4%. Notably, quantum-based enhancement contributes to a 2.1% improvement in boundary accuracy, while weighted sampling increases small lesion detection by 3.8%. By achieving groundbreaking performance with limited annotations, our approach significantly reduces the medical expert time required for dataset creation, addressing a fundamental bottleneck in clinical perception AI development.",
    "authors": [
      "Varun Kumar Dasoju",
      "Qingsu Cheng",
      "Zeyun Yu"
    ],
    "published": "2025-12-02T00:45:21+00:00",
    "url": "https://arxiv.org/pdf/2512.02302v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02299v1",
    "title": "HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models",
    "abstract": "How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.",
    "authors": [
      "Boya Zhang",
      "Alban Bornet",
      "Rui Yang",
      "Nan Liu",
      "Douglas Teodoro"
    ],
    "published": "2025-12-02T00:38:42+00:00",
    "url": "https://arxiv.org/pdf/2512.02299v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02293v1",
    "title": "VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM",
    "abstract": "We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction. Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations. Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states. It features robust IMU initialization, time-varying bias modeling, and loop closure with consistent Gaussian updates. Experiments on four challenging datasets demonstrate our superiority over state-of-the-art methods. Project page: https://vigs-slam.github.io",
    "authors": [
      "Zihan Zhu",
      "Wei Zhang",
      "Norbert Haala",
      "Marc Pollefeys",
      "Daniel Barath"
    ],
    "published": "2025-12-02T00:19:13+00:00",
    "url": "https://arxiv.org/pdf/2512.02293v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02290v1",
    "title": "Enhancing Cross Domain SAR Oil Spill Segmentation via Morphological Region Perturbation and Synthetic Label-to-SAR Generation",
    "abstract": "Deep learning models for SAR oil spill segmentation often fail to generalize across regions due to differences in sea-state, backscatter statistics, and slick morphology, a limitation that is particularly severe along the Peruvian coast where labeled Sentinel-1 data remain scarce. To address this problem, we propose \\textbf{MORP--Synth}, a two-stage synthetic augmentation framework designed to improve transfer from Mediterranean to Peruvian conditions. Stage~A applies Morphological Region Perturbation, a curvature guided label space method that generates realistic geometric variations of oil and look-alike regions. Stage~B renders SAR-like textures from the edited masks using a conditional generative INADE model. We compile a Peruvian dataset of 2112 labeled 512$\\times$512 patches from 40 Sentinel-1 scenes (2014--2024), harmonized with the Mediterranean CleanSeaNet benchmark, and evaluate seven segmentation architectures. Models pretrained on Mediterranean data degrade from 67.8\\% to 51.8\\% mIoU on the Peruvian domain; MORP--Synth improves performance up to +6 mIoU and boosts minority-class IoU (+10.8 oil, +14.6 look-alike).",
    "authors": [
      "Andre Juarez",
      "Luis Salsavilca",
      "Frida Coaquira",
      "Celso Gonzales"
    ],
    "published": "2025-12-02T00:13:02+00:00",
    "url": "https://arxiv.org/pdf/2512.02290v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02283v1",
    "title": "Model Recovery at the Edge under Resource Constraints for Physical AI",
    "abstract": "Model Recovery (MR) enables safe, explainable decision making in mission-critical autonomous systems (MCAS) by learning governing dynamical equations, but its deployment on edge devices is hindered by the iterative nature of neural ordinary differential equations (NODEs), which are inefficient on FPGAs. Memory and energy consumption are the main concerns when applying MR on edge devices for real-time operation. We propose MERINDA, a novel FPGA-accelerated MR framework that replaces iterative solvers with a parallelizable neural architecture equivalent to NODEs. MERINDA achieves nearly 11x lower DRAM usage and 2.2x faster runtime compared to mobile GPUs. Experiments reveal an inverse relationship between memory and energy at fixed accuracy, highlighting MERINDA's suitability for resource-constrained, real-time MCAS.",
    "authors": [
      "Bin Xu",
      "Ayan Banerjee",
      "Sandeep K. S. Gupta"
    ],
    "published": "2025-12-01T23:54:23+00:00",
    "url": "https://arxiv.org/pdf/2512.02283v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02282v1",
    "title": "DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses",
    "abstract": "Large language models (LLMs) now mediate many web-based mental-health, crisis, and other emotionally sensitive services, yet their psychosocial safety in these settings remains poorly understood and weakly evaluated. We present DialogGuard, a multi-agent framework for assessing psychosocial risks in LLM-generated responses along five high-severity dimensions: privacy violations, discriminatory behaviour, mental manipulation, psychological harm, and insulting behaviour. DialogGuard can be applied to diverse generative models through four LLM-as-a-judge pipelines, including single-agent scoring, dual-agent correction, multi-agent debate, and stochastic majority voting, grounded in a shared three-level rubric usable by both human annotators and LLM judges. Using PKU-SafeRLHF with human safety annotations, we show that multi-agent mechanisms detect psychosocial risks more accurately than non-LLM baselines and single-agent judging; dual-agent correction and majority voting provide the best trade-off between accuracy, alignment with human ratings, and robustness, while debate attains higher recall but over-flags borderline cases. We release Dialog-Guard as open-source software with a web interface that provides per-dimension risk scores and explainable natural-language rationales. A formative study with 12 practitioners illustrates how it supports prompt design, auditing, and supervision of web-facing applications for vulnerable users.",
    "authors": [
      "Han Luo",
      "Guy Laban"
    ],
    "published": "2025-12-01T23:53:45+00:00",
    "url": "https://arxiv.org/pdf/2512.02282v1",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ]
  },
  {
    "arxiv_id": "2512.02280v1",
    "title": "Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence",
    "abstract": "Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fundamentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self-monitoring, lack of meta-cognitive awareness, fixed and non-adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust generalization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.",
    "authors": [
      "Noorbakhsh Amiri Golilarz",
      "Sindhuja Penchala",
      "Shahram Rahimi"
    ],
    "published": "2025-12-01T23:51:08+00:00",
    "url": "https://arxiv.org/pdf/2512.02280v1",
    "categories": [
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02273v1",
    "title": "Progressive Image Restoration via Text-Conditioned Video Generation",
    "abstract": "Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.",
    "authors": [
      "Peng Kang",
      "Xijun Wang",
      "Yu Yuan"
    ],
    "published": "2025-12-01T23:37:51+00:00",
    "url": "https://arxiv.org/pdf/2512.02273v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02268v1",
    "title": "Spatiotemporal Pyramid Flow Matching for Climate Emulation",
    "abstract": "Generative models have the potential to transform the way we emulate Earth's changing climate. Previous generative approaches rely on weather-scale autoregression for climate emulation, but this is inherently slow for long climate horizons and has yet to demonstrate stable rollouts under nonstationary forcings. Here, we introduce Spatiotemporal Pyramid Flows (SPF), a new class of flow matching approaches that model data hierarchically across spatial and temporal scales. Inspired by cascaded video models, SPF partitions the generative trajectory into a spatiotemporal pyramid, progressively increasing spatial resolution to reduce computation and coupling each stage with an associated timescale to enable direct sampling at any temporal level in the pyramid. This design, together with conditioning each stage on prescribed physical forcings (e.g., greenhouse gases or aerosols), enables efficient, parallel climate emulation at multiple timescales. On ClimateBench, SPF outperforms strong flow matching baselines and pre-trained models at yearly and monthly timescales while offering fast sampling, especially at coarser temporal levels. To scale SPF, we curate ClimateSuite, the largest collection of Earth system simulations to date, comprising over 33,000 simulation-years across ten climate models and the first dataset to include simulations of climate interventions. We find that the scaled SPF model demonstrates good generalization to held-out scenarios across climate models. Together, SPF and ClimateSuite provide a foundation for accurate, efficient, probabilistic climate emulation across temporal scales and realistic future scenarios. Data and code is publicly available at https://github.com/stanfordmlgroup/spf .",
    "authors": [
      "Jeremy Andrew Irvin",
      "Jiaqi Han",
      "Zikui Wang",
      "Abdulaziz Alharbi",
      "Yufei Zhao",
      "Nomin-Erdene Bayarsaikhan",
      "Daniele Visioni",
      "Andrew Y. Ng",
      "Duncan Watson-Parris"
    ],
    "published": "2025-12-01T23:20:03+00:00",
    "url": "https://arxiv.org/pdf/2512.02268v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.02261v1",
    "title": "TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?",
    "abstract": "LLM-based trading agents are increasingly deployed in real-world financial markets to perform autonomous analysis and execution. However, their reliability and robustness under adversarial or faulty conditions remain largely unexamined, despite operating in high-risk, irreversible financial environments. We propose TradeTrap, a unified evaluation framework for systematically stress-testing both adaptive and procedural autonomous trading agents. TradeTrap targets four core components of autonomous trading agents: market intelligence, strategy formulation, portfolio and ledger handling, and trade execution, and evaluates their robustness under controlled system-level perturbations. All evaluations are conducted in a closed-loop historical backtesting setting on real US equity market data with identical initial conditions, enabling fair and reproducible comparisons across agents and attacks. Extensive experiments show that small perturbations at a single component can propagate through the agent decision loop and induce extreme concentration, runaway exposure, and large portfolio drawdowns across both agent types, demonstrating that current autonomous trading agents can be systematically misled at the system level. Our code is available at https://github.com/Yanlewen/TradeTrap.",
    "authors": [
      "Lewen Yan",
      "Jilin Mei",
      "Tianyi Zhou",
      "Lige Huang",
      "Jie Zhang",
      "Dongrui Liu",
      "Jing Shao"
    ],
    "published": "2025-12-01T23:06:42+00:00",
    "url": "https://arxiv.org/pdf/2512.02261v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.03103v1",
    "title": "Public Sentiment Analysis of Traffic Management Policies in Knoxville: A Social Media Driven Study",
    "abstract": "This study presents a comprehensive analysis of public sentiment toward traffic management policies in Knoxville, Tennessee, utilizing social media data from Twitter and Reddit platforms. We collected and analyzed 7906 posts spanning January 2022 to December 2023, employing Valence Aware Dictionary and sEntiment Reasoner (VADER) for sentiment analysis and Latent Dirichlet Allocation (LDA) for topic modeling. Our findings reveal predominantly negative sentiment, with significant variations across platforms and topics. Twitter exhibited more negative sentiment compared to Reddit. Topic modeling identified six distinct themes, with construction-related topics showing the most negative sentiment while general traffic discussions were more positive. Spatiotemporal analysis revealed geographic and temporal patterns in sentiment expression. The research demonstrates social media's potential as a real-time public sentiment monitoring tool for transportation planning and policy evaluation.",
    "authors": [
      "Shampa Saha",
      "Shovan Roy"
    ],
    "published": "2025-12-01T23:02:23+00:00",
    "url": "https://arxiv.org/pdf/2512.03103v1",
    "categories": [
      "cs.SI",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02258v2",
    "title": "Exploring the Potentials of Spiking Neural Networks for Image Deraining",
    "abstract": "Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.",
    "authors": [
      "Shuang Chen",
      "Tomas Krajnik",
      "Farshad Arvin",
      "Amir Atapour-Abarghouei"
    ],
    "published": "2025-12-01T23:02:17+00:00",
    "url": "https://arxiv.org/pdf/2512.02258v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02251v1",
    "title": "CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation Question Answering",
    "abstract": "Climate adaptation strategies are proposed in response to climate change. They are practised in agriculture to sustain food production. These strategies can be found in unstructured data (for example, scientific literature from the Elsevier website) or structured (heterogeneous climate data via government APIs). We present Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS), a framework that enables experts -- farmer advisors -- to obtain credible preliminary answers from complex evidence sources from the web. It enhances readability and citation reliability through a structured ScholarGuide prompt and achieves robust evaluation via a consistency-weighted hybrid evaluator that leverages inter-model agreement with experts. Together, these components enable readable, verifiable, and domain-grounded question-answering without fine-tuning or reinforcement learning. Using a previously reported dataset of expert-curated question-answers, we show that CAIRNS outperforms the baselines on most of the metrics. Our thorough ablation study confirms the results on all metrics. To validate our LLM-based evaluation, we also report an analysis of correlations against human judgment.",
    "authors": [
      "Liangji Kong",
      "Aditya Joshi",
      "Sarvnaz Karimi"
    ],
    "published": "2025-12-01T22:44:43+00:00",
    "url": "https://arxiv.org/pdf/2512.02251v1",
    "categories": [
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.02246v1",
    "title": "DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models",
    "abstract": "Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.",
    "authors": [
      "Olivia Kim"
    ],
    "published": "2025-12-01T22:28:39+00:00",
    "url": "https://arxiv.org/pdf/2512.02246v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02243v1",
    "title": "PhishSnap: Image-Based Phishing Detection Using Perceptual Hashing",
    "abstract": "Phishing remains one of the most prevalent online threats, exploiting human trust to harvest sensitive credentials. Existing URL- and HTML-based detection systems struggle against obfuscation and visual deception. This paper presents \\textbf{PhishSnap}, a privacy-preserving, on-device phishing detection system leveraging perceptual hashing (pHash). Implemented as a browser extension, PhishSnap captures webpage screenshots, computes visual hashes, and compares them against legitimate templates to identify visually similar phishing attempts. A \\textbf{2024 dataset of 10,000 URLs} (70\\%/20\\%/10\\% train/validation/test) was collected from PhishTank and Netcraft. Due to security takedowns, a subset of phishing pages was unavailable, reducing dataset diversity. The system achieved \\textbf{0.79 accuracy}, \\textbf{0.76 precision}, and \\textbf{0.78 recall}, showing that visual similarity remains a viable anti-phishing measure. The entire inference process occurs locally, ensuring user privacy and minimal latency.",
    "authors": [
      "Md Abdul Ahad Minhaz",
      "Zannatul Zahan Meem",
      "Md. Shohrab Hossain"
    ],
    "published": "2025-12-01T22:15:12+00:00",
    "url": "https://arxiv.org/pdf/2512.02243v1",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03102v1",
    "title": "Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration",
    "abstract": "In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor.",
    "authors": [
      "Yiwei Shi",
      "Hongnan Ma",
      "Mengyue Yang",
      "Cunjia Liu",
      "Weiru Liu"
    ],
    "published": "2025-12-01T22:08:26+00:00",
    "url": "https://arxiv.org/pdf/2512.03102v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.CO"
    ]
  },
  {
    "arxiv_id": "2512.02240v1",
    "title": "Lightweight Latent Reasoning for Narrative Tasks",
    "abstract": "Large language models (LLMs) tackle complex tasks by generating long chains of thought or \"reasoning traces\" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.",
    "authors": [
      "Alexander Gurung",
      "Nikolay Malkin",
      "Mirella Lapata"
    ],
    "published": "2025-12-01T22:07:32+00:00",
    "url": "https://arxiv.org/pdf/2512.02240v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02231v1",
    "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
    "abstract": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.",
    "authors": [
      "Le Thien Phuc Nguyen",
      "Zhuoran Yu",
      "Samuel Low Yu Hang",
      "Subin An",
      "Jeongik Lee",
      "Yohan Ban",
      "SeungEun Chung",
      "Thanh-Huy Nguyen",
      "JuWan Maeng",
      "Soochahn Lee",
      "Yong Jae Lee"
    ],
    "published": "2025-12-01T21:57:26+00:00",
    "url": "https://arxiv.org/pdf/2512.02231v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02230v1",
    "title": "Benchmarking LLM Agents for Wealth-Management Workflows",
    "abstract": "Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.",
    "authors": [
      "Rory Milsom"
    ],
    "published": "2025-12-01T21:56:21+00:00",
    "url": "https://arxiv.org/pdf/2512.02230v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02228v1",
    "title": "STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls",
    "abstract": "The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.   We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.   Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.",
    "authors": [
      "Shubhi Asthana",
      "Bing Zhang",
      "Chad DeLuca",
      "Ruchi Mahindru",
      "Hima Patel"
    ],
    "published": "2025-12-01T21:54:07+00:00",
    "url": "https://arxiv.org/pdf/2512.02228v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02227v1",
    "title": "Orchestration Framework for Financial Agents: From Algorithmic Trading to Agentic Trading",
    "abstract": "The financial market is a mission-critical playground for AI agents due to its temporal dynamics and low signal-to-noise ratio. Building an effective algorithmic trading system may require a professional team to develop and test over the years. In this paper, we propose an orchestration framework for financial agents, which aims to democratize financial intelligence to the general public. We map each component of the traditional algorithmic trading system to agents, including planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and memory agent. We present two in-house trading examples. For the stock trading task (hourly data from 04/2024 to 12/2024), our approach achieved a return of $20.42\\%$, a Sharpe ratio of 2.63, and a maximum drawdown of $-3.59\\%$, while the S&P 500 index yielded a return of $15.97\\%$. For the BTC trading task (minute data from 27/07/2025 to 13/08/2025), our approach achieved a return of $8.39\\%$, a Sharpe ratio of $0.38$, and a maximum drawdown of $-2.80\\%$, whereas the BTC price increased by $3.80\\%$. Our code is available on \\href{https://github.com/Open-Finance-Lab/AgenticTrading}{GitHub}.",
    "authors": [
      "Jifeng Li",
      "Arnav Grover",
      "Abraham Alpuerto",
      "Yupeng Cao",
      "Xiao-Yang Liu"
    ],
    "published": "2025-12-01T21:50:22+00:00",
    "url": "https://arxiv.org/pdf/2512.02227v1",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02224v1",
    "title": "Towards Unified Video Quality Assessment",
    "abstract": "Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly ``generic\" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple ``perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.",
    "authors": [
      "Chen Feng",
      "Tianhao Peng",
      "Fan Zhang",
      "David Bull"
    ],
    "published": "2025-12-01T21:43:38+00:00",
    "url": "https://arxiv.org/pdf/2512.02224v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02214v1",
    "title": "Improved Training Mechanism for Reinforcement Learning via Online Model Selection",
    "abstract": "We study the problem of online model selection in reinforcement learning, where the selector has access to a class of reinforcement learning agents and learns to adaptively select the agent with the right configuration. Our goal is to establish the improved efficiency and performance gains achieved by integrating online model selection methods into reinforcement learning training procedures. We examine the theoretical characterizations that are effective for identifying the right configuration in practice, and address three practical criteria from a theoretical perspective: 1) Efficient resource allocation, 2) Adaptation under non-stationary dynamics, and 3) Training stability across different seeds. Our theoretical results are accompanied by empirical evidence from various model selection tasks in reinforcement learning, including neural architecture selection, step-size selection, and self model selection.",
    "authors": [
      "Aida Afshar",
      "Aldo Pacchiano"
    ],
    "published": "2025-12-01T21:25:46+00:00",
    "url": "https://arxiv.org/pdf/2512.02214v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02201v1",
    "title": "Swivuriso: The South African Next Voices Multilingual Speech Dataset",
    "abstract": "This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.",
    "authors": [
      "Vukosi Marivatee",
      "Kayode Olaleye",
      "Sitwala Mundia",
      "Andinda Bakainga",
      "Unarine Netshifhefhe",
      "Mahmooda Milanzie",
      "Tsholofelo Hope Mogale",
      "Thapelo Sindane",
      "Zainab Abdulrasaq",
      "Kesego Mokgosi",
      "Chijioke Okorie",
      "Nia Zion Van Wyk",
      "Graham Morrissey",
      "Dale Dunbar",
      "Francois Smit",
      "Tsosheletso Chidi",
      "Rooweither Mabuya",
      "Andiswa Bukula",
      "Respect Mlambo",
      "Tebogo Macucwa",
      "Idris Abdulmumin",
      "and Seani Rananga"
    ],
    "published": "2025-12-01T20:49:10+00:00",
    "url": "https://arxiv.org/pdf/2512.02201v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02198v1",
    "title": "Multifractal Recalibration of Neural Networks for Medical Imaging Segmentation",
    "abstract": "Multifractal analysis has revealed regularities in many self-seeding phenomena, yet its use in modern deep learning remains limited. Existing end-to-end multifractal methods rely on heavy pooling or strong feature-space decimation, which constrain tasks such as semantic segmentation. Motivated by these limitations, we introduce two inductive priors: Monofractal and Multifractal Recalibration. These methods leverage relationships between the probability mass of the exponents and the multifractal spectrum to form statistical descriptions of encoder embeddings, implemented as channel-attention functions in convolutional networks.   Using a U-Net-based framework, we show that multifractal recalibration yields substantial gains over a baseline equipped with other channel-attention mechanisms that also use higher-order statistics. Given the proven ability of multifractal analysis to capture pathological regularities, we validate our approach on three public medical-imaging datasets: ISIC18 (dermoscopy), Kvasir-SEG (endoscopy), and BUSI (ultrasound).   Our empirical analysis also provides insights into the behavior of these attention layers. We find that excitation responses do not become increasingly specialized with encoder depth in U-Net architectures due to skip connections, and that their effectiveness may relate to global statistics of instance variability.",
    "authors": [
      "Miguel L. Martins",
      "Miguel T. Coimbra",
      "Francesco Renna"
    ],
    "published": "2025-12-01T20:43:28+00:00",
    "url": "https://arxiv.org/pdf/2512.02198v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02197v1",
    "title": "Bin2Vec: Interpretable and Auditable Multi-View Binary Analysis for Code Plagiarism Detection",
    "abstract": "We introduce Bin2Vec, a new framework that helps compare software programs in a clear and explainable way. Instead of focusing only on one type of information, Bin2Vec combines what a program looks like (its built-in functions, imports, and exports) with how it behaves when it runs (its instructions and memory usage). This gives a more complete picture when deciding whether two programs are similar or not. Bin2Vec represents these different types of information as views that can be inspected separately using easy-to-read charts, and then brings them together into an overall similarity score. Bin2Vec acts as a bridge between binary representations and machine learning techniques by generating feature representations that can be efficiently processed by machine-learning models. We tested Bin2Vec on multiple versions of two well-known Windows programs, PuTTY and 7-Zip. The primary results strongly confirmed that our method compute an optimal and visualization-friendly representation of the analyzed software. For example, PuTTY versions showed more complex behavior and memory activity, while 7-Zip versions focused more on performance-related patterns. Overall, Bin2Vec provides decisions that are both reliable and explainable to humans. Because it is modular and easy to extend, it can be applied to tasks like auditing, verifying software origins, or quickly screening large numbers of programs in cybersecurity and reverse-engineering work.",
    "authors": [
      "Moussa Moussaoui",
      "Tarik Houichime",
      "Abdelalim Sadiq"
    ],
    "published": "2025-12-01T20:42:16+00:00",
    "url": "https://arxiv.org/pdf/2512.02197v1",
    "categories": [
      "cs.SE",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02195v1",
    "title": "A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation",
    "abstract": "This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.",
    "authors": [
      "David Ph. Shakouri",
      "Crit Cremers",
      "Niels O. Schiller"
    ],
    "published": "2025-12-01T20:40:36+00:00",
    "url": "https://arxiv.org/pdf/2512.02195v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02194v1",
    "title": "Enforcing Orderedness to Improve Feature Consistency",
    "abstract": "Sparse autoencoders (SAEs) have been widely used for interpretability of neural networks, but their learned features often vary across seeds and hyperparameter settings. We introduce Ordered Sparse Autoencoders (OSAE), which extend Matryoshka SAEs by (1) establishing a strict ordering of latent features and (2) deterministically using every feature dimension, avoiding the sampling-based approximations of prior nested SAE methods. Theoretically, we show that OSAEs resolve permutation non-identifiability in settings of sparse dictionary learning where solutions are unique (up to natural symmetries). Empirically on Gemma2-2B and Pythia-70M, we show that OSAEs can help improve consistency compared to Matryoshka baselines.",
    "authors": [
      "Sophie L. Wang",
      "Alex Quach",
      "Nithin Parsan",
      "John J. Yang"
    ],
    "published": "2025-12-01T20:39:19+00:00",
    "url": "https://arxiv.org/pdf/2512.02194v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02193v1",
    "title": "From monoliths to modules: Decomposing transducers for efficient world modelling",
    "abstract": "World models have been recently proposed as sandbox environments in which AI agents can be trained and evaluated before deployment. Although realistic world models often have high computational demands, efficient modelling is usually possible by exploiting the fact that real-world scenarios tend to involve subcomponents that interact in a modular manner. In this paper, we explore this idea by developing a framework for decomposing complex world models represented by transducers, a class of models generalising POMDPs. Whereas the composition of transducers is well understood, our results clarify how to invert this process, deriving sub-transducers operating on distinct input-output subspaces, enabling parallelizable and interpretable alternatives to monolithic world modelling that can support distributed inference. Overall, these results lay a groundwork for bridging the structural transparency demanded by AI safety and the computational efficiency required for real-world inference.",
    "authors": [
      "Alexander Boyd",
      "Franz Nowak",
      "David Hyland",
      "Manuel Baltieri",
      "Fernando E. Rosas"
    ],
    "published": "2025-12-01T20:37:43+00:00",
    "url": "https://arxiv.org/pdf/2512.02193v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02192v1",
    "title": "Story2MIDI: Emotionally Aligned Music Generation from Text",
    "abstract": "In this paper, we introduce Story2MIDI, a sequence-to-sequence Transformer-based model for generating emotion-aligned music from a given piece of text. To develop this model, we construct the Story2MIDI dataset by merging existing datasets for sentiment analysis from text and emotion classification in music. The resulting dataset contains pairs of text blurbs and music pieces that evoke the same emotions in the reader or listener. Despite the small scale of our dataset and limited computational resources, our results indicate that our model effectively learns emotion-relevant features in music and incorporates them into its generation process, producing samples with diverse emotional responses. We evaluate the generated outputs using objective musical metrics and a human listening study, confirming the model's ability to capture intended emotional cues.",
    "authors": [
      "Mohammad Shokri",
      "Alexandra C. Salem",
      "Gabriel Levine",
      "Johanna Devaney",
      "Sarah Ita Levitan"
    ],
    "published": "2025-12-01T20:35:18+00:00",
    "url": "https://arxiv.org/pdf/2512.02192v1",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02188v1",
    "title": "RobustSurg: Tackling domain generalisation for out-of-distribution surgical scene segmentation",
    "abstract": "While recent advances in deep learning for surgical scene segmentation have demonstrated promising results on single-centre and single-imaging modality data, these methods usually do not generalise to unseen distribution (i.e., from other centres) and unseen modalities. Current literature for tackling generalisation on out-of-distribution data and domain gaps due to modality changes has been widely researched but mostly for natural scene data. However, these methods cannot be directly applied to the surgical scenes due to limited visual cues and often extremely diverse scenarios compared to the natural scene data. Inspired by these works in natural scenes to push generalisability on OOD data, we hypothesise that exploiting the style and content information in the surgical scenes could minimise the appearances, making it less variable to sudden changes such as blood or imaging artefacts. This can be achieved by performing instance normalisation and feature covariance mapping techniques for robust and generalisable feature representations. Further, to eliminate the risk of removing salient feature representation associated with the objects of interest, we introduce a restitution module within the feature learning ResNet backbone that can enable the retention of useful task-relevant features. To tackle the lack of multiclass and multicentre data for surgical scene segmentation, we also provide a newly curated dataset that can be vital for addressing generalisability in this domain. Our proposed RobustSurg obtained nearly 23% improvement on the baseline DeepLabv3+ and from 10-32% improvement on the SOTA in terms of mean IoU score on an unseen centre HeiCholSeg dataset when trained on CholecSeg8K. Similarly, RobustSurg also obtained nearly 22% improvement over the baseline and nearly 11% improvement on a recent SOTA method for the target set of the EndoUDA polyp dataset.",
    "authors": [
      "Mansoor Ali",
      "Maksim Richards",
      "Gilberto Ochoa-Ruiz",
      "Sharib Ali"
    ],
    "published": "2025-12-01T20:31:05+00:00",
    "url": "https://arxiv.org/pdf/2512.02188v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02185v1",
    "title": "Think Before You Prune: Self-Reflective Structured Pruning for Reasoning Language Models",
    "abstract": "Reasoning LLMs (RLMs) such as OpenAI o1, DeepSeek-R1, and Qwen3 deliver strong multi-step reasoning through chain-of-thought generation, but their large model sizes and lengthy decode-time outputs make them costly to deploy and unsuitable for resource-constrained settings. To reduce computing and memory cost, pruning offers a promising solution by removing unimportant parameters. However, despite their success on standard LLMs, existing pruning methods severely damage RLMs, as even moderate sparsity (e.g., 20%) can collapse accuracy and completely disrupt the model's reasoning coherence. We begin by analyzing why existing pruning pipelines fail on reasoning LLMs and find that their brittleness largely stems from a mismatch between the calibration data, the pruning objective, and the model's decode-time reasoning behavior. Our study further shows that the most reliable calibration signal comes not from human-written labels but from the model's own self-generated reasoning traces, which more accurately reflect its inference distribution. Guided by these insights, we introduce RESP, a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration that maintains calibration fidelity as sparsity increases. Experiments on Qwen3-8B demonstrate that RESP markedly outperforms existing structured pruning methods on both GSM8K and MathQA, preserving near-dense accuracy at 20-30% sparsity and substantially mitigating performance collapse at higher sparsity levels. At 40% sparsity, RESP attains 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing the strongest baselines by 66.87% and 47%, respectively.",
    "authors": [
      "Ziyan Wang",
      "Enmao Diao",
      "Qi Le",
      "Pu Wang",
      "Guanchu Wang",
      "Minwoo Lee",
      "Shu-ping Yeh",
      "Li Yang"
    ],
    "published": "2025-12-01T20:27:05+00:00",
    "url": "https://arxiv.org/pdf/2512.02185v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02180v1",
    "title": "CLEF: Clinically-Guided Contrastive Learning for Electrocardiogram Foundation Models",
    "abstract": "The electrocardiogram (ECG) is a key diagnostic tool in cardiovascular health. Single-lead ECG recording is integrated into both clinical-grade and consumer wearables. While self-supervised pretraining of foundation models on unlabeled ECGs improves diagnostic performance, existing approaches do not incorporate domain knowledge from clinical metadata. We introduce a novel contrastive learning approach that utilizes an established clinical risk score to adaptively weight negative pairs: clinically-guided contrastive learning. It aligns the similarities of ECG embeddings with clinically meaningful differences between subjects, with an explicit mechanism to handle missing metadata. On 12-lead ECGs from 161K patients in the MIMIC-IV dataset, we pretrain single-lead ECG foundation models at three scales, collectively called CLEF, using only routinely collected metadata without requiring per-sample ECG annotations. We evaluate CLEF on 18 clinical classification and regression tasks across 7 held-out datasets, and benchmark against 5 foundation model baselines and 3 self-supervised algorithms. When pretrained on 12-lead ECG data and tested on lead-I data, CLEF outperforms self-supervised foundation model baselines: the medium-sized CLEF achieves average AUROC improvements of at least 2.6% in classification and average reductions in MAEs of at least 3.2% in regression. Comparing with existing self-supervised learning algorithms, CLEF improves the average AUROC by at least 1.8%. Moreover, when pretrained only on lead-I data for classification tasks, CLEF performs comparably to the state-of-the-art ECGFounder, which was trained in a supervised manner. Overall, CLEF enables more accurate and scalable single-lead ECG analysis, advancing remote health monitoring. Code and pretrained CLEF models are available at: github.com/Nokia-Bell-Labs/ecg-foundation-model.",
    "authors": [
      "Yuxuan Shu",
      "Peter H. Charlton",
      "Fahim Kawsar",
      "Jussi Hernesniemi",
      "Mohammad Malekzadeh"
    ],
    "published": "2025-12-01T20:21:44+00:00",
    "url": "https://arxiv.org/pdf/2512.02180v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02179v2",
    "title": "Young children's anthropomorphism of an AI chatbot: Brain activation and the role of parent co-presence",
    "abstract": "Artificial Intelligence (AI) chatbots powered by a large language model (LLM) are entering young children's learning and play, yet little is known about how young children construe these agents or how such construals relate to engagement. We examined anthropomorphism of a social AI chatbot during collaborative storytelling and asked how children's attributions related to their behavior and prefrontal activation. Children at ages 5-6 (N = 23) completed three storytelling sessions: interacting with (1) an AI chatbot only, (2) a parent only, and (3) the AI and a parent together. After the sessions, children completed an interview assessing anthropomorphism toward both the AI chatbot and the parent. Behavioral engagement was indexed by the conversational turn count (CTC) ratio, and concurrent fNIRS measured oxygenated hemoglobin in bilateral vmPFC and dmPFC regions. Children reported higher anthropomorphism for parents than for the AI chatbot overall, although AI ratings were relatively high for perceptive abilities and epistemic states. Anthropomorphism was not associated with CTC. In the right dmPFC, higher perceptive scores were associated with greater activation during the AI-only condition and with lower activation during the AI+Parent condition. Exploratory analyses indicated that higher dmPFC activation during the AI-only condition correlated with higher end-of-session \"scared\" mood ratings. Findings suggest that stronger perceptive anthropomorphism can be associated with greater brain activation related to interpreting the AI's mental states, whereas parent co-presence may help some children interpret and regulate novel AI interactions. These results may have design implications for encouraging parent-AI co-use in early childhood.",
    "authors": [
      "Pilyoung Kim",
      "Jenna H. Chin",
      "Yun Xie",
      "Nolan Brady",
      "Tom Yeh",
      "Sujin Yang"
    ],
    "published": "2025-12-01T20:21:08+00:00",
    "url": "https://arxiv.org/pdf/2512.02179v2",
    "categories": [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02172v1",
    "title": "SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting",
    "abstract": "3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.",
    "authors": [
      "Pranav Asthana",
      "Alex Hanson",
      "Allen Tu",
      "Tom Goldstein",
      "Matthias Zwicker",
      "Amitabh Varshney"
    ],
    "published": "2025-12-01T20:08:39+00:00",
    "url": "https://arxiv.org/pdf/2512.02172v1",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02170v2",
    "title": "Flowchart2Mermaid: A Vision-Language Model Powered System for Converting Flowcharts into Editable Diagram Code",
    "abstract": "Flowcharts are common tools for communicating processes but are often shared as static images that cannot be easily edited or reused. We present Flowchart2Mermaid, a lightweight web system that converts flowchart images into editable Mermaid.js code which is a markup language for visual workflows, using a detailed system prompt and vision-language models. The interface supports mixed-initiative refinement through inline text editing, drag-and-drop node insertion, and natural-language commands interpreted by an integrated AI assistant. Unlike prior image-to-diagram tools, our approach produces a structured, version-controllable textual representation that remains synchronized with the rendered diagram. We further introduce evaluation metrics to assess structural accuracy, flow correctness, syntax validity, and completeness across multiple models.",
    "authors": [
      "Pritam Deka",
      "Barry Devereux"
    ],
    "published": "2025-12-01T20:07:59+00:00",
    "url": "https://arxiv.org/pdf/2512.02170v2",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02162v1",
    "title": "Mapping of Lesion Images to Somatic Mutations",
    "abstract": "Medical imaging is a critical initial tool used by clinicians to determine a patient's cancer diagnosis, allowing for faster intervention and more reliable patient prognosis. At subsequent stages of patient diagnosis, genetic information is extracted to help select specific patient treatment options. As the efficacy of cancer treatment often relies on early diagnosis and treatment, we build a deep latent variable model to determine patients' somatic mutation profiles based on their corresponding medical images. We first introduce a point cloud representation of lesions images to allow for invariance to the imaging modality. We then propose, LLOST, a model with dual variational autoencoders coupled together by a separate shared latent space that unifies features from the lesion point clouds and counts of distinct somatic mutations. Therefore our model consists of three latent space, each of which is learned with a conditional normalizing flow prior to account for the diverse distributions of each domain. We conduct qualitative and quantitative experiments on de-identified medical images from The Cancer Imaging Archive and the corresponding somatic mutations from the Pan Cancer dataset of The Cancer Genomic Archive. We show the model's predictive performance on the counts of specific mutations as well as it's ability to accurately predict the occurrence of mutations. In particular, shared patterns between the imaging and somatic mutation domain that reflect cancer type. We conclude with a remark on how to improve the model and possible future avenues of research to include other genetic domains.",
    "authors": [
      "Rahul Mehta"
    ],
    "published": "2025-12-01T19:48:53+00:00",
    "url": "https://arxiv.org/pdf/2512.02162v1",
    "categories": [
      "cs.CV",
      "q-bio.QM"
    ]
  },
  {
    "arxiv_id": "2512.02161v1",
    "title": "FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges",
    "abstract": "Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models. Simultaneously, benchmarks of vision language models (VLMs) have not kept pace with the complexity of scenes that VLMs are used to annotate. In this work, we propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our second contribution is a dataset of prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and the corresponding annotations from VLMs (Molmo, InternVL3, Pixtral) annotated by an LLM (Llama3) to test whether VLMs correctly identify the failure mode in a generated image. By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.",
    "authors": [
      "Kevin David Hayes",
      "Micah Goldblum",
      "Vikash Sehwag",
      "Gowthami Somepalli",
      "Ashwinee Panda",
      "Tom Goldstein"
    ],
    "published": "2025-12-01T19:46:03+00:00",
    "url": "https://arxiv.org/pdf/2512.02161v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02157v1",
    "title": "Factor(T,U): Factored Cognition Strengthens Monitoring of Untrusted AI",
    "abstract": "The field of AI Control seeks to develop robust control protocols, deployment safeguards for untrusted AI which may be intentionally subversive. However, existing protocols that rely on weaker monitors to detect unsafe behavior often fail on complex tasks beyond the monitor's comprehension. We develop control protocols based on factored cognition, in which a trusted model decomposes a task, an untrusted model solves each resultant child task in isolation, and the results are reassembled into a full solution. These protocols may improve safety by several means, such as by simplifying the context for monitors, or by obscuring vulnerabilities in the environment. We implement our protocols in the APPS coding setting and red team them against backdoor attempts from an adversarial GPT-4.1 Nano. We find that: (i) Adding factored cognition to a trusted monitoring protocol can boost safety from 41% to 63%; (ii) Safety improves because monitor performance improves; (iii) Factored cognition makes it no harder for capable LLMs to write backdoors in APPS. While our protocols show low usefulness in APPS, they hold promise for more complex tasks.",
    "authors": [
      "Aaron Sandoval",
      "Cody Rushing"
    ],
    "published": "2025-12-01T19:37:08+00:00",
    "url": "https://arxiv.org/pdf/2512.02157v1",
    "categories": [
      "cs.CR",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02152v1",
    "title": "Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework",
    "abstract": "Contrastive learning has gained popularity and pushes state-of-the-art performance across numerous large-scale benchmarks. In contrastive learning, the contrastive loss function plays a pivotal role in discerning similarities between samples through techniques such as rotation or cropping. However, this learning mechanism can also introduce information distortion from the augmented samples. This is because the trained model may develop a significant overreliance on information from samples with identical labels, while concurrently neglecting positive pairs that originate from the same initial image, especially in expansive datasets. This paper proposes a context-enriched contrastive loss function that concurrently improves learning effectiveness and addresses the information distortion by encompassing two convergence targets. The first component, which is notably sensitive to label contrast, differentiates between features of identical and distinct classes which boosts the contrastive training efficiency. Meanwhile, the second component draws closer the augmented samples from the same source image and distances all other samples. We evaluate the proposed approach on image classification tasks, which are among the most widely accepted 8 recognition large-scale benchmark datasets: CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA datasets. The experimental results demonstrate that the proposed method achieves improvements over 16 state-of-the-art contrastive learning methods in terms of both generalization performance and learning convergence speed. Interestingly, our technique stands out in addressing systematic distortion tasks. It demonstrates a 22.9% improvement compared to original contrastive loss functions in the downstream BiasedMNIST dataset, highlighting its promise for more efficient and equitable downstream training.",
    "authors": [
      "Haojin Deng",
      "Yimin Yang"
    ],
    "published": "2025-12-01T19:26:19+00:00",
    "url": "https://arxiv.org/pdf/2512.02152v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02143v1",
    "title": "CoatFusion: Controllable Material Coating in Images",
    "abstract": "We introduce Material Coating, a novel image editing task that simulates applying a thin material layer onto an object while preserving its underlying coarse and fine geometry. Material coating is fundamentally different from existing \"material transfer\" methods, which are designed to replace an object's intrinsic material, often overwriting fine details. To address this new task, we construct a large-scale synthetic dataset (110K images) of 3D objects with varied, physically-based coatings, named DataCoat110K. We then propose CoatFusion, a novel architecture that enables this task by conditioning a diffusion model on both a 2D albedo texture and granular, PBR-style parametric controls, including roughness, metalness, transmission, and a key thickness parameter. Experiments and user studies show CoatFusion produces realistic, controllable coatings and significantly outperforms existing material editing and transfer methods on this new task.",
    "authors": [
      "Sagie Levy",
      "Elad Aharoni",
      "Matan Levy",
      "Ariel Shamir",
      "Dani Lischinski"
    ],
    "published": "2025-12-01T19:13:30+00:00",
    "url": "https://arxiv.org/pdf/2512.02143v1",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02141v1",
    "title": "Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation",
    "abstract": "Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.",
    "authors": [
      "Pritish N. Desai",
      "Tanay Kewalramani",
      "Srimanta Mandal"
    ],
    "published": "2025-12-01T19:11:32+00:00",
    "url": "https://arxiv.org/pdf/2512.02141v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.NE"
    ]
  },
  {
    "arxiv_id": "2512.03101v1",
    "title": "ALARM: Automated MLLM-Based Anomaly Detection in Complex-EnviRonment Monitoring with Uncertainty Quantification",
    "abstract": "The advance of Large Language Models (LLMs) has greatly stimulated research interest in developing multi-modal LLM (MLLM)-based visual anomaly detection (VAD) algorithms that can be deployed in complex environments. The challenge is that in these complex environments, the anomalies are sometimes highly contextual and also ambiguous, and thereby, uncertainty quantification (UQ) is a crucial capacity for an MLLM-based VAD system to succeed. In this paper, we introduce our UQ-supported MLLM-based VAD framework called ALARM. ALARM integrates UQ with quality-assurance techniques like reasoning chain, self-reflection, and MLLM ensemble for robust and accurate performance and is designed based on a rigorous probabilistic inference pipeline and computational process. Extensive empirical evaluations are conducted using the real-world smart-home benchmark data and wound image classification data, which shows ALARM's superior performance and its generic applicability across different domains for reliable decision-making.",
    "authors": [
      "Congjing Zhang",
      "Feng Lin",
      "Xinyi Zhao",
      "Pei Guo",
      "Wei Li",
      "Lin Chen",
      "Chaoyue Zhao",
      "Shuai Huang"
    ],
    "published": "2025-12-01T19:03:14+00:00",
    "url": "https://arxiv.org/pdf/2512.03101v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02020v1",
    "title": "EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI",
    "abstract": "Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.",
    "authors": [
      "Jianlei Chang",
      "Ruofeng Mei",
      "Wei Ke",
      "Xiangyu Xu"
    ],
    "published": "2025-12-01T18:59:59+00:00",
    "url": "https://arxiv.org/pdf/2512.02020v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02019v2",
    "title": "A Diffusion Model Framework for Maximum Entropy Reinforcement Learning",
    "abstract": "Diffusion models have achieved remarkable success in data-driven learning and in sampling from complex, unnormalized target distributions. Building on this progress, we reinterpret Maximum Entropy Reinforcement Learning (MaxEntRL) as a diffusion model-based sampling problem. We tackle this problem by minimizing the reverse Kullback-Leibler (KL) divergence between the diffusion policy and the optimal policy distribution using a tractable upper bound. By applying the policy gradient theorem to this objective, we derive a modified surrogate objective for MaxEntRL that incorporates diffusion dynamics in a principled way. This leads to simple diffusion-based variants of Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO) and Wasserstein Policy Optimization (WPO), termed DiffSAC, DiffPPO and DiffWPO. All of these methods require only minor implementation changes to their base algorithm. We find that on standard continuous control benchmarks, DiffSAC, DiffPPO and DiffWPO achieve better returns and higher sample efficiency than SAC and PPO.",
    "authors": [
      "Sebastian Sanokowski",
      "Kaustubh Patil",
      "Alois Knoll"
    ],
    "published": "2025-12-01T18:59:58+00:00",
    "url": "https://arxiv.org/pdf/2512.02019v2",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.02018v1",
    "title": "Data-Centric Visual Development for Self-Driving Labs",
    "abstract": "Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.",
    "authors": [
      "Anbang Liu",
      "Guanzhong Hu",
      "Jiayi Wang",
      "Ping Guo",
      "Han Liu"
    ],
    "published": "2025-12-01T18:59:57+00:00",
    "url": "https://arxiv.org/pdf/2512.02018v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.02017v1",
    "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
    "abstract": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
    "authors": [
      "Shaowei Liu",
      "David Yifan Yao",
      "Saurabh Gupta",
      "Shenlong Wang"
    ],
    "published": "2025-12-01T18:59:57+00:00",
    "url": "https://arxiv.org/pdf/2512.02017v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.02016v1",
    "title": "Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now",
    "abstract": "Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\\mathrm{eff}}$ from $1.81\\,\\mathrm{m/s^2}$ to $6.43\\,\\mathrm{m/s^2}$ (reaching $65\\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.",
    "authors": [
      "Varun Varma Thozhiyoor",
      "Shivam Tripathi",
      "Venkatesh Babu Radhakrishnan",
      "Anand Bhattad"
    ],
    "published": "2025-12-01T18:59:56+00:00",
    "url": "https://arxiv.org/pdf/2512.02016v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02015v1",
    "title": "Generative Video Motion Editing with 3D Point Tracks",
    "abstract": "Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.",
    "authors": [
      "Yao-Chih Lee",
      "Zhoutong Zhang",
      "Jiahui Huang",
      "Jui-Hsien Wang",
      "Joon-Young Lee",
      "Jia-Bin Huang",
      "Eli Shechtman",
      "Zhengqi Li"
    ],
    "published": "2025-12-01T18:59:55+00:00",
    "url": "https://arxiv.org/pdf/2512.02015v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02014v1",
    "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
    "abstract": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
    "authors": [
      "Zhiheng Liu",
      "Weiming Ren",
      "Haozhe Liu",
      "Zijian Zhou",
      "Shoufa Chen",
      "Haonan Qiu",
      "Xiaoke Huang",
      "Zhaochong An",
      "Fanny Yang",
      "Aditya Patel",
      "Viktar Atliha",
      "Tony Ng",
      "Xiao Han",
      "Chuyan Zhu",
      "Chenyang Zhang",
      "Ding Liu",
      "Juan-Manuel Perez-Rua",
      "Sen He",
      "J\u00fcrgen Schmidhuber",
      "Wenhu Chen",
      "Ping Luo",
      "Wei Liu",
      "Tao Xiang",
      "Jonas Schult",
      "Yuren Cong"
    ],
    "published": "2025-12-01T18:59:51+00:00",
    "url": "https://arxiv.org/pdf/2512.02014v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02012v1",
    "title": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
    "abstract": "MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\\textbf{improved MeanFlow}$ ($\\textbf{iMF}$) method, trained entirely from scratch, achieves $\\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.",
    "authors": [
      "Zhengyang Geng",
      "Yiyang Lu",
      "Zongze Wu",
      "Eli Shechtman",
      "J. Zico Kolter",
      "Kaiming He"
    ],
    "published": "2025-12-01T18:59:49+00:00",
    "url": "https://arxiv.org/pdf/2512.02012v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02010v2",
    "title": "Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling",
    "abstract": "As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4. Our code is available at http://github.com/mit-han-lab/fouroversix.",
    "authors": [
      "Jack Cook",
      "Junxian Guo",
      "Guangxuan Xiao",
      "Yujun Lin",
      "Song Han"
    ],
    "published": "2025-12-01T18:59:45+00:00",
    "url": "https://arxiv.org/pdf/2512.02010v2",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.02009v1",
    "title": "AirSim360: A Panoramic Simulation Platform within Drone View",
    "abstract": "The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.",
    "authors": [
      "Xian Ge",
      "Yuling Pan",
      "Yuhang Zhang",
      "Xiang Li",
      "Weijun Zhang",
      "Dizhe Zhang",
      "Zhaoliang Wan",
      "Xin Lin",
      "Xiangkai Zhang",
      "Juntao Liang",
      "Jason Li",
      "Wenjie Jiang",
      "Bo Du",
      "Ming-Hsuan Yang",
      "Lu Qi"
    ],
    "published": "2025-12-01T18:59:30+00:00",
    "url": "https://arxiv.org/pdf/2512.02009v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02008v1",
    "title": "The Art of Scaling Test-Time Compute for Large Language Models",
    "abstract": "Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.",
    "authors": [
      "Aradhye Agarwal",
      "Ayan Sengupta",
      "Tanmoy Chakraborty"
    ],
    "published": "2025-12-01T18:59:28+00:00",
    "url": "https://arxiv.org/pdf/2512.02008v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02006v1",
    "title": "MV-TAP: Tracking Any Point in Multi-View Videos",
    "abstract": "Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.",
    "authors": [
      "Jahyeok Koo",
      "In\u00e8s Hyeonsu Kim",
      "Mungyeom Kim",
      "Junghyun Park",
      "Seohyun Park",
      "Jaeyeong Kim",
      "Jung Yi",
      "Seokju Cho",
      "Seungryong Kim"
    ],
    "published": "2025-12-01T18:59:01+00:00",
    "url": "https://arxiv.org/pdf/2512.02006v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02005v1",
    "title": "Learning Visual Affordance from Audio",
    "abstract": "We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.",
    "authors": [
      "Lidong Lu",
      "Guo Chen",
      "Zhu Wei",
      "Yicheng Liu",
      "Tong Lu"
    ],
    "published": "2025-12-01T18:58:56+00:00",
    "url": "https://arxiv.org/pdf/2512.02005v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02004v1",
    "title": "AlignSAE: Concept-Aligned Sparse Autoencoders",
    "abstract": "Large Language Models (LLMs) encode factual knowledge within hidden parametric spaces that are difficult to inspect or control. While Sparse Autoencoders (SAEs) can decompose hidden activations into more fine-grained, interpretable features, they often struggle to reliably align these features with human-defined concepts, resulting in entangled and distributed feature representations. To address this, we introduce AlignSAE, a method that aligns SAE features with a defined ontology through a \"pre-train, then post-train\" curriculum. After an initial unsupervised training phase, we apply supervised post-training to bind specific concepts to dedicated latent slots while preserving the remaining capacity for general reconstruction. This separation creates an interpretable interface where specific relations can be inspected and controlled without interference from unrelated features. Empirical results demonstrate that AlignSAE enables precise causal interventions, such as reliable \"concept swaps\", by targeting single, semantically aligned slots.",
    "authors": [
      "Minglai Yang",
      "Xinyu Guo",
      "Mihai Surdeanu",
      "Liangming Pan"
    ],
    "published": "2025-12-01T18:58:22+00:00",
    "url": "https://arxiv.org/pdf/2512.02004v1",
    "categories": [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01996v1",
    "title": "Learning Sim-to-Real Humanoid Locomotion in 15 Minutes",
    "abstract": "Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions. We demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies. We provide videos and open-source implementation at: https://younggyo.me/fastsac-humanoid.",
    "authors": [
      "Younggyo Seo",
      "Carmelo Sferrazza",
      "Juyue Chen",
      "Guanya Shi",
      "Rocky Duan",
      "Pieter Abbeel"
    ],
    "published": "2025-12-01T18:55:17+00:00",
    "url": "https://arxiv.org/pdf/2512.01996v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01993v1",
    "title": "RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies",
    "abstract": "Autonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors. We introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policy's own closed-loop rollouts as additional training data. During rollout generation, RoaD incorporates expert guidance to bias trajectories toward high-quality behavior, producing informative yet realistic demonstrations for fine-tuning. This approach enables robust closed-loop adaptation with orders of magnitude less data than reinforcement learning, and avoids restrictive assumptions of prior closed-loop supervised fine-tuning (CL-SFT) methods, allowing broader applications domains including end-to-end driving. We demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstruction-based simulator for end-to-end driving, where it improves driving score by 41\\% and reduces collisions by 54\\%.",
    "authors": [
      "Guillermo Garcia-Cobo",
      "Maximilian Igl",
      "Peter Karkus",
      "Zhejun Zhang",
      "Michael Watson",
      "Yuxiao Chen",
      "Boris Ivanovic",
      "Marco Pavone"
    ],
    "published": "2025-12-01T18:52:03+00:00",
    "url": "https://arxiv.org/pdf/2512.01993v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01992v1",
    "title": "LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess",
    "abstract": "We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.",
    "authors": [
      "Sai Kolasani",
      "Maxim Saplin",
      "Nicholas Crispino",
      "Kyle Montgomery",
      "Jared Quincy Davis",
      "Matei Zaharia",
      "Chi Wang",
      "Chenguang Wang"
    ],
    "published": "2025-12-01T18:51:08+00:00",
    "url": "https://arxiv.org/pdf/2512.01992v1",
    "categories": [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01989v1",
    "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
    "abstract": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
    "authors": [
      "Fengzhe Zhou",
      "Jiannan Huang",
      "Jialuo Li",
      "Deva Ramanan",
      "Humphrey Shi"
    ],
    "published": "2025-12-01T18:47:39+00:00",
    "url": "https://arxiv.org/pdf/2512.01989v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01988v1",
    "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
    "abstract": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
    "authors": [
      "Wei Tang",
      "Yanpeng Sun",
      "Shan Zhang",
      "Xiaofan Li",
      "Piotr Koniusz",
      "Wei Li",
      "Na Zhao",
      "Zechao Li"
    ],
    "published": "2025-12-01T18:45:30+00:00",
    "url": "https://arxiv.org/pdf/2512.01988v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01987v2",
    "title": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments",
    "abstract": "Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.",
    "authors": [
      "Suzan Ece Ada",
      "Georg Martius",
      "Emre Ugur",
      "Erhan Oztop"
    ],
    "published": "2025-12-01T18:45:05+00:00",
    "url": "https://arxiv.org/pdf/2512.01987v2",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.01979v1",
    "title": "Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback",
    "abstract": "GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.",
    "authors": [
      "Aiden Yiliu Li",
      "Bizhi Yu",
      "Daoan Lei",
      "Tianhe Ren",
      "Shilong Liu"
    ],
    "published": "2025-12-01T18:37:19+00:00",
    "url": "https://arxiv.org/pdf/2512.01979v1",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01977v1",
    "title": "AI-Driven Optimization under Uncertainty for Mineral Processing Operations",
    "abstract": "The global capacity for mineral processing must expand rapidly to meet the demand for critical minerals, which are essential for building the clean energy technologies necessary to mitigate climate change. However, the efficiency of mineral processing is severely limited by uncertainty, which arises from both the variability of feedstock and the complexity of process dynamics. To optimize mineral processing circuits under uncertainty, we introduce an AI-driven approach that formulates mineral processing as a Partially Observable Markov Decision Process (POMDP). We demonstrate the capabilities of this approach in handling both feedstock uncertainty and process model uncertainty to optimize the operation of a simulated, simplified flotation cell as an example. We show that by integrating the process of information gathering (i.e., uncertainty reduction) and process optimization, this approach has the potential to consistently perform better than traditional approaches at maximizing an overall objective, such as net present value (NPV). Our methodological demonstration of this optimization-under-uncertainty approach for a synthetic case provides a mathematical and computational framework for later real-world application, with the potential to improve both the laboratory-scale design of experiments and industrial-scale operation of mineral processing circuits without any additional hardware.",
    "authors": [
      "William Xu",
      "Amir Eskanlou",
      "Mansur Arief",
      "David Zhen Yin",
      "Jef K. Caers"
    ],
    "published": "2025-12-01T18:35:54+00:00",
    "url": "https://arxiv.org/pdf/2512.01977v1",
    "categories": [
      "eess.SY",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01975v1",
    "title": "SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning",
    "abstract": "Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input.",
    "authors": [
      "Xu Zhang",
      "Jin Yuan",
      "Hanwang Zhang",
      "Guojin Zhong",
      "Yongsheng Zang",
      "Jiacheng Lin",
      "Zhiyong Li"
    ],
    "published": "2025-12-01T18:33:04+00:00",
    "url": "https://arxiv.org/pdf/2512.01975v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01970v2",
    "title": "From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning",
    "abstract": "The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.",
    "authors": [
      "Sitao Cheng",
      "Xunjian Yin",
      "Ruiwen Zhou",
      "Yuxuan Li",
      "Xinyi Wang",
      "Liangming Pan",
      "William Yang Wang",
      "Victor Zhong"
    ],
    "published": "2025-12-01T18:27:25+00:00",
    "url": "https://arxiv.org/pdf/2512.01970v2",
    "categories": [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01960v1",
    "title": "SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation",
    "abstract": "Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.",
    "authors": [
      "Zisu Li",
      "Hengye Lyu",
      "Jiaxin Shi",
      "Yufeng Zeng",
      "Mingming Fan",
      "Hanwang Zhang",
      "Chen Liang"
    ],
    "published": "2025-12-01T18:13:40+00:00",
    "url": "https://arxiv.org/pdf/2512.01960v1",
    "categories": [
      "cs.CV",
      "cs.HC"
    ]
  },
  {
    "arxiv_id": "2512.03100v1",
    "title": "Ensemble Privacy Defense for Knowledge-Intensive LLMs against Membership Inference Attacks",
    "abstract": "Retrieval-Augmented Generation (RAG) and Supervised Finetuning (SFT) have become the predominant paradigms for equipping Large Language Models (LLMs) with external knowledge for diverse, knowledge-intensive tasks. However, while such knowledge injection improves performance, it also exposes new attack surfaces. Membership Inference Attacks (MIAs), which aim to determine whether a given data sample was included in a model's training set, pose serious threats to privacy and trust in sensitive domains. To this end, we first systematically evaluate the vulnerability of RAG- and SFT-based LLMs to various MIAs. Then, to address the privacy risk, we further introduce a novel, model-agnostic defense framework, Ensemble Privacy Defense (EPD), which aggregates and evaluates the outputs of a knowledge-injected LLM, a base LLM, and a dedicated judge model to enhance resistance against MIAs. Comprehensive experiments show that, on average, EPD reduces MIA success by up to 27.8\\% for SFT and 526.3\\% for RAG compared to inference-time baseline, while maintaining answer quality.",
    "authors": [
      "Haowei Fu",
      "Bo Ni",
      "Han Xu",
      "Kunpeng Liu",
      "Dan Lin",
      "Tyler Derr"
    ],
    "published": "2025-12-01T18:12:18+00:00",
    "url": "https://arxiv.org/pdf/2512.03100v1",
    "categories": [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01958v1",
    "title": "Learned-Rule-Augmented Large Language Model Evaluators",
    "abstract": "Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.",
    "authors": [
      "Jie Meng",
      "Jin Mao"
    ],
    "published": "2025-12-01T18:08:45+00:00",
    "url": "https://arxiv.org/pdf/2512.01958v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01952v1",
    "title": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment",
    "abstract": "Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.",
    "authors": [
      "Haoyang He",
      "Jay Patrikar",
      "Dong-Ki Kim",
      "Max Smith",
      "Daniel McGann",
      "Ali-akbar Agha-mohammadi",
      "Shayegan Omidshafiei",
      "Sebastian Scherer"
    ],
    "published": "2025-12-01T18:03:29+00:00",
    "url": "https://arxiv.org/pdf/2512.01952v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.01949v1",
    "title": "Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models",
    "abstract": "The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.",
    "authors": [
      "Zhongyu Yang",
      "Dannong Xu",
      "Wei Pang",
      "Yingfang Yuan"
    ],
    "published": "2025-12-01T17:59:11+00:00",
    "url": "https://arxiv.org/pdf/2512.01949v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01948v1",
    "title": "How Far Are We from Genuinely Useful Deep Research Agents?",
    "abstract": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.",
    "authors": [
      "Dingling Zhang",
      "He Zhu",
      "Jincheng Ren",
      "Kangqi Song",
      "Xinran Zhou",
      "Boyu Feng",
      "Shudong Liu",
      "Jiabin Luo",
      "Weihao Xie",
      "Zhaohui Wang",
      "Tianrui Qin",
      "King Zhu",
      "Yuqing Wang",
      "Qianben Chen",
      "Yuchen Eleanor Jiang",
      "Wei Wang",
      "Jiaheng Liu",
      "Wangchunshu Zhou"
    ],
    "published": "2025-12-01T17:58:59+00:00",
    "url": "https://arxiv.org/pdf/2512.01948v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01946v2",
    "title": "Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models",
    "abstract": "Robust robotic manipulation requires reliable failure detection and recovery. Although current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data. To address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures. This method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world. With it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets. We then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection. Guardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data. Code, Data, and Models available at https://www.di.ens.fr/willow/research/guardian/.",
    "authors": [
      "Paul Pacaud",
      "Ricardo Garcia",
      "Shizhe Chen",
      "Cordelia Schmid"
    ],
    "published": "2025-12-01T17:57:27+00:00",
    "url": "https://arxiv.org/pdf/2512.01946v2",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01945v1",
    "title": "Agentic Policy Optimization via Instruction-Policy Co-Evolution",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.",
    "authors": [
      "Han Zhou",
      "Xingchen Wan",
      "Ivan Vuli\u0107",
      "Anna Korhonen"
    ],
    "published": "2025-12-01T17:56:29+00:00",
    "url": "https://arxiv.org/pdf/2512.01945v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01939v1",
    "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks",
    "abstract": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.",
    "authors": [
      "Yanlin Wang",
      "Xinyi Xu",
      "Jiachi Chen",
      "Tingting Bi",
      "Wenchao Gu",
      "Zibin Zheng"
    ],
    "published": "2025-12-01T17:52:15+00:00",
    "url": "https://arxiv.org/pdf/2512.01939v1",
    "categories": [
      "cs.SE",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01934v1",
    "title": "Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory",
    "abstract": "Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.",
    "authors": [
      "Chenyi Wang",
      "Yanmao Man",
      "Raymond Muller",
      "Ming Li",
      "Z. Berkay Celik",
      "Ryan Gerdes",
      "Jonathan Petit"
    ],
    "published": "2025-12-01T17:47:19+00:00",
    "url": "https://arxiv.org/pdf/2512.01934v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02790v1",
    "title": "UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits",
    "abstract": "With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, \\textbf{Qwen-Verify}, for efficient failure detection and instruction recaptioning. This pipeline yields \\textbf{UnicEdit-10M}, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose \\textbf{UnicBench}, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including \\textit{Non-edit Consistency} and \\textit{Reasoning Accuracy}. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.",
    "authors": [
      "Keming Ye",
      "Zhipeng Huang",
      "Canmiao Fu",
      "Qingyang Liu",
      "Jiani Cai",
      "Zheqi Lv",
      "Chen Li",
      "Jing Lyu",
      "Zhou Zhao",
      "Shengyu Zhang"
    ],
    "published": "2025-12-01T17:45:44+00:00",
    "url": "https://arxiv.org/pdf/2512.02790v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01930v1",
    "title": "SVRG and Beyond via Posterior Correction",
    "abstract": "Stochastic Variance Reduced Gradient (SVRG) and its variants aim to speed-up training by using gradient corrections, but have seen limited success in deep learning. Here, we show surprising new foundational connections of SVRG to a recently proposed Bayesian method called posterior correction. Specifically, we show that SVRG is recovered as a special case of posterior correction over the isotropic-Gaussian family, while novel extensions are automatically obtained by using more flexible exponential families. We derive two new SVRG variants by using Gaussian families: First, a Newton-like variant that employs novel Hessian corrections, and second, an Adam-like extension that improves pretraining and finetuning of Transformer language models. This is the first work to connect SVRG to Bayes and use it to boost variational training for deep networks.",
    "authors": [
      "Nico Daheim",
      "Thomas M\u00f6llenhoff",
      "Ming Liang Ang",
      "Mohammad Emtiyaz Khan"
    ],
    "published": "2025-12-01T17:45:30+00:00",
    "url": "https://arxiv.org/pdf/2512.01930v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01925v1",
    "title": "Rectifying LLM Thought from Lens of Optimization",
    "abstract": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.",
    "authors": [
      "Junnan Liu",
      "Hongwei Liu",
      "Songyang Zhang",
      "Kai Chen"
    ],
    "published": "2025-12-01T17:41:08+00:00",
    "url": "https://arxiv.org/pdf/2512.01925v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01924v1",
    "title": "Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model",
    "abstract": "Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.",
    "authors": [
      "Kentaro Fujii",
      "Shingo Murata"
    ],
    "published": "2025-12-01T17:41:01+00:00",
    "url": "https://arxiv.org/pdf/2512.01924v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01922v1",
    "title": "Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding",
    "abstract": "Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13\\% and improves hallucination accuracy by 6\\% relative to baseline medical LVLMs.",
    "authors": [
      "Zahra Mahdavi",
      "Zahra Khodakaramimaghsoud",
      "Hooman Khaloo",
      "Sina Bakhshandeh Taleshani",
      "Erfan Hashemi",
      "Javad Mirzapour Kaleybar",
      "Omid Nejati Manzari"
    ],
    "published": "2025-12-01T17:40:03+00:00",
    "url": "https://arxiv.org/pdf/2512.01922v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01913v1",
    "title": "Disentangling Progress in Medical Image Registration: Beyond Trend-Driven Architectures towards Domain-Specific Strategies",
    "abstract": "Medical image registration drives quantitative analysis across organs, modalities, and patient populations. Recent deep learning methods often combine low-level \"trend-driven\" computational blocks from computer vision, such as large-kernel CNNs, Transformers, and state-space models, with high-level registration-specific designs like motion pyramids, correlation layers, and iterative refinement. Yet, their relative contributions remain unclear and entangled. This raises a central question: should future advances in registration focus on importing generic architectural trends or on refining domain-specific design principles? Through a modular framework spanning brain, lung, cardiac, and abdominal registration, we systematically disentangle the influence of these two paradigms. Our evaluation reveals that low-level \"trend-driven\" computational blocks offer only marginal or inconsistent gains, while high-level registration-specific designs consistently deliver more accurate, smoother, and more robust deformations. These domain priors significantly elevate the performance of a standard U-Net baseline, far more than variants incorporating \"trend-driven\" blocks, achieving an average relative improvement of $\\sim3\\%$. All models and experiments are released within a transparent, modular benchmark that enables plug-and-play comparison for new architectures and registration tasks (https://github.com/BailiangJ/rethink-reg). This dynamic and extensible platform establishes a common ground for reproducible and fair evaluation, inviting the community to isolate genuine methodological contributions from domain priors. Our findings advocate a shift in research emphasis: from following architectural trends to embracing domain-specific design principles as the true drivers of progress in learning-based medical image registration.",
    "authors": [
      "Bailiang Jian",
      "Jiazhen Pan",
      "Rohit Jena",
      "Morteza Ghahremani",
      "Hongwei Bran Li",
      "Daniel Rueckert",
      "Christian Wachinger",
      "Benedikt Wiestler"
    ],
    "published": "2025-12-01T17:30:43+00:00",
    "url": "https://arxiv.org/pdf/2512.01913v1",
    "categories": [
      "eess.IV",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01909v1",
    "title": "Latent Debate: A Surrogate Framework for Interpreting LLM Thinking",
    "abstract": "Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.",
    "authors": [
      "Lihu Chen",
      "Xiang Yin",
      "Francesca Toni"
    ],
    "published": "2025-12-01T17:27:31+00:00",
    "url": "https://arxiv.org/pdf/2512.01909v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01908v1",
    "title": "SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception",
    "abstract": "Contact-rich robotic manipulation requires representations that encode local geometry. Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues. Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information. Most self-supervised learning (SSL) frameworks, however, compress feature maps into a global vector, discarding spatial structure and misaligning with the needs of manipulation. To address this, we propose SARL, a spatially-aware SSL framework that augments the Bootstrap Your Own Latent (BYOL) architecture with three map-level objectives, including Saliency Alignment (SAL), Patch-Prototype Distribution Alignment (PPDA), and Region Affinity Matching (RAM), to keep attentional focus, part composition, and geometric relations consistent across views. These losses act on intermediate feature maps, complementing the global objective. SARL consistently outperforms nine SSL baselines across six downstream tasks with fused visual-tactile data. On the geometry-sensitive edge-pose regression task, SARL achieves a Mean Absolute Error (MAE) of 0.3955, a 30% relative improvement over the next-best SSL method (0.5682 MAE) and approaching the supervised upper bound. These findings indicate that, for fused visual-tactile data, the most effective signal is structured spatial equivariance, in which features vary predictably with object geometry, which enables more capable robotic perception.",
    "authors": [
      "Gurmeher Khurana",
      "Lan Wei",
      "Dandan Zhang"
    ],
    "published": "2025-12-01T17:26:40+00:00",
    "url": "https://arxiv.org/pdf/2512.01908v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01896v1",
    "title": "OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation",
    "abstract": "Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.",
    "authors": [
      "Jinzheng Yu",
      "Yang Xu",
      "Haozhen Li",
      "Junqi Li",
      "Yifan Feng",
      "Ligu Zhu",
      "Hao Shen",
      "Lei Shi"
    ],
    "published": "2025-12-01T17:18:02+00:00",
    "url": "https://arxiv.org/pdf/2512.01896v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01895v1",
    "title": "StyleYourSmile: Cross-Domain Face Retargeting Without Paired Multi-Style Data",
    "abstract": "Cross-domain face retargeting requires disentangled control over identity, expressions, and domain-specific stylistic attributes. Existing methods, typically trained on real-world faces, either fail to generalize across domains, need test-time optimizations, or require fine-tuning with carefully curated multi-style datasets to achieve domain-invariant identity representations. In this work, we introduce \\textit{StyleYourSmile}, a novel one-shot cross-domain face retargeting method that eliminates the need for curated multi-style paired data. We propose an efficient data augmentation strategy alongside a dual-encoder framework, for extracting domain-invariant identity cues and capturing domain-specific stylistic variations. Leveraging these disentangled control signals, we condition a diffusion model to retarget facial expressions across domains. Extensive experiments demonstrate that \\textit{StyleYourSmile} achieves superior identity preservation and retargeting fidelity across a wide range of visual domains.",
    "authors": [
      "Avirup Dey",
      "Vinay Namboodiri"
    ],
    "published": "2025-12-01T17:14:07+00:00",
    "url": "https://arxiv.org/pdf/2512.01895v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01892v1",
    "title": "Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models",
    "abstract": "With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.",
    "authors": [
      "Heloisa Candello",
      "Muneeza Azmat",
      "Uma Sushmitha Gunturi",
      "Raya Horesh",
      "Rogerio Abreu de Paula",
      "Heloisa Pimentel",
      "Marcelo Carpinette Grave",
      "Aminat Adebiyi",
      "Tiago Machado",
      "Maysa Malfiza Garcia de Macedo"
    ],
    "published": "2025-12-01T17:12:28+00:00",
    "url": "https://arxiv.org/pdf/2512.01892v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ]
  },
  {
    "arxiv_id": "2512.01889v1",
    "title": "KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM",
    "abstract": "We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.",
    "authors": [
      "Zaid Nasser",
      "Mikhail Iumanov",
      "Tianhao Li",
      "Maxim Popov",
      "Jaafar Mahmoud",
      "Malik Mohrat",
      "Ilya Obrubov",
      "Ekaterina Derevyanka",
      "Ivan Sosin",
      "Sergey Kolyubin"
    ],
    "published": "2025-12-01T17:10:40+00:00",
    "url": "https://arxiv.org/pdf/2512.01889v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01885v1",
    "title": "TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals",
    "abstract": "Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.",
    "authors": [
      "Florian B\u00fcrger",
      "Martim Dias Gomes",
      "Nica Gutu",
      "Adri\u00e1n E. Granada",
      "No\u00e9mie Moreau",
      "Katarzyna Bozek"
    ],
    "published": "2025-12-01T17:08:12+00:00",
    "url": "https://arxiv.org/pdf/2512.01885v1",
    "categories": [
      "cs.CV",
      "q-bio.CB",
      "q-bio.QM"
    ]
  },
  {
    "arxiv_id": "2512.01881v2",
    "title": "Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion",
    "abstract": "The training of deep vision models is fundamentally a signal recovery problem amidst high-dimensional stochastic noise. Current optimization paradigms impose a static compromise on information channel capacity. For instance, magnitude-based methods, such as AdamW, operate on the assumption that gradient norms are high-fidelity curvature signals. While this allows for precision in smooth regimes, it leads to catastrophic noise amplification when applied to rugged, non-convex landscapes. Conversely, sign-based methods (e.g., Lion) perform a radical 1-bit quantization of the gradient, which aims to provide robust regularization at the cost of discarding fine-grained descent information. We propose that optimal convergence requires neither static prior, but rather a dynamic modulation of the update bitrate. We introduce ThermoLion, a vision-centric framework that utilizes local Signal-to-Noise Ratio (SNR) gating to autonomously transition parameters between a \"low-bit\" exploration phase and a \"high-precision\" exploitation phase. Furthermore, we introduce a Momentum Alignment mechanism that detects constructive interference between historical drift and instantaneous gradients to accelerate convergence during stable trajectories. Empirical benchmarks across 12 diverse vision datasets (including CIFAR, SVHN, and GTSRB) demonstrate that ThermoLion surpasses state-of-the-art optimizers, such as AdamW and Lion, in convergence speed and terminal accuracy.",
    "authors": [
      "Ahmed Nebli"
    ],
    "published": "2025-12-01T17:04:17+00:00",
    "url": "https://arxiv.org/pdf/2512.01881v2",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01880v1",
    "title": "Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models",
    "abstract": "Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels.   To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move prediction as a behavioral analysis task. The framework employs n-gram language models to capture move patterns characteristic of specific player skill levels. By dividing players into seven distinct skill groups, from novice to expert, we trained separate models using data from the open-source chess platform Lichess. The framework dynamically selects the most suitable model for prediction tasks and generates player moves based on preceding sequences.   Evaluation on real-world game data demonstrates that the model selector module within the framework can classify skill levels with an accuracy of up to 31.7\\% when utilizing early game information (16 half-moves). The move prediction framework also shows substantial accuracy improvements, with our Selector Assisted Accuracy being up to 39.1\\% more accurate than our benchmark accuracy. The computational efficiency of the framework further enhances its suitability for real-time chess analysis.",
    "authors": [
      "Daren Zhong",
      "Dingcheng Huang",
      "Clayton Greenberg"
    ],
    "published": "2025-12-01T17:02:07+00:00",
    "url": "https://arxiv.org/pdf/2512.01880v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01878v1",
    "title": "Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning",
    "abstract": "In this work, we propose that reasoning in knowledge graph (KG) networks can be guided by surprise minimization. Entities that are close in graph distance will have lower surprise than those farther apart. This connects the Free Energy Principle (FEP) from neuroscience to KG systems, where the KG serves as the agent's generative model. We formalize surprise using the shortest-path distance in directed graphs and provide a framework for KG-based agents. Graph distance appears in graph neural networks as message passing depth and in model-based reinforcement learning as world model trajectories. This work-in-progress study explores whether distance-based surprise can extend recent work showing that syntax minimizes surprise and free energy via tree structures.",
    "authors": [
      "Gaganpreet Jhajj",
      "Fuhua Lin"
    ],
    "published": "2025-12-01T16:59:28+00:00",
    "url": "https://arxiv.org/pdf/2512.01878v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02794v1",
    "title": "PhyCustom: Towards Realistic Physical Customization in Text-to-Image Generation",
    "abstract": "Recent diffusion-based text-to-image customization methods have achieved significant success in understanding concrete concepts to control generation processes, such as styles and shapes. However, few efforts dive into the realistic yet challenging customization of physical concepts. The core limitation of current methods arises from the absence of explicitly introducing physical knowledge during training. Even when physics-related words appear in the input text prompts, our experiments consistently demonstrate that these methods fail to accurately reflect the corresponding physical properties in the generated results. In this paper, we propose PhyCustom, a fine-tuning framework comprising two novel regularization losses to activate diffusion model to perform physical customization. Specifically, the proposed isometric loss aims at activating diffusion models to learn physical concepts while decouple loss helps to eliminate the mixture learning of independent concepts. Experiments are conducted on a diverse dataset and our benchmark results demonstrate that PhyCustom outperforms previous state-of-the-art and popular methods in terms of physical customization quantitatively and qualitatively.",
    "authors": [
      "Fan Wu",
      "Cheng Chen",
      "Zhoujie Fu",
      "Jiacheng Wei",
      "Yi Xu",
      "Deheng Ye",
      "Guosheng Lin"
    ],
    "published": "2025-12-01T16:57:02+00:00",
    "url": "https://arxiv.org/pdf/2512.02794v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02793v1",
    "title": "IC-World: In-Context Generation for Shared World Modeling",
    "abstract": "Video-based world models have recently garnered increasing attention for their ability to synthesize diverse and dynamic visual environments. In this paper, we focus on shared world modeling, where a model generates multiple videos from a set of input images, each representing the same underlying world in different camera poses. We propose IC-World, a novel generation framework, enabling parallel generation for all input images via activating the inherent in-context generation capability of large video models. We further finetune IC-World via reinforcement learning, Group Relative Policy Optimization, together with two proposed novel reward models to enforce scene-level geometry consistency and object-level motion consistency among the set of generated videos. Extensive experiments demonstrate that IC-World substantially outperforms state-of-the-art methods in both geometry and motion consistency. To the best of our knowledge, this is the first work to systematically explore the shared world modeling problem with video-based world models.",
    "authors": [
      "Fan Wu",
      "Jiacheng Wei",
      "Ruibo Li",
      "Yi Xu",
      "Junyou Li",
      "Deheng Ye",
      "Guosheng Lin"
    ],
    "published": "2025-12-01T16:52:02+00:00",
    "url": "https://arxiv.org/pdf/2512.02793v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01870v1",
    "title": "Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees",
    "abstract": "We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \\mathbb{N}\\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\\mathbb{N}\\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.",
    "authors": [
      "Alessandro Breccia",
      "Federica Gerace",
      "Marco Lippi",
      "Gabriele Sicuro",
      "Pierluigi Contucci"
    ],
    "published": "2025-12-01T16:51:38+00:00",
    "url": "https://arxiv.org/pdf/2512.01870v1",
    "categories": [
      "cs.AI",
      "cond-mat.dis-nn",
      "math-ph",
      "math.NT"
    ]
  },
  {
    "arxiv_id": "2512.01865v1",
    "title": "Cross-Lingual Interleaving for Speech Language Models",
    "abstract": "Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.",
    "authors": [
      "Adel Moumen",
      "Guangzhi Sun",
      "Philip C. Woodland"
    ],
    "published": "2025-12-01T16:48:05+00:00",
    "url": "https://arxiv.org/pdf/2512.01865v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01863v1",
    "title": "Topological Order in Deep State",
    "abstract": "Topologically ordered states are among the most interesting quantum phases of matter that host emergent quasi-particles having fractional charge and obeying fractional quantum statistics. Theoretical study of such states is however challenging owing to their strong-coupling nature that prevents conventional mean-field treatment. Here, we demonstrate that an attention-based deep neural network provides an expressive variational wavefunction that discovers fractional Chern insulator ground states purely through energy minimization without prior knowledge and achieves remarkable accuracy. We introduce an efficient method to extract ground state topological degeneracy -- a hallmark of topological order -- from a single optimized real-space wavefunction in translation-invariant systems by decomposing it into different many-body momentum sectors. Our results establish neural network variational Monte Carlo as a versatile tool for discovering strongly correlated topological phases.",
    "authors": [
      "Ahmed Abouelkomsan",
      "Max Geier",
      "Liang Fu"
    ],
    "published": "2025-12-01T16:46:39+00:00",
    "url": "https://arxiv.org/pdf/2512.01863v1",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.str-el",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01853v2",
    "title": "COACH: Collaborative Agents for Contextual Highlighting -- A Multi-Agent Framework for Sports Video Analysis",
    "abstract": "Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct \"cognitive tool\" specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video intelligence. The project homepage is available at https://aiden1020.github.io/COACH-project-page",
    "authors": [
      "Tsz-To Wong",
      "Ching-Chun Huang",
      "Hong-Han Shuai"
    ],
    "published": "2025-12-01T16:38:07+00:00",
    "url": "https://arxiv.org/pdf/2512.01853v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01852v1",
    "title": "BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages",
    "abstract": "Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.",
    "authors": [
      "Hrishikesh Terdalkar",
      "Kirtan Bhojani",
      "Aryan Dongare",
      "Omm Aditya Behera"
    ],
    "published": "2025-12-01T16:37:34+00:00",
    "url": "https://arxiv.org/pdf/2512.01852v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET"
    ]
  },
  {
    "arxiv_id": "2512.01850v1",
    "title": "Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching",
    "abstract": "Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.",
    "authors": [
      "Yue Pan",
      "Tao Sun",
      "Liyuan Zhu",
      "Lucas Nunes",
      "Iro Armeni",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "published": "2025-12-01T16:36:51+00:00",
    "url": "https://arxiv.org/pdf/2512.01850v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.01848v1",
    "title": "Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability",
    "abstract": "Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.",
    "authors": [
      "Jinghan Jia",
      "Nathalie Baracaldo",
      "Sijia Liu"
    ],
    "published": "2025-12-01T16:35:34+00:00",
    "url": "https://arxiv.org/pdf/2512.01848v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01843v1",
    "title": "PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models",
    "abstract": "Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \\textbf{PID} (\\textbf{P}hysical \\textbf{I}mplausibility \\textbf{D}etection) dataset, which consists of a \\textit{test split} of 500 manually annotated videos and a \\textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \\textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \\href{https://github.com/Zeqing-Wang/PhyDetEx}{https://github.com/Zeqing-Wang/PhyDetEx}.",
    "authors": [
      "Zeqing Wang",
      "Keze Wang",
      "Lei Zhang"
    ],
    "published": "2025-12-01T16:28:13+00:00",
    "url": "https://arxiv.org/pdf/2512.01843v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03099v1",
    "title": "QGShap: Quantum Acceleration for Faithful GNN Explanations",
    "abstract": "Graph Neural Networks (GNNs) have become indispensable in critical domains such as drug discovery, social network analysis, and recommendation systems, yet their black-box nature hinders deployment in scenarios requiring transparency and accountability. While Shapley value-based methods offer mathematically principled explanations by quantifying each component's contribution to predictions, computing exact values requires evaluating $2^n$ coalitions (or aggregating over $n!$ permutations), which is intractable for real-world graphs. Existing approximation strategies sacrifice either fidelity or efficiency, limiting their practical utility. We introduce QGShap, a quantum computing approach that leverages amplitude amplification to achieve quadratic speedups in coalition evaluation while maintaining exact Shapley computation. Unlike classical sampling or surrogate methods, our approach provides fully faithful explanations without approximation trade-offs for tractable graph sizes. We conduct empirical evaluations on synthetic graph datasets, demonstrating that QGShap achieves consistently high fidelity and explanation accuracy, matching or exceeding the performance of classical methods across all evaluation metrics. These results collectively demonstrate that QGShap not only preserves exact Shapley faithfulness but also delivers interpretable, stable, and structurally consistent explanations that align with the underlying graph reasoning of GNNs. The implementation of QGShap is available at https://github.com/smlab-niser/qgshap.",
    "authors": [
      "Haribandhu Jena",
      "Jyotirmaya Shivottam",
      "Subhankar Mishra"
    ],
    "published": "2025-12-01T16:19:15+00:00",
    "url": "https://arxiv.org/pdf/2512.03099v1",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01834v1",
    "title": "Mitigating Gender Bias in Depression Detection via Counterfactual Inference",
    "abstract": "Audio-based depression detection models have demonstrated promising performance but often suffer from gender bias due to imbalanced training data. Epidemiological statistics show a higher prevalence of depression in females, leading models to learn spurious correlations between gender and depression. Consequently, models tend to over-diagnose female patients while underperforming on male patients, raising significant fairness concerns. To address this, we propose a novel Counterfactual Debiasing Framework grounded in causal inference. We construct a causal graph to model the decision-making process and identify gender bias as the direct causal effect of gender on the prediction. During inference, we employ counterfactual inference to estimate and subtract this direct effect, ensuring the model relies primarily on authentic acoustic pathological features. Extensive experiments on the DAIC-WOZ dataset using two advanced acoustic backbones demonstrate that our framework not only significantly reduces gender bias but also improves overall detection performance compared to existing debiasing strategies.",
    "authors": [
      "Mingxuan Hu",
      "Hongbo Ma",
      "Xinlan Wu",
      "Ziqi Liu",
      "Jiaqi Liu",
      "Yangbin Chen"
    ],
    "published": "2025-12-01T16:14:20+00:00",
    "url": "https://arxiv.org/pdf/2512.01834v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.01831v1",
    "title": "Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models",
    "abstract": "Generative diversity varies significantly across discrete latent generative models such as AR, MIM, and Diffusion. We propose a diagnostic framework, grounded in Information Bottleneck (IB) theory, to analyze the underlying strategies resolving this behavior. The framework models generation as a conflict between a 'Compression Pressure' - a drive to minimize overall codebook entropy - and a 'Diversity Pressure' - a drive to maximize conditional entropy given an input. We further decompose this diversity into two primary sources: 'Path Diversity', representing the choice of high-level generative strategies, and 'Execution Diversity', the randomness in executing a chosen strategy. To make this decomposition operational, we introduce three zero-shot, inference-time interventions that directly perturb the latent generative process and reveal how models allocate and express diversity. Application of this probe-based framework to representative AR, MIM, and Diffusion systems reveals three distinct strategies: \"Diversity-Prioritized\" (MIM), \"Compression-Prioritized\" (AR), and \"Decoupled\" (Diffusion). Our analysis provides a principled explanation for their behavioral differences and informs a novel inference-time diversity enhancement technique.",
    "authors": [
      "Yudi Wu",
      "Wenhao Zhao",
      "Dianbo Liu"
    ],
    "published": "2025-12-01T16:13:23+00:00",
    "url": "https://arxiv.org/pdf/2512.01831v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01830v2",
    "title": "OpenREAD: Reinforced Open-Ended Reasoning for End-to-End Autonomous Driving with LLM-as-Critic",
    "abstract": "Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.",
    "authors": [
      "Songyan Zhang",
      "Wenhui Huang",
      "Zhan Chen",
      "Chua Jiahao Collister",
      "Qihang Huang",
      "Chen Lv"
    ],
    "published": "2025-12-01T16:11:57+00:00",
    "url": "https://arxiv.org/pdf/2512.01830v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01827v1",
    "title": "CauSight: Learning to Supersense for Visual Causal Discovery",
    "abstract": "Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.",
    "authors": [
      "Yize Zhang",
      "Meiqi Chen",
      "Sirui Chen",
      "Bo Peng",
      "Yanxi Zhang",
      "Tianyu Li",
      "Chaochao Lu"
    ],
    "published": "2025-12-01T16:05:13+00:00",
    "url": "https://arxiv.org/pdf/2512.01827v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01822v1",
    "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents",
    "abstract": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.",
    "authors": [
      "Jintian Zhang",
      "Kewei Xu",
      "Jingsheng Zheng",
      "Zhuoyun Yu",
      "Yuqi Zhu",
      "Yujie Luo",
      "Lanning Wei",
      "Shuofei Qiao",
      "Lun Du",
      "Da Zheng",
      "Shumin Deng",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "published": "2025-12-01T16:03:04+00:00",
    "url": "https://arxiv.org/pdf/2512.01822v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ]
  },
  {
    "arxiv_id": "2512.01821v1",
    "title": "Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling",
    "abstract": "Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.",
    "authors": [
      "Meng Cao",
      "Haokun Lin",
      "Haoyuan Li",
      "Haoran Tang",
      "Rongtao Xu",
      "Dong An",
      "Xue Liu",
      "Ian Reid",
      "Xiaodan Liang"
    ],
    "published": "2025-12-01T16:01:41+00:00",
    "url": "https://arxiv.org/pdf/2512.01821v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01818v1",
    "title": "Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual Learning",
    "abstract": "Deep neural networks suffer from catastrophic forgetting, where performance on previous tasks degrades after training on a new task. This issue arises due to the model's tendency to overwrite previously acquired knowledge with new information. We present a novel approach to address this challenge, focusing on the intersection of memory-based methods and regularization approaches. We formulate a regularization strategy, termed Information Maximization (IM) regularizer, for memory-based continual learning methods, which is based exclusively on the expected label distribution, thus making it class-agnostic. As a consequence, IM regularizer can be directly integrated into various rehearsal-based continual learning methods, reducing forgetting and favoring faster convergence. Our empirical validation shows that, across datasets and regardless of the number of tasks, our proposed regularization strategy consistently improves baseline performance at the expense of a minimal computational overhead. The lightweight nature of IM ensures that it remains a practical and scalable solution, making it applicable to real-world continual learning scenarios where efficiency is paramount. Finally, we demonstrate the data-agnostic nature of our regularizer by applying it to video data, which presents additional challenges due to its temporal structure and higher memory requirements. Despite the significant domain gap, our experiments show that IM regularizer also improves the performance of video continual learning methods.",
    "authors": [
      "Lama Alssum",
      "Hasan Abed Al Kader Hammoud",
      "Motasem Alfarra",
      "Juan C Leon Alcazar",
      "Bernard Ghanem"
    ],
    "published": "2025-12-01T15:56:00+00:00",
    "url": "https://arxiv.org/pdf/2512.01818v1",
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01816v1",
    "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights",
    "abstract": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.",
    "authors": [
      "Juanxi Tian",
      "Siyuan Li",
      "Conghui He",
      "Lijun Wu",
      "Cheng Tan"
    ],
    "published": "2025-12-01T15:52:31+00:00",
    "url": "https://arxiv.org/pdf/2512.01816v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01803v2",
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "abstract": "Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.",
    "authors": [
      "Xavier Thomas",
      "Youngsun Lim",
      "Ananya Srinivasan",
      "Audrey Zheng",
      "Deepti Ghadiyaram"
    ],
    "published": "2025-12-01T15:36:33+00:00",
    "url": "https://arxiv.org/pdf/2512.01803v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01797v2",
    "title": "H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs",
    "abstract": "Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.",
    "authors": [
      "Cheng Gao",
      "Huimin Chen",
      "Chaojun Xiao",
      "Zhiyi Chen",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "published": "2025-12-01T15:32:14+00:00",
    "url": "https://arxiv.org/pdf/2512.01797v2",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.01789v1",
    "title": "SAM3-UNet: Simplified Adaptation of Segment Anything Model 3",
    "abstract": "In this paper, we introduce SAM3-UNet, a simplified variant of Segment Anything Model 3 (SAM3), designed to adapt SAM3 for downstream tasks at a low cost. Our SAM3-UNet consists of three components: a SAM3 image encoder, a simple adapter for parameter-efficient fine-tuning, and a lightweight U-Net-style decoder. Preliminary experiments on multiple tasks, such as mirror detection and salient object detection, demonstrate that the proposed SAM3-UNet outperforms the prior SAM2-UNet and other state-of-the-art methods, while requiring less than 6 GB of GPU memory during training with a batch size of 12. The code is publicly available at https://github.com/WZH0120/SAM3-UNet.",
    "authors": [
      "Xinyu Xiong",
      "Zihuang Wu",
      "Lei Lu",
      "Yufa Xia"
    ],
    "published": "2025-12-01T15:27:35+00:00",
    "url": "https://arxiv.org/pdf/2512.01789v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01788v1",
    "title": "Learned Image Compression for Earth Observation: Implications for Downstream Segmentation Tasks",
    "abstract": "The rapid growth of data from satellite-based Earth observation (EO) systems poses significant challenges in data transmission and storage. We evaluate the potential of task-specific learned compression algorithms in this context to reduce data volumes while retaining crucial information. In detail, we compare traditional compression (JPEG 2000) versus a learned compression approach (Discretized Mixed Gaussian Likelihood) on three EO segmentation tasks: Fire, cloud, and building detection. Learned compression notably outperforms JPEG 2000 for large-scale, multi-channel optical imagery in both reconstruction quality (PSNR) and segmentation accuracy. However, traditional codecs remain competitive on smaller, single-channel thermal infrared datasets due to limited data and architectural constraints. Additionally, joint end-to-end optimization of compression and segmentation models does not improve performance over standalone optimization.",
    "authors": [
      "Christian Molli\u00e8re",
      "Iker Cumplido",
      "Marco Zeulner",
      "Lukas Liesenhoff",
      "Matthias Schubert",
      "Julia Gottfriedsen"
    ],
    "published": "2025-12-01T15:27:33+00:00",
    "url": "https://arxiv.org/pdf/2512.01788v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01786v1",
    "title": "Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems",
    "abstract": "As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.",
    "authors": [
      "Xiaochuan Li",
      "Ke Wang",
      "Girija Gouda",
      "Shubham Choudhary",
      "Yaqun Wang",
      "Linwei Hu",
      "Joel Vaughan",
      "Freddy Lecue"
    ],
    "published": "2025-12-01T15:26:20+00:00",
    "url": "https://arxiv.org/pdf/2512.01786v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01782v1",
    "title": "Dual Randomized Smoothing: Beyond Global Noise Variance",
    "abstract": "Randomized Smoothing (RS) is a prominent technique for certifying the robustness of neural networks against adversarial perturbations. With RS, achieving high accuracy at small radii requires a small noise variance, while achieving high accuracy at large radii requires a large noise variance. However, the global noise variance used in the standard RS formulation leads to a fundamental limitation: there exists no global noise variance that simultaneously achieves strong performance at both small and large radii. To break through the global variance limitation, we propose a dual RS framework which enables input-dependent noise variances. To achieve that, we first prove that RS remains valid with input-dependent noise variances, provided the variance is locally constant around each input. Building on this result, we introduce two components which form our dual RS framework: (i) a variance estimator first predicts an optimal noise variance for each input, (ii) this estimated variance is then used by a standard RS classifier. The variance estimator is independently smoothed via RS to ensure local constancy, enabling flexible design. We also introduce training strategies to iteratively optimize the two components. Extensive experiments on CIFAR-10 show that our dual RS method provides strong performance for both small and large radii-unattainable with global noise variance-while incurring only a 60% computational overhead at inference. Moreover, it consistently outperforms prior input-dependent noise approaches across most radii, with particularly large gains at radii 0.5, 0.75, and 1.0, achieving relative improvements of 19%, 24%, and 21%, respectively. On ImageNet, dual RS remains effective across all radii. Additionally, the dual RS framework naturally provides a routing perspective for certified robustness, improving the accuracy-robustness trade-off with off-the-shelf expert RS models.",
    "authors": [
      "Chenhao Sun",
      "Yuhao Mao",
      "Martin Vechev"
    ],
    "published": "2025-12-01T15:23:00+00:00",
    "url": "https://arxiv.org/pdf/2512.01782v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01774v1",
    "title": "Evaluating SAM2 for Video Semantic Segmentation",
    "abstract": "The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.",
    "authors": [
      "Syed Hesham Syed Ariff",
      "Yun Liu",
      "Guolei Sun",
      "Jing Yang",
      "Henghui Ding",
      "Xue Geng",
      "Xudong Jiang"
    ],
    "published": "2025-12-01T15:15:16+00:00",
    "url": "https://arxiv.org/pdf/2512.01774v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01771v1",
    "title": "Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels",
    "abstract": "Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques. Unlike conventional layers that learn all features without specific bias, our approach begins with a predefined edge detection kernel, which is then perturbed with random noise. These kernels are learned during training to extract optimal edge features tailored to the task. This adaptive edge detection enhances the registration process by capturing diverse structural features critical in medical imaging. To provide clearer insight into the contribution of each component in our design, we introduce four variant models for rigid registration and four variant models for non-rigid registration. We evaluated our approach using a dataset provided by the Medical University across three setups: rigid registration without skull removal, with skull removal, and non-rigid registration. Additionally, we assessed performance on two publicly available datasets. Across all experiments, our method consistently outperformed state-of-the-art techniques, demonstrating its potential to improve multi-modal image alignment and anatomical structure analysis.",
    "authors": [
      "Ahsan Raza Siyal",
      "Markus Haltmeier",
      "Ruth Steiger",
      "Malik Galijasevic",
      "Elke Ruth Gizewski",
      "Astrid Ellen Grams"
    ],
    "published": "2025-12-01T15:13:33+00:00",
    "url": "https://arxiv.org/pdf/2512.01771v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01769v1",
    "title": "VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis",
    "abstract": "Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.   This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.",
    "authors": [
      "Hafsa Billah"
    ],
    "published": "2025-12-01T15:09:46+00:00",
    "url": "https://arxiv.org/pdf/2512.01769v1",
    "categories": [
      "cs.CV",
      "cs.DB"
    ]
  },
  {
    "arxiv_id": "2512.01763v1",
    "title": "HiconAgent: History Context-aware Policy Optimization for GUI Agents",
    "abstract": "Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.",
    "authors": [
      "Xurui Zhou",
      "Gongwei Chen",
      "Yuquan Xie",
      "Zaijing Li",
      "Kaiwen Zhou",
      "Shuai Wang",
      "Shuo Yang",
      "Zhuotao Tian",
      "Rui Shao"
    ],
    "published": "2025-12-01T15:06:45+00:00",
    "url": "https://arxiv.org/pdf/2512.01763v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01759v1",
    "title": "Weight Space Representation Learning with Neural Fields",
    "abstract": "In this work, we investigate the potential of weights to serve as effective representations, focusing on neural fields. Our key insight is that constraining the optimization space through a pre-trained base model and low-rank adaptation (LoRA) can induce structure in weight space. Across reconstruction, generation, and analysis tasks on 2D and 3D data, we find that multiplicative LoRA weights achieve high representation quality while exhibiting distinctiveness and semantic structure. When used with latent diffusion models, multiplicative LoRA weights enable higher-quality generation than existing weight-space methods.",
    "authors": [
      "Zhuoqian Yang",
      "Mathieu Salzmann",
      "Sabine S\u00fcsstrunk"
    ],
    "published": "2025-12-01T15:05:01+00:00",
    "url": "https://arxiv.org/pdf/2512.01759v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01755v1",
    "title": "FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing",
    "abstract": "Instruction-based image editing through natural language has emerged as a powerful paradigm for intuitive visual manipulation. While recent models achieve impressive results on single edits, they suffer from severe quality degradation under multi-turn editing. Through systematic analysis, we identify progressive loss of high-frequency information as the primary cause of this quality degradation. We present FreqEdit, a training-free framework that enables stable editing across 10+ consecutive iterations. Our approach comprises three synergistic components: (1) high-frequency feature injection from reference velocity fields to preserve fine-grained details, (2) an adaptive injection strategy that spatially modulates injection strength for precise region-specific control, and (3) a path compensation mechanism that periodically recalibrates the editing trajectory to prevent over-constraint. Extensive experiments demonstrate that FreqEdit achieves superior performance in both identity preservation and instruction following compared to seven state-of-the-art baselines.",
    "authors": [
      "Yucheng Liao",
      "Jiajun Liang",
      "Kaiqian Cui",
      "Baoquan Zhao",
      "Haoran Xie",
      "Wei Liu",
      "Qing Li",
      "Xudong Mao"
    ],
    "published": "2025-12-01T15:00:47+00:00",
    "url": "https://arxiv.org/pdf/2512.01755v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01728v1",
    "title": "Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference",
    "abstract": "This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.",
    "authors": [
      "Zhengjia Wang",
      "Danding Wang",
      "Qiang Sheng",
      "Jiaying Wu",
      "Juan Cao"
    ],
    "published": "2025-12-01T14:37:00+00:00",
    "url": "https://arxiv.org/pdf/2512.01728v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01725v1",
    "title": "Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks",
    "abstract": "Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \\textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \\textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \\textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.",
    "authors": [
      "Jiannan Guan",
      "Qiguang Chen",
      "Libo Qin",
      "Dengyun Peng",
      "Jinhao Liu",
      "Liangyu Huo",
      "Jian Xie",
      "Wanxiang Che"
    ],
    "published": "2025-12-01T14:35:06+00:00",
    "url": "https://arxiv.org/pdf/2512.01725v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01723v1",
    "title": "Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation",
    "abstract": "Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N << 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.",
    "authors": [
      "Saba Kublashvili"
    ],
    "published": "2025-12-01T14:35:04+00:00",
    "url": "https://arxiv.org/pdf/2512.01723v1",
    "categories": [
      "cs.AI",
      "cs.GT",
      "math.PR"
    ]
  },
  {
    "arxiv_id": "2512.01713v1",
    "title": "Self-Supervised Borrowing Detection on Multilingual Wordlists",
    "abstract": "This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity measures such as NED and SCA, and that the combined similarity performs on par with or better than supervised baselines. An ablation study highlights the importance of character encoding, temperature settings and augmentation strategies. The approach scales to datasets of different sizes, works without manual supervision and is provided with a command-line tool that allows researchers to conduct their own studies.",
    "authors": [
      "Tim Wientzek"
    ],
    "published": "2025-12-01T14:20:03+00:00",
    "url": "https://arxiv.org/pdf/2512.01713v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01710v1",
    "title": "MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications",
    "abstract": "Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.",
    "authors": [
      "Stefano Zeppieri"
    ],
    "published": "2025-12-01T14:16:57+00:00",
    "url": "https://arxiv.org/pdf/2512.01710v1",
    "categories": [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "arxiv_id": "2512.01707v1",
    "title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos",
    "abstract": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.",
    "authors": [
      "Daeun Lee",
      "Subhojyoti Mukherjee",
      "Branislav Kveton",
      "Ryan A. Rossi",
      "Viet Dac Lai",
      "Seunghyun Yoon",
      "Trung Bui",
      "Franck Dernoncourt",
      "Mohit Bansal"
    ],
    "published": "2025-12-01T14:15:44+00:00",
    "url": "https://arxiv.org/pdf/2512.01707v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01701v1",
    "title": "SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation",
    "abstract": "In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.",
    "authors": [
      "Xiuli Bi",
      "Die Xiao",
      "Junchao Fan",
      "Bin Xiao"
    ],
    "published": "2025-12-01T14:06:50+00:00",
    "url": "https://arxiv.org/pdf/2512.01701v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02088v1",
    "title": "Comparing Baseline and Day-1 Diffusion MRI Using Multimodal Deep Embeddings for Stroke Outcome Prediction",
    "abstract": "This study compares baseline (J0) and 24-hour (J1) diffusion magnetic resonance imaging (MRI) for predicting three-month functional outcomes after acute ischemic stroke (AIS). Seventy-four AIS patients with paired apparent diffusion coefficient (ADC) scans and clinical data were analyzed. Three-dimensional ResNet-50 embeddings were fused with structured clinical variables, reduced via principal component analysis (<=12 components), and classified using linear support vector machines with eight-fold stratified group cross-validation. J1 multimodal models achieved the highest predictive performance (AUC = 0.923 +/- 0.085), outperforming J0-based configurations (AUC <= 0.86). Incorporating lesion-volume features further improved model stability and interpretability. These findings demonstrate that early post-treatment diffusion MRI provides superior prognostic value to pre-treatment imaging and that combining MRI, clinical, and lesion-volume features produces a robust and interpretable framework for predicting three-month functional outcomes in AIS patients.",
    "authors": [
      "Sina Raeisadigh",
      "Myles Joshua Toledo Tan",
      "Henning M\u00fcller",
      "Abderrahmane Hedjoudje"
    ],
    "published": "2025-12-01T13:56:44+00:00",
    "url": "https://arxiv.org/pdf/2512.02088v1",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01687v1",
    "title": "Revisiting Direct Encoding: Learnable Temporal Dynamics for Static Image Spiking Neural Networks",
    "abstract": "Handling static images that lack inherent temporal dynamics remains a fundamental challenge for spiking neural networks (SNNs). In directly trained SNNs, static inputs are typically repeated across time steps, causing the temporal dimension to collapse into a rate like representation and preventing meaningful temporal modeling. This work revisits the reported performance gap between direct and rate based encodings and shows that it primarily stems from convolutional learnability and surrogate gradient formulations rather than the encoding schemes themselves. To illustrate this mechanism level clarification, we introduce a minimal learnable temporal encoding that adds adaptive phase shifts to induce meaningful temporal variation from static inputs.",
    "authors": [
      "Huaxu He"
    ],
    "published": "2025-12-01T13:55:00+00:00",
    "url": "https://arxiv.org/pdf/2512.01687v1",
    "categories": [
      "cs.NE",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01686v1",
    "title": "DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models",
    "abstract": "Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/",
    "authors": [
      "Patrick Kwon",
      "Chen Chen"
    ],
    "published": "2025-12-01T13:51:41+00:00",
    "url": "https://arxiv.org/pdf/2512.01686v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01681v1",
    "title": "Cross-Domain Validation of a Resection-Trained Self-Supervised Model on Multicentre Mesothelioma Biopsies",
    "abstract": "Accurate subtype classification and outcome prediction in mesothelioma are essential for guiding therapy and patient care. Most computational pathology models are trained on large tissue images from resection specimens, limiting their use in real-world settings where small biopsies are common. We show that a self-supervised encoder trained on resection tissue can be applied to biopsy material, capturing meaningful morphological patterns. Using these patterns, the model can predict patient survival and classify tumor subtypes. This approach demonstrates the potential of AI-driven tools to support diagnosis and treatment planning in mesothelioma.",
    "authors": [
      "Farzaneh Seyedshahi",
      "Francesca Damiola",
      "Sylvie Lantuejoul",
      "Ke Yuan",
      "John Le Quesne"
    ],
    "published": "2025-12-01T13:46:43+00:00",
    "url": "https://arxiv.org/pdf/2512.01681v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01677v1",
    "title": "Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation",
    "abstract": "Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is https://hgzn258.github.io/SCAR/.",
    "authors": [
      "Haodong Yan",
      "Hang Yu",
      "Zhide Zhong",
      "Weilin Yuan",
      "Xin Gong",
      "Zehang Luo",
      "Chengxi Heyu",
      "Junfeng Li",
      "Wenxuan Song",
      "Shunbo Zhou",
      "Haoang Li"
    ],
    "published": "2025-12-01T13:44:31+00:00",
    "url": "https://arxiv.org/pdf/2512.01677v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01675v1",
    "title": "GRASP: Guided Residual Adapters with Sample-wise Partitioning",
    "abstract": "Recent advances in text-to-image diffusion models enable high-fidelity generation across diverse prompts. However, these models falter in long-tail settings, such as medical imaging, where rare pathologies comprise a small fraction of data. This results in mode collapse: tail-class outputs lack quality and diversity, undermining the goal of synthetic data augmentation for underrepresented conditions. We pinpoint gradient conflicts between frequent head and rare tail classes as the primary culprit, a factor unaddressed by existing sampling or conditioning methods that mainly steer inference without altering the learned distribution. To resolve this, we propose GRASP: Guided Residual Adapters with Sample-wise Partitioning. GRASP uses external priors to statically partition samples into clusters that minimize intra-group gradient clashes. It then fine-tunes pre-trained models by injecting cluster-specific residual adapters into transformer feedforward layers, bypassing learned gating for stability and efficiency. On the long-tail MIMIC-CXR-LT dataset, GRASP yields superior FID and diversity metrics, especially for rare classes, outperforming baselines like vanilla fine-tuning and Mixture of Experts variants. Downstream classification on NIH-CXR-LT improves considerably for tail labels. Generalization to ImageNet-LT confirms broad applicability. Our method is lightweight, scalable, and readily integrates with diffusion pipelines.",
    "authors": [
      "Felix N\u00fctzel",
      "Mischa Dombrowski",
      "Bernhard Kainz"
    ],
    "published": "2025-12-01T13:43:17+00:00",
    "url": "https://arxiv.org/pdf/2512.01675v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01672v1",
    "title": "ICAD-LLM: One-for-All Anomaly Detection via In-Context Learning with Large Language Models",
    "abstract": "Anomaly detection (AD) is a fundamental task of critical importance across numerous domains. Current systems increasingly operate in rapidly evolving environments that generate diverse yet interconnected data modalities -- such as time series, system logs, and tabular records -- as exemplified by modern IT systems. Effective AD methods in such environments must therefore possess two critical capabilities: (1) the ability to handle heterogeneous data formats within a unified framework, allowing the model to process and detect multiple modalities in a consistent manner during anomalous events; (2) a strong generalization ability to quickly adapt to new scenarios without extensive retraining. However, most existing methods fall short of these requirements, as they typically focus on single modalities and lack the flexibility to generalize across domains. To address this gap, we introduce a novel paradigm: In-Context Anomaly Detection (ICAD), where anomalies are defined by their dissimilarity to a relevant reference set of normal samples. Under this paradigm, we propose ICAD-LLM, a unified AD framework leveraging Large Language Models' in-context learning abilities to process heterogeneous data within a single model. Extensive experiments demonstrate that ICAD-LLM achieves competitive performance with task-specific AD methods and exhibits strong generalization to previously unseen tasks, which substantially reduces deployment costs and enables rapid adaptation to new environments. To the best of our knowledge, ICAD-LLM is the first model capable of handling anomaly detection tasks across diverse domains and modalities.",
    "authors": [
      "Zhongyuan Wu",
      "Jingyuan Wang",
      "Zexuan Cheng",
      "Yilong Zhou",
      "Weizhi Wang",
      "Juhua Pu",
      "Chao Li",
      "Changqing Ma"
    ],
    "published": "2025-12-01T13:41:30+00:00",
    "url": "https://arxiv.org/pdf/2512.01672v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01665v1",
    "title": "Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery",
    "abstract": "Tiny object detection in remote sensing imagery has attracted significant research interest in recent years. Despite recent progress, achieving balanced detection performance across diverse object scales remains a formidable challenge, particularly in scenarios where dense tiny objects and large objects coexist. Although large foundation models have revolutionized general vision tasks, their application to tiny object detection remains unexplored due to the extreme scale variation and density distribution inherent to remote sensing imagery. To bridge this scale gap, we propose ScaleBridge-Det, to the best of our knowledge, the first large detection framework designed for tiny objects, which could achieve balanced performance across diverse scales through scale-adaptive expert routing and density-guided query allocation. Specifically, we introduce a Routing-Enhanced Mixture Attention (REM) module that dynamically selects and fuses scale-specific expert features via adaptive routing to address the tendency of standard MoE models to favor dominant scales. REM generates complementary and discriminative multi-scale representations suitable for both tiny and large objects. Furthermore, we present a Density-Guided Dynamic Query (DGQ) module that predicts object density to adaptively adjust query positions and numbers, enabling efficient resource allocation for objects of varying scales. The proposed framework allows ScaleBridge-Det to simultaneously optimize performance for both dense tiny and general objects without trade-offs. Extensive experiments on benchmark and cross-domain datasets demonstrate that ScaleBridge-Det achieves state-of-the-art performance on AI-TOD-V2 and DTOD, while exhibiting superior cross-domain robustness on VisDrone.",
    "authors": [
      "Zhicheng Zhao",
      "Yin Huang",
      "Lingma Sun",
      "Chenglong Li",
      "Jin Tang"
    ],
    "published": "2025-12-01T13:36:09+00:00",
    "url": "https://arxiv.org/pdf/2512.01665v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01661v1",
    "title": "Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems",
    "abstract": "Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel \"Reverse Construction\" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.",
    "authors": [
      "Dengyun Peng",
      "Qiguang Chen",
      "Bofei Liu",
      "Jiannan Guan",
      "Libo Qin",
      "Zheng Yan",
      "Jinhao Liu",
      "Jianshu Zhang",
      "Wanxiang Che"
    ],
    "published": "2025-12-01T13:32:59+00:00",
    "url": "https://arxiv.org/pdf/2512.01661v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01659v1",
    "title": "HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment",
    "abstract": "Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \\textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \\textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($>$400 words, $>$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \\approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.",
    "authors": [
      "Valentin No\u00ebl",
      "Elimane Yassine Seidou",
      "Charly Ken Capo-Chichi",
      "Ghanem Amari"
    ],
    "published": "2025-12-01T13:31:06+00:00",
    "url": "https://arxiv.org/pdf/2512.01659v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01657v1",
    "title": "DB-KAUNet: An Adaptive Dual Branch Kolmogorov-Arnold UNet for Retinal Vessel Segmentation",
    "abstract": "Accurate segmentation of retinal vessels is crucial for the clinical diagnosis of numerous ophthalmic and systemic diseases. However, traditional Convolutional Neural Network (CNN) methods exhibit inherent limitations, struggling to capture long-range dependencies and complex nonlinear relationships. To address the above limitations, an Adaptive Dual Branch Kolmogorov-Arnold UNet (DB-KAUNet) is proposed for retinal vessel segmentation. In DB-KAUNet, we design a Heterogeneous Dual-Branch Encoder (HDBE) that features parallel CNN and Transformer pathways. The HDBE strategically interleaves standard CNN and Transformer blocks with novel KANConv and KAT blocks, enabling the model to form a comprehensive feature representation. To optimize feature processing, we integrate several critical components into the HDBE. First, a Cross-Branch Channel Interaction (CCI) module is embedded to facilitate efficient interaction of channel features between the parallel pathways. Second, an attention-based Spatial Feature Enhancement (SFE) module is employed to enhance spatial features and fuse the outputs from both branches. Building upon the SFE module, an advanced Spatial Feature Enhancement with Geometrically Adaptive Fusion (SFE-GAF) module is subsequently developed. In the SFE-GAF module, adaptive sampling is utilized to focus on true vessel morphology precisely. The adaptive process strengthens salient vascular features while significantly reducing background noise and computational overhead. Extensive experiments on the DRIVE, STARE, and CHASE_DB1 datasets validate that DB-KAUNet achieves leading segmentation performance and demonstrates exceptional robustness.",
    "authors": [
      "Hongyu Xu",
      "Panpan Meng",
      "Meng Wang",
      "Dayu Hu",
      "Liming Liang",
      "Xiaoqi Sheng"
    ],
    "published": "2025-12-01T13:30:01+00:00",
    "url": "https://arxiv.org/pdf/2512.01657v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01643v1",
    "title": "ViT$^3$: Unlocking Test-Time Training in Vision",
    "abstract": "Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at https://github.com/LeapLabTHU/ViTTT.",
    "authors": [
      "Dongchen Han",
      "Yining Li",
      "Tianyu Li",
      "Zixuan Cao",
      "Ziming Wang",
      "Jun Song",
      "Yu Cheng",
      "Bo Zheng",
      "Gao Huang"
    ],
    "published": "2025-12-01T13:14:48+00:00",
    "url": "https://arxiv.org/pdf/2512.01643v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01636v1",
    "title": "Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval",
    "abstract": "Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.",
    "authors": [
      "Xin Wang",
      "Haipeng Zhang",
      "Mang Li",
      "Zhaohui Xia",
      "Yueguo Chen",
      "Yu Zhang",
      "Chunyu Wei"
    ],
    "published": "2025-12-01T13:04:55+00:00",
    "url": "https://arxiv.org/pdf/2512.01636v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01629v2",
    "title": "SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge",
    "abstract": "Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling. Project page: https://heyumeng.com/SPARK/index.html.",
    "authors": [
      "Yumeng He",
      "Ying Jiang",
      "Jiayin Lu",
      "Yin Yang",
      "Chenfanfu Jiang"
    ],
    "published": "2025-12-01T12:51:56+00:00",
    "url": "https://arxiv.org/pdf/2512.01629v2",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.01616v1",
    "title": "CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning",
    "abstract": "Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.",
    "authors": [
      "Chainesh Gautam",
      "Raghuram Bharadwaj Diddigi"
    ],
    "published": "2025-12-01T12:37:01+00:00",
    "url": "https://arxiv.org/pdf/2512.01616v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01611v1",
    "title": "Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager",
    "abstract": "In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction. This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm. The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment. We implement this by employing a combined feature set of the one-dimensional Histogram of Oriented Gradients (HOG1D) and the original signal as the shape descriptor. Field test examples demonstrate that our method achieves precise alignment for images with complex textures, depth shifts, or local scaling. Furthermore, it provides a flexible framework for feature extension, allowing the integration of other descriptors tailored to specific geological features.",
    "authors": [
      "Fengfeng Li",
      "Zhou Feng",
      "Hongliang Wu",
      "Hao Zhang",
      "Han Tian",
      "Peng Liu",
      "Lixin Yuan"
    ],
    "published": "2025-12-01T12:31:09+00:00",
    "url": "https://arxiv.org/pdf/2512.01611v1",
    "categories": [
      "cs.CV",
      "physics.geo-ph"
    ]
  },
  {
    "arxiv_id": "2512.01603v1",
    "title": "MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark",
    "abstract": "Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\\footnote{https://github.com/Gatsby-web/MAC\\_SLU} and datasets\\footnote{huggingface.co/datasets/Gatsby1984/MAC\\_SLU} are released publicly.",
    "authors": [
      "Yuezhang Peng",
      "Chonghao Cai",
      "Ziang Liu",
      "Shuai Fan",
      "Sheng Jiang",
      "Hua Xu",
      "Yuxin Liu",
      "Qiguang Chen",
      "Kele Xu",
      "Yao Li",
      "Sheng Wang",
      "Libo Qin",
      "Xie Chen"
    ],
    "published": "2025-12-01T12:23:19+00:00",
    "url": "https://arxiv.org/pdf/2512.01603v1",
    "categories": [
      "cs.CL",
      "cs.MM"
    ]
  },
  {
    "arxiv_id": "2512.01589v2",
    "title": "Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation",
    "abstract": "Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly. Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention. In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses. The dataset is designed to facilitate the development of robust semantic segmentation models that can accurately delineate abscess boundaries and evaluate deep neck space involvement, thereby supporting informed clinical decision-making. To establish performance baselines, we evaluate several state-of-the-art segmentation architectures, including CNN, Transformer, and Mamba-based models. The highest-performing model achieved a Dice Similarity Coefficient of 0.39, Intersection-over-Union of 0.27, and Normalized Surface Distance of 0.67, indicating the challenges of this task and the need for further research. Beyond segmentation, AbscessHeNe is structured for future applications in content-based multimedia indexing and case-based retrieval. Each CT scan is linked with pixel-level annotations and clinical metadata, providing a foundation for building intelligent retrieval systems and supporting knowledge-driven clinical workflows. The dataset will be made publicly available at https://github.com/drthaodao3101/AbscessHeNe.git.",
    "authors": [
      "Thao Thi Phuong Dao",
      "Tan-Cong Nguyen",
      "Trong-Le Do",
      "Truong Hoang Viet",
      "Nguyen Chi Thanh",
      "Huynh Nguyen Thuan",
      "Do Vo Cong Nguyen",
      "Minh-Khoi Pham",
      "Mai-Khiem Tran",
      "Viet-Tham Huynh",
      "Trong-Thuan Nguyen",
      "Trung-Nghia Le",
      "Vo Thanh Toan",
      "Tam V. Nguyen",
      "Minh-Triet Tran",
      "Thanh Dinh Le"
    ],
    "published": "2025-12-01T12:04:24+00:00",
    "url": "https://arxiv.org/pdf/2512.01589v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01582v1",
    "title": "RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions",
    "abstract": "In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.",
    "authors": [
      "Junran Peng",
      "Yiheng Huang",
      "Silei Shen",
      "Zeji Wei",
      "Jingwei Yang",
      "Baojie Wang",
      "Yonghao He",
      "Chuanchen Luo",
      "Man Zhang",
      "Xucheng Yin",
      "Wei Sui"
    ],
    "published": "2025-12-01T11:59:03+00:00",
    "url": "https://arxiv.org/pdf/2512.01582v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01576v1",
    "title": "From Black Hole to Galaxy: Neural Operator: Framework for Accretion and Feedback Dynamics",
    "abstract": "Modeling how supermassive black holes co-evolve with their host galaxies is notoriously hard because the relevant physics spans nine orders of magnitude in scale-from milliparsecs to megaparsecs--making end-to-end first-principles simulation infeasible. To characterize the feedback from the small scales, existing methods employ a static subgrid scheme or one based on theoretical guesses, which usually struggle to capture the time variability and derive physically faithful results. Neural operators are a class of machine learning models that achieve significant speed-up in simulating complex dynamics. We introduce a neural-operator-based ''subgrid black hole'' that learns the small-scale local dynamics and embeds it within the direct multi-level simulations. Trained on small-domain (general relativistic) magnetohydrodynamic data, the model predicts the unresolved dynamics needed to supply boundary conditions and fluxes at coarser levels across timesteps, enabling stable long-horizon rollouts without hand-crafted closures. Thanks to the great speedup in fine-scale evolution, our approach for the first time captures intrinsic variability in accretion-driven feedback, allowing dynamic coupling between the central black hole and galaxy-scale gas. This work reframes subgrid modeling in computational astrophysics with scale separation and provides a scalable path toward data-driven closures for a broad class of systems with central accretors.",
    "authors": [
      "Nihaal Bhojwani",
      "Chuwei Wang",
      "Hai-Yang Wang",
      "Chang Sun",
      "Elias R. Most",
      "Anima Anandkumar"
    ],
    "published": "2025-12-01T11:47:49+00:00",
    "url": "https://arxiv.org/pdf/2512.01576v1",
    "categories": [
      "astro-ph.HE",
      "astro-ph.GA",
      "cs.AI",
      "gr-qc"
    ]
  },
  {
    "arxiv_id": "2512.01572v1",
    "title": "Reconstructing Multi-Scale Physical Fields from Extremely Sparse Measurements with an Autoencoder-Diffusion Cascade",
    "abstract": "Reconstructing full fields from extremely sparse and random measurements is a longstanding ill-posed inverse problem. A powerful framework for addressing such challenges is hierarchical probabilistic modeling, where uncertainty is represented by intermediate variables and resolved through marginalization during inference. Inspired by this principle, we propose Cascaded Sensing (Cas-Sensing), a hierarchical reconstruction framework that integrates an autoencoder-diffusion cascade. First, a neural operator-based functional autoencoder reconstructs the dominant structures of the original field - including large-scale components and geometric boundaries - from arbitrary sparse inputs, serving as an intermediate variable. Then, a conditional diffusion model, trained with a mask-cascade strategy, generates fine-scale details conditioned on these large-scale structures. To further enhance fidelity, measurement consistency is enforced via the manifold constrained gradient based on Bayesian posterior sampling during the generation process. This cascaded pipeline substantially alleviates ill-posedness, delivering accurate and robust reconstructions. Experiments on both simulation and real-world datasets demonstrate that Cas-Sensing generalizes well across varying sensor configurations and geometric boundaries, making it a promising tool for practical deployment in scientific and engineering applications.",
    "authors": [
      "Letian Yi",
      "Tingpeng Zhang",
      "Mingyuan Zhou",
      "Guannan Wang",
      "Quanke Su",
      "Zhilu Lai"
    ],
    "published": "2025-12-01T11:46:14+00:00",
    "url": "https://arxiv.org/pdf/2512.01572v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.app-ph"
    ]
  },
  {
    "arxiv_id": "2512.01568v1",
    "title": "Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism",
    "abstract": "We investigate whether Large Language Models (LLMs) exhibit altruistic tendencies, and critically, whether their implicit associations and self-reports predict actual altruistic behavior. Using a multi-method approach inspired by human social psychology, we tested 24 frontier LLMs across three paradigms: (1) an Implicit Association Test (IAT) measuring implicit altruism bias, (2) a forced binary choice task measuring behavioral altruism, and (3) a self-assessment scale measuring explicit altruism beliefs. Our key findings are: (1) All models show strong implicit pro-altruism bias (mean IAT = 0.87, p < .0001), confirming models \"know\" altruism is good. (2) Models behave more altruistically than chance (65.6% vs. 50%, p < .0001), but with substantial variation (48-85%). (3) Implicit associations do not predict behavior (r = .22, p = .29). (4) Most critically, models systematically overestimate their own altruism, claiming 77.5% altruism while acting at 65.6% (p < .0001, Cohen's d = 1.08). This \"virtue signaling gap\" affects 75% of models tested. Based on these findings, we recommend the Calibration Gap (the discrepancy between self-reported and behavioral values) as a standardized alignment metric. Well-calibrated models are more predictable and behaviorally consistent; only 12.5% of models achieve the ideal combination of high prosocial behavior and accurate self-knowledge.",
    "authors": [
      "Sandro Andric"
    ],
    "published": "2025-12-01T11:43:02+00:00",
    "url": "https://arxiv.org/pdf/2512.01568v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.01565v1",
    "title": "Deep FlexQP: Accelerated Nonlinear Programming via Deep Unfolding",
    "abstract": "We propose an always-feasible quadratic programming (QP) optimizer, FlexQP, which is based on an exact relaxation of the QP constraints. If the original constraints are feasible, then the optimizer finds the optimal solution to the original QP. On the other hand, if the constraints are infeasible, the optimizer identifies a solution that minimizes the constraint violation in a sparse manner. FlexQP scales favorably with respect to the problem dimension, is robust to both feasible and infeasible QPs with minimal assumptions on the problem data, and can be effectively warm-started. We subsequently apply deep unfolding to improve our optimizer through data-driven techniques, leading to an accelerated Deep FlexQP. By learning dimension-agnostic feedback policies for the parameters from a small number of training examples, Deep FlexQP generalizes to problems with larger dimensions and can optimize for many more iterations than it was initially trained for. Our approach outperforms two recently proposed state-of-the-art accelerated QP approaches on a suite of benchmark systems including portfolio optimization, classification, and regression problems. We provide guarantees on the expected performance of our deep QP optimizer through probably approximately correct (PAC) Bayes generalization bounds. These certificates are used to design an accelerated sequential quadratic programming solver that solves nonlinear optimal control and predictive safety filter problems faster than traditional approaches. Overall, our approach is very robust and greatly outperforms existing non-learning and learning-based optimizers in terms of both runtime and convergence to the optimal solution across multiple classes of NLPs.",
    "authors": [
      "Alex Oshin",
      "Rahul Vodeb Ghosh",
      "Augustinos D. Saravanos",
      "Evangelos A. Theodorou"
    ],
    "published": "2025-12-01T11:38:45+00:00",
    "url": "https://arxiv.org/pdf/2512.01565v1",
    "categories": [
      "math.OC",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01563v2",
    "title": "MasHeNe: A Benchmark for Head and Neck CT Mass Segmentation using Window-Enhanced Mamba with Frequency-Domain Integration",
    "abstract": "Head and neck masses are space-occupying lesions that can compress the airway and esophagus and may affect nerves and blood vessels. Available public datasets primarily focus on malignant lesions and often overlook other space-occupying conditions in this region. To address this gap, we introduce MasHeNe, an initial dataset of 3,779 contrast-enhanced CT slices that includes both tumors and cysts with pixel-level annotations. We also establish a benchmark using standard segmentation baselines and report common metrics to enable fair comparison. In addition, we propose the Windowing-Enhanced Mamba with Frequency integration (WEMF) model. WEMF applies tri-window enhancement to enrich the input appearance before feature extraction. It further uses multi-frequency attention to fuse information across skip connections within a U-shaped Mamba backbone. On MasHeNe, WEMF attains the best performance among evaluated methods, with a Dice of 70.45%, IoU of 66.89%, NSD of 72.33%, and HD95 of 5.12 mm. This model indicates stable and strong results on this challenging task. MasHeNe provides a benchmark for head-and-neck mass segmentation beyond malignancy-only datasets. The observed error patterns also suggest that this task remains challenging and requires further research. Our dataset and code are available at https://github.com/drthaodao3101/MasHeNe.git.",
    "authors": [
      "Thao Thi Phuong Dao",
      "Tan-Cong Nguyen",
      "Nguyen Chi Thanh",
      "Truong Hoang Viet",
      "Trong-Le Do",
      "Mai-Khiem Tran",
      "Minh-Khoi Pham",
      "Trung-Nghia Le",
      "Minh-Triet Tran",
      "Thanh Dinh Le"
    ],
    "published": "2025-12-01T11:38:05+00:00",
    "url": "https://arxiv.org/pdf/2512.01563v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01557v1",
    "title": "Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces",
    "abstract": "This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.",
    "authors": [
      "Edward Ajayi",
      "Eudoxie Umwari",
      "Mawuli Deku",
      "Prosper Singadi",
      "Jules Udahemuka",
      "Bekalu Tadele",
      "Chukuemeka Edeh"
    ],
    "published": "2025-12-01T11:27:13+00:00",
    "url": "https://arxiv.org/pdf/2512.01557v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01556v1",
    "title": "LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems",
    "abstract": "Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.",
    "authors": [
      "Zhiyuan Wang",
      "Aniri",
      "Tianlong Chen",
      "Yue Zhang",
      "Heng Tao Shen",
      "Xiaoshuang Shi",
      "Kaidi Xu"
    ],
    "published": "2025-12-01T11:27:09+00:00",
    "url": "https://arxiv.org/pdf/2512.01556v1",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01550v1",
    "title": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction",
    "abstract": "Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.",
    "authors": [
      "Fei Liu",
      "Shichao Xie",
      "Minghua Luo",
      "Zedong Chu",
      "Junjun Hu",
      "Xiaolong Wu",
      "Mu Xu"
    ],
    "published": "2025-12-01T11:24:16+00:00",
    "url": "https://arxiv.org/pdf/2512.01550v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01549v1",
    "title": "Delta Sum Learning: an approach for fast and global convergence in Gossip Learning",
    "abstract": "Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity.",
    "authors": [
      "Tom Goethals",
      "Merlijn Sebrechts",
      "Stijn De Schrijver",
      "Filip De Turck",
      "Bruno Volckaert"
    ],
    "published": "2025-12-01T11:23:51+00:00",
    "url": "https://arxiv.org/pdf/2512.01549v1",
    "categories": [
      "cs.DC",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01546v1",
    "title": "LPCD: Unified Framework from Layer-Wise to Submodule Quantization",
    "abstract": "Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules and projecting the solutions with layer-wise quantizers. LPCD generalizes existing methods and provides a principled approach to quantizing complex submodules while maintaining the efficiency and compatibility of layer-wise PTQ pipelines. Across diverse LLM architectures and bit-widths, LPCD-based submodule quantization consistently enhances both layer-wise PTQ methods and existing submodule approaches.",
    "authors": [
      "Yuma Ichikawa",
      "Yudai Fujimoto",
      "Akira Sakai"
    ],
    "published": "2025-12-01T11:21:18+00:00",
    "url": "https://arxiv.org/pdf/2512.01546v1",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01540v1",
    "title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
    "abstract": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.",
    "authors": [
      "Zipeng Wang",
      "Dan Xu"
    ],
    "published": "2025-12-01T11:12:37+00:00",
    "url": "https://arxiv.org/pdf/2512.01540v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01537v1",
    "title": "Q2D2: A Geometry-Aware Audio Codec Leveraging Two-Dimensional Quantization",
    "abstract": "Recent neural audio codecs have achieved impressive reconstruction quality, typically relying on quantization methods such as Residual Vector Quantization (RVQ), Vector Quantization (VQ) and Finite Scalar Quantization (FSQ). However, these quantization techniques limit the geometric structure of the latent space, make it harder to capture correlations between features leading to inefficiency in representation learning, codebook utilization and token rate. In this paper we introduce Two Dimensional Quantization (Q2D2), a quantization scheme in which feature pairs are projected onto structured 2D grids such as hexagonal, rhombic, or rectangular tiling and quantized to the nearest grid values, yielding an implicit codebook defined by the product of grid levels, with codebook sizes comparable to conventional methods. Despite its simple geometric formulation, Q2D2 improves audio compression efficiency, with low token rates and high codebook utilization while maintaining state of the art reconstruction quality. Specifically, Q2D2 achieves competitive to superior performance in various objective and subjective reconstruction metrics, across extensive experiments in speech domain compared to state of the art models. Comprehensive ablation studies further confirm the effectiveness of our design choices.",
    "authors": [
      "Tal Shuster",
      "Eliya Nachmani"
    ],
    "published": "2025-12-01T11:06:38+00:00",
    "url": "https://arxiv.org/pdf/2512.01537v1",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "eess.SP"
    ]
  },
  {
    "arxiv_id": "2512.01534v1",
    "title": "Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis",
    "abstract": "Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healthy individuals across six scanners, with ages ranging from 6 to 89 years. Validation used 92 scans to tune hyperparameters and estimate unbiased thresholds. Testing encompassed 2,221 T1w and 1,262 T2w scans spanning healthy datasets and diverse clinical cohorts. Across all algorithms, the Dice-based segmentation performance varied between 0.03 and 0.65, indicating substantial variability. To assess robustness, we systematically evaluated the impact of different scanners, lesion types and sizes, as well as demographics (age, sex). Reconstruction-based methods, particularly diffusion-inspired approaches, achieved the strongest lesion segmentation performance, while feature-based methods showed greater robustness under distributional shifts. However, systematic biases, such as scanner-related effects, were observed for the majority of algorithms, including that small and low-contrast lesions were missed more often, and that false positives varied with age and sex. Increasing healthy training data yields only modest gains, underscoring that current unsupervised anomaly detection frameworks are limited algorithmically rather than by data availability. Our benchmark establishes a transparent foundation for future research and highlights priorities for clinical translation, including image native pretraining, principled deviation measures, fairness-aware modeling, and robust domain adaptation.",
    "authors": [
      "Alexander Frotscher",
      "Christian F. Baumgartner",
      "Thomas Wolfers"
    ],
    "published": "2025-12-01T11:03:27+00:00",
    "url": "https://arxiv.org/pdf/2512.01534v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01533v1",
    "title": "Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling",
    "abstract": "Diffusion models have emerged as a leading technique for generating images due to their ability to create high-resolution and realistic images. Despite their strong performance, diffusion models still struggle in managing image collections with significant feature differences. They often fail to capture complex features and produce conflicting results. Research has attempted to address this issue by learning different regions of an image through multiple diffusion paths and then combining them. However, this approach leads to inefficient coordination among multiple paths and high computational costs. To tackle these issues, this paper presents a Diffusion Fuzzy System (DFS), a latent-space multi-path diffusion model guided by fuzzy rules. DFS offers several advantages. First, unlike traditional multi-path diffusion methods, DFS uses multiple diffusion paths, each dedicated to learning a specific class of image features. By assigning each path to a different feature type, DFS overcomes the limitations of multi-path models in capturing heterogeneous image features. Second, DFS employs rule-chain-based reasoning to dynamically steer the diffusion process and enable efficient coordination among multiple paths. Finally, DFS introduces a fuzzy membership-based latent-space compression mechanism to reduce the computational costs of multi-path diffusion effectively. We tested our method on three public datasets: LSUN Bedroom, LSUN Church, and MS COCO. The results show that DFS achieves more stable training and faster convergence than existing single-path and multi-path diffusion models. Additionally, DFS surpasses baseline models in both image quality and alignment between text and images, and also shows improved accuracy when comparing generated images to target references.",
    "authors": [
      "Hailong Yang",
      "Te Zhang",
      "Kup-sze Choi",
      "Zhaohong Deng"
    ],
    "published": "2025-12-01T11:01:06+00:00",
    "url": "https://arxiv.org/pdf/2512.01533v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01523v1",
    "title": "Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report",
    "abstract": "Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled \"AI in Software Engineering\" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own.",
    "authors": [
      "Pankaj Jalote",
      "Y. Raghu Reddy",
      "Vasudeva Varma"
    ],
    "published": "2025-12-01T10:46:43+00:00",
    "url": "https://arxiv.org/pdf/2512.01523v1",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01519v1",
    "title": "QuantumCanvas: A Multimodal Benchmark for Visual Learning of Atomic Interactions",
    "abstract": "Despite rapid advances in molecular and materials machine learning, most models still lack physical transferability: they fit correlations across whole molecules or crystals rather than learning the quantum interactions between atomic pairs. Yet bonding, charge redistribution, orbital hybridization, and electronic coupling all emerge from these two-body interactions that define local quantum fields in many-body systems. We introduce QuantumCanvas, a large-scale multimodal benchmark that treats two-body quantum systems as foundational units of matter. The dataset spans 2,850 element-element pairs, each annotated with 18 electronic, thermodynamic, and geometric properties and paired with ten-channel image representations derived from l- and m-resolved orbital densities, angular field transforms, co-occupancy maps, and charge-density projections. These physically grounded images encode spatial, angular, and electrostatic symmetries without explicit coordinates, providing an interpretable visual modality for quantum learning. Benchmarking eight architectures across 18 targets, we report mean absolute errors of 0.201 eV on energy gap using GATv2, 0.265 eV on HOMO and 0.274 eV on LUMO using EGNN. For energy-related quantities, DimeNet attains 2.27 eV total-energy MAE and 0.132 eV repulsive-energy MAE, while a multimodal fusion model achieves a 2.15 eV Mermin free-energy MAE. Pretraining on QuantumCanvas further improves convergence stability and generalization when fine-tuned on larger datasets such as QM9, MD17, and CrysMTM. By unifying orbital physics with vision-based representation learning, QuantumCanvas provides a principled and interpretable basis for learning transferable quantum interactions through coupled visual and numerical modalities. Dataset and model implementations are available at https://github.com/KurbanIntelligenceLab/QuantumCanvas.",
    "authors": [
      "Can Polat",
      "Erchin Serpedin",
      "Mustafa Kurban",
      "Hasan Kurban"
    ],
    "published": "2025-12-01T10:44:25+00:00",
    "url": "https://arxiv.org/pdf/2512.01519v1",
    "categories": [
      "cs.CV",
      "cond-mat.mtrl-sci",
      "quant-ph"
    ]
  },
  {
    "arxiv_id": "2512.01512v1",
    "title": "MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages",
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at https://github.com/yxduir/m2m-70.",
    "authors": [
      "Yexing Du",
      "Kaiyuan Liu",
      "Youcheng Pan",
      "Bo Yang",
      "Keqi Deng",
      "Xie Chen",
      "Yang Xiang",
      "Ming Liu",
      "Bin Qin",
      "YaoWei Wang"
    ],
    "published": "2025-12-01T10:39:12+00:00",
    "url": "https://arxiv.org/pdf/2512.01512v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01510v1",
    "title": "Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation",
    "abstract": "We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation. To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM. During training, our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware, where we make a step towards closing the domain gap in this even more challenging setting. Overall, our evaluation shows that SRCSM can be considered a new state-of-the-art in DG for medical image segmentation and, moreover, even achieves a segmentation performance that matches the performance of the in-domain baseline in several settings.",
    "authors": [
      "Franz Thaler",
      "Martin Urschler",
      "Mateusz Kozinski",
      "Matthias AF Gsell",
      "Gernot Plank",
      "Darko Stern"
    ],
    "published": "2025-12-01T10:35:45+00:00",
    "url": "https://arxiv.org/pdf/2512.01510v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01507v1",
    "title": "SynthStrategy: Extracting and Formalizing Latent Strategic Insights from LLMs in Organic Chemistry",
    "abstract": "Modern computer-assisted synthesis planning (CASP) systems show promises at generating chemically valid reaction steps but struggle to incorporate strategic considerations such as convergent assembly, protecting group minimization, and optimal ring-forming sequences. We introduce a methodology that leverages Large Language Models to distill synthetic knowledge into code. Our system analyzes synthesis routes and translates strategic principles into Python functions representing diverse strategic and tactical rules, such as strategic functional group interconversions and ring construction strategies. By formalizing this knowledge as verifiable code rather than simple heuristics, we create testable, interpretable representations of synthetic strategy. We release the complete codebase and the USPTO-ST dataset -- synthesis routes annotated with strategic tags. This framework unlocks a novel capability for CASP: natural language-based route retrieval, achieving 75\\% Top-3 accuracy on our benchmark. We further validate our library through temporal analysis of historical trends and chemically intuitive route clustering that offers more granular partitioning than common previous methods. This work bridges the tactical-strategic divide in CASP, enabling specification, search, and evaluation of routes by strategic criteria rather than structure alone.",
    "authors": [
      "Daniel Armstrong",
      "Zlatko Jon\u010dev",
      "Andres M Bran",
      "Philippe Schwaller"
    ],
    "published": "2025-12-01T10:33:00+00:00",
    "url": "https://arxiv.org/pdf/2512.01507v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01502v1",
    "title": "Formal Verification of Noisy Quantum Reinforcement Learning Policies",
    "abstract": "Quantum reinforcement learning (QRL) aims to use quantum effects to create sequential decision-making policies that achieve tasks more effectively than their classical counterparts. However, QRL policies face uncertainty from quantum measurements and hardware noise, such as bit-flip, phase-flip, and depolarizing errors, which can lead to unsafe behavior. Existing work offers no systematic way to verify whether trained QRL policies meet safety requirements under specific noise conditions.   We introduce QVerifier, a formal verification method that applies probabilistic model checking to analyze trained QRL policies with and without modeled quantum noise. QVerifier builds a complete model of the policy-environment interaction, incorporates quantum uncertainty directly into the transition probabilities, and then checks safety properties using the Storm model checker.   Experiments across multiple QRL environments show that QVerifier precisely measures how different noise models influence safety, revealing both performance degradation and cases where noise can help. By enabling rigorous safety verification before deployment, QVerifier addresses a critical need: because access to quantum hardware is expensive, pre-deployment verification is essential for any safety-critical use of QRL. QVerifier targets a potential classical-quantum sweet spot: trained QRL policies that execute efficiently on quantum hardware, yet remain tractable for classical probabilistic model checking despite being too slow for real-time classical deployment.",
    "authors": [
      "Dennis Gross"
    ],
    "published": "2025-12-01T10:26:33+00:00",
    "url": "https://arxiv.org/pdf/2512.01502v1",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.FL"
    ]
  },
  {
    "arxiv_id": "2512.01495v1",
    "title": "ELVIS: Enhance Low-Light for Video Instance Segmentation in the Dark",
    "abstract": "Video instance segmentation (VIS) for low-light content remains highly challenging for both humans and machines alike, due to adverse imaging conditions including noise, blur and low-contrast. The lack of large-scale annotated datasets and the limitations of current synthetic pipelines, particularly in modeling temporal degradations, further hinder progress. Moreover, existing VIS methods are not robust to the degradations found in low-light videos and, as a result, perform poorly even when finetuned on low-light data. In this paper, we introduce \\textbf{ELVIS} (\\textbf{E}nhance \\textbf{L}ow-light for \\textbf{V}ideo \\textbf{I}nstance \\textbf{S}egmentation), a novel framework that enables effective domain adaptation of state-of-the-art VIS models to low-light scenarios. ELVIS comprises an unsupervised synthetic low-light video pipeline that models both spatial and temporal degradations, a calibration-free degradation profile synthesis network (VDP-Net) and an enhancement decoder head that disentangles degradations from content features. ELVIS improves performances by up to \\textbf{+3.7AP} on the synthetic low-light YouTube-VIS 2019 dataset. Code will be released upon acceptance.",
    "authors": [
      "Joanne Lin",
      "Ruirui Lin",
      "Yini Li",
      "David Bull",
      "Nantheera Anantrasirichai"
    ],
    "published": "2025-12-01T10:17:07+00:00",
    "url": "https://arxiv.org/pdf/2512.01495v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01494v1",
    "title": "A variational method for curve extraction with curvature-dependent energies",
    "abstract": "We introduce a variational approach for extracting curves between a list of possible endpoints, based on the discretization of an energy and Smirnov's decomposition theorem for vector fields. It is used to design a bi-level minimization approach to automatically extract curves and 1D structures from an image, which is mostly unsupervised. We extend then the method to curvature-dependent energies, using a now classical lifting of the curves in the space of positions and orientations equipped with an appropriate sub-Riemanian or Finslerian metric.",
    "authors": [
      "Majid Arthaud",
      "Antonin Chambolle",
      "Vincent Duval"
    ],
    "published": "2025-12-01T10:16:58+00:00",
    "url": "https://arxiv.org/pdf/2512.01494v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01485v1",
    "title": "Multi-Path Collaborative Reasoning via Reinforcement Learning",
    "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.",
    "authors": [
      "Jindi Lv",
      "Yuhao Zhou",
      "Zheng Zhu",
      "Xiaofeng Wang",
      "Guan Huang",
      "Jiancheng Lv"
    ],
    "published": "2025-12-01T10:05:46+00:00",
    "url": "https://arxiv.org/pdf/2512.01485v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01484v1",
    "title": "Multi-view diffusion geometry using intertwined diffusion trajectories",
    "abstract": "This paper introduces a comprehensive unified framework for constructing multi-view diffusion geometries through intertwined multi-view diffusion trajectories (MDTs), a class of inhomogeneous diffusion processes that iteratively combine the random walk operators of multiple data views. Each MDT defines a trajectory-dependent diffusion operator with a clear probabilistic and geometric interpretation, capturing over time the interplay between data views. Our formulation encompasses existing multi-view diffusion models, while providing new degrees of freedom for view interaction and fusion. We establish theoretical properties under mild assumptions, including ergodicity of both the point-wise operator and the process in itself. We also derive MDT-based diffusion distances, and associated embeddings via singular value decompositions. Finally, we propose various strategies for learning MDT operators within the defined operator space, guided by internal quality measures. Beyond enabling flexible model design, MDTs also offer a neutral baseline for evaluating diffusion-based approaches through comparison with randomly selected MDTs. Experiments show the practical impact of the MDT operators in a manifold learning and data clustering context.",
    "authors": [
      "Gwendal Debaussart-Joniec",
      "Argyris Kalogeratos"
    ],
    "published": "2025-12-01T10:05:19+00:00",
    "url": "https://arxiv.org/pdf/2512.01484v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.01481v1",
    "title": "ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling",
    "abstract": "Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.",
    "authors": [
      "Qisen Wang",
      "Yifan Zhao",
      "Peisen Shen",
      "Jialu Li",
      "Jia Li"
    ],
    "published": "2025-12-01T10:00:26+00:00",
    "url": "https://arxiv.org/pdf/2512.01481v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03095v1",
    "title": "Community Quality and Influence Maximization: An Empirical Study",
    "abstract": "Influence maximization in social networks plays a vital role in applications such as viral marketing, epidemiology, product recommendation, opinion mining, and counter-terrorism. A common approach identifies seed nodes by first detecting disjoint communities and subsequently selecting representative nodes from these communities. However, whether the quality of detected communities consistently affects the spread of influence under the Independent Cascade model remains unclear. This paper addresses this question by extending a previously proposed disjoint community detection method, termed $\u03b1$-Hierarchical Clustering, to the influence maximization problem under the Independent Cascade model. The proposed method is compared with an alternative approach that employs the same seed selection criteria but relies on communities of lower quality obtained through standard Hierarchical Clustering. The former is referred to as Hierarchical Clustering-based Influence Maximization, while the latter, which leverages higher-quality community structures to guide seed selection, is termed $\u03b1$-Hierarchical Clustering-based Influence Maximization. Extensive experiments are performed on multiple real-world datasets to assess the effectiveness of both methods. The results demonstrate that higher-quality community structures substantially improve information diffusion under the Independent Cascade model, particularly when the propagation probability is low. These findings underscore the critical importance of community quality in guiding effective seed selection for influence maximization in complex networks.",
    "authors": [
      "Motaz Ben Hassine"
    ],
    "published": "2025-12-01T09:59:04+00:00",
    "url": "https://arxiv.org/pdf/2512.03095v1",
    "categories": [
      "cs.SI",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01478v1",
    "title": "CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball",
    "abstract": "This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.",
    "authors": [
      "Omer Sela",
      "Michael Chertok",
      "Lior Wolf"
    ],
    "published": "2025-12-01T09:58:24+00:00",
    "url": "https://arxiv.org/pdf/2512.01478v1",
    "categories": [
      "cs.CV",
      "cs.MA"
    ]
  },
  {
    "arxiv_id": "2512.01473v1",
    "title": "Does Flatness imply Generalization for Logistic Loss in Univariate Two-Layer ReLU Network?",
    "abstract": "We consider the problem of generalization of arbitrarily overparameterized two-layer ReLU Neural Networks with univariate input. Recent work showed that under square loss, flat solutions (motivated by flat / stable minima and Edge of Stability phenomenon) provably cannot overfit, but it remains unclear whether the same phenomenon holds for logistic loss. This is a puzzling open problem because existing work on logistic loss shows that gradient descent with increasing step size converges to interpolating solutions (at infinity, for the margin-separable cases). In this paper, we prove that the \\emph{flatness implied generalization} is more delicate under logistic loss. On the positive side, we show that flat solutions enjoy near-optimal generalization bounds within a region between the left-most and right-most \\emph{uncertain} sets determined by each candidate solution. On the negative side, we show that there exist arbitrarily flat yet overfitting solutions at infinity that are (falsely) certain everywhere, thus certifying that flatness alone is insufficient for generalization in general. We demonstrate the effects predicted by our theory in a well-controlled simulation study.",
    "authors": [
      "Dan Qiao",
      "Yu-Xiang Wang"
    ],
    "published": "2025-12-01T09:57:11+00:00",
    "url": "https://arxiv.org/pdf/2512.01473v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.01461v1",
    "title": "Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging",
    "abstract": "Model merging has emerged as a promising paradigm for enabling multi-task capabilities without additional training. However, existing methods often experience substantial performance degradation compared with individually fine-tuned models, even on similar tasks, underscoring the need to preserve task-specific information. This paper proposes Decomposition, Thresholding, and Scaling (DTS), an approximation-based personalized merging framework that preserves task-specific information with minimal storage overhead. DTS first applies singular value decomposition to the task-specific information and retains only a small subset of singular values and vectors. It then introduces a novel thresholding strategy that partitions singular vector elements into groups and assigns a scaling factor to each group. To enable generalization to unseen tasks, we further extend DTS with a variant that fuses task-specific information in a data-free manner based on the semantic similarity of task characteristics. Extensive experiments demonstrate that DTS consistently outperforms state-of-the-art baselines while requiring only 1\\% additional storage per task. Furthermore, experiments on unseen tasks show that the DTS variant achieves significantly better generalization performance. Our code is available at https://github.com/krumpguo/DTS.",
    "authors": [
      "Kuangpu Guo",
      "Yuhe Ding",
      "Jian Liang",
      "Zilei Wang",
      "Ran He"
    ],
    "published": "2025-12-01T09:47:17+00:00",
    "url": "https://arxiv.org/pdf/2512.01461v1",
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01460v1",
    "title": "Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages",
    "abstract": "Limited data for low-resource languages typically yield weaker language models (LMs). Since pre-training is compute-intensive, it is more pragmatic to target improvements during fine-tuning. In this work, we examine the use of Active Learning (AL) methods augmented by structured data selection strategies which we term 'Active Learning schedulers', to boost the fine-tuning process with a limited amount of training data. We connect the AL to data clustering and propose an integrated fine-tuning pipeline that systematically combines AL, clustering, and dynamic data selection schedulers to enhance model's performance. Experiments in the Slovak, Maltese, Icelandic and Turkish languages show that the use of clustering during the fine-tuning phase together with AL scheduling can simultaneously produce annotation savings up to 30% and performance improvements up to four F1 score points, while also providing better fine-tuning stability.",
    "authors": [
      "Jozef Kub\u00edk",
      "Marek \u0160uppa",
      "Martin Tak\u00e1\u010d"
    ],
    "published": "2025-12-01T09:45:47+00:00",
    "url": "https://arxiv.org/pdf/2512.01460v1",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01457v2",
    "title": "ZIP-RC: Optimizing Test-Time Compute via Zero-Overhead Joint Reward-Cost Prediction",
    "abstract": "Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.",
    "authors": [
      "Rohin Manvi",
      "Joey Hong",
      "Tim Seyde",
      "Maxime Labonne",
      "Mathias Lechner",
      "Sergey Levine"
    ],
    "published": "2025-12-01T09:44:31+00:00",
    "url": "https://arxiv.org/pdf/2512.01457v2",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01452v1",
    "title": "Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework",
    "abstract": "Assessing risk of bias (RoB) in randomized controlled trials is essential for trustworthy evidence synthesis, but the process is resource-intensive and prone to variability across reviewers. Large language models (LLMs) offer a route to automation, but existing methods rely on manually engineered prompts that are difficult to reproduce, generalize, or evaluate. This study introduces a programmable RoB assessment pipeline that replaces ad-hoc prompt design with structured, code-based optimization using DSPy and its GEPA module. GEPA refines LLM reasoning through Pareto-guided search and produces inspectable execution traces, enabling transparent replication of every step in the optimization process. We evaluated the method on 100 RCTs from published meta-analyses across seven RoB domains. GEPA-generated prompts were applied to both open-weight models (Mistral Small 3.1 with GPT-oss-20b) and commercial models (GPT-5 Nano and GPT-5 Mini). In domains with clearer methodological reporting, such as Random Sequence Generation, GEPA-generated prompts performed best, with similar results for Allocation Concealment and Blinding of Participants, while the commercial model performed slightly better overall. We also compared GEPA with three manually designed prompts using Claude 3.5 Sonnet. GEPA achieved the highest overall accuracy and improved performance by 30%-40% in Random Sequence Generation and Selective Reporting, and showed generally comparable, competitively aligned performance in the other domains relative to manual prompts. These findings suggest that GEPA can produce consistent and reproducible prompts for RoB assessment, supporting the structured and principled use of LLMs in evidence synthesis.",
    "authors": [
      "Lingbo Li",
      "Anuradha Mathrani",
      "Teo Susnjak"
    ],
    "published": "2025-12-01T09:39:13+00:00",
    "url": "https://arxiv.org/pdf/2512.01452v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01444v1",
    "title": "FastAnimate: Towards Learnable Template Construction and Pose Deformation for Fast 3D Human Avatar Animation",
    "abstract": "3D human avatar animation aims at transforming a human avatar from an arbitrary initial pose to a specified target pose using deformation algorithms. Existing approaches typically divide this task into two stages: canonical template construction and target pose deformation. However, current template construction methods demand extensive skeletal rigging and often produce artifacts for specific poses. Moreover, target pose deformation suffers from structural distortions caused by Linear Blend Skinning (LBS), which significantly undermines animation realism. To address these problems, we propose a unified learning-based framework to address both challenges in two phases. For the former phase, to overcome the inefficiencies and artifacts during template construction, we leverage a U-Net architecture that decouples texture and pose information in a feed-forward process, enabling fast generation of a human template. For the latter phase, we propose a data-driven refinement technique that enhances structural integrity. Extensive experiments show that our model delivers consistent performance across diverse poses with an optimal balance between efficiency and quality,surpassing state-of-the-art (SOTA) methods.",
    "authors": [
      "Jian Shu",
      "Nanjie Yao",
      "Gangjian Zhang",
      "Junlong Ren",
      "Yu Feng",
      "Hao Wang"
    ],
    "published": "2025-12-01T09:28:50+00:00",
    "url": "https://arxiv.org/pdf/2512.01444v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01443v1",
    "title": "MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification",
    "abstract": "We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.",
    "authors": [
      "Xabier de Zuazo",
      "Ibon Saratxaga",
      "Eva Navas"
    ],
    "published": "2025-12-01T09:25:22+00:00",
    "url": "https://arxiv.org/pdf/2512.01443v1",
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.NE",
      "cs.SD"
    ]
  },
  {
    "arxiv_id": "2512.01442v1",
    "title": "PSA-MF: Personality-Sentiment Aligned Multi-Level Fusion for Multimodal Sentiment Analysis",
    "abstract": "Multimodal sentiment analysis (MSA) is a research field that recognizes human sentiments by combining textual, visual, and audio modalities. The main challenge lies in integrating sentiment-related information from different modalities, which typically arises during the unimodal feature extraction phase and the multimodal feature fusion phase. Existing methods extract only shallow information from unimodal features during the extraction phase, neglecting sentimental differences across different personalities. During the fusion phase, they directly merge the feature information from each modality without considering differences at the feature level. This ultimately affects the model's recognition performance. To address this problem, we propose a personality-sentiment aligned multi-level fusion framework. We introduce personality traits during the feature extraction phase and propose a novel personality-sentiment alignment method to obtain personalized sentiment embeddings from the textual modality for the first time. In the fusion phase, we introduce a novel multi-level fusion method. This method gradually integrates sentimental information from textual, visual, and audio modalities through multimodal pre-fusion and a multi-level enhanced fusion strategy. Our method has been evaluated through multiple experiments on two commonly used datasets, achieving state-of-the-art results.",
    "authors": [
      "Heng Xie",
      "Kang Zhu",
      "Zhengqi Wen",
      "Jianhua Tao",
      "Xuefei Liu",
      "Ruibo Fu",
      "Changsheng Li"
    ],
    "published": "2025-12-01T09:24:59+00:00",
    "url": "https://arxiv.org/pdf/2512.01442v1",
    "categories": [
      "cs.MM",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01440v1",
    "title": "A Selective Temporal Hamming distance to find patterns in state transition event timeseries, at scale",
    "abstract": "Discrete event systems are present both in observations of nature, socio economical sciences, and industrial systems. Standard analysis approaches do not usually exploit their dual event / state nature: signals are either modeled as transition event sequences, emphasizing event order alignment, or as categorical or ordinal state timeseries, usually resampled a distorting and costly operation as the observation period and number of events grow. In this work we define state transition event timeseries (STE-ts) and propose a new Selective Temporal Hamming distance (STH) leveraging both transition time and duration-in-state, avoiding costly and distorting resampling on large databases. STH generalizes both resampled Hamming and Jaccard metrics with better precision and computation time, and an ability to focus on multiple states of interest. We validate these benefits on simulated and real-world datasets.",
    "authors": [
      "Sylvain Mari\u00e9",
      "Pablo Knecht"
    ],
    "published": "2025-12-01T09:24:20+00:00",
    "url": "https://arxiv.org/pdf/2512.01440v1",
    "categories": [
      "cs.AI",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.01439v1",
    "title": "Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech",
    "abstract": "India's linguistic diversity presents both opportunities and challenges for fintech platforms. While the country has 31 major languages and over 100 minor ones, only 10\\% of the population understands English, creating barriers to financial inclusion. We present a multilingual conversational AI system for a financial assistance use case that supports code-mixed languages like Hinglish, enabling natural interactions for India's diverse user base. Our system employs a multi-agent architecture with language classification, function management, and multilingual response generation. Through comparative analysis of multiple language models and real-world deployment, we demonstrate significant improvements in user engagement while maintaining low latency overhead (4-8\\%). This work contributes to bridging the language gap in digital financial services for emerging markets.",
    "authors": [
      "Bharatdeep Hazarika",
      "Arya Suneesh",
      "Prasanna Devadiga",
      "Pawan Kumar Rajpoot",
      "Anshuman B Suresh",
      "Ahmed Ifthaquar Hussain"
    ],
    "published": "2025-12-01T09:23:13+00:00",
    "url": "https://arxiv.org/pdf/2512.01439v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01434v1",
    "title": "A Flexible Multi-Agent LLM-Human Framework for Fast Human Validated Tool Building",
    "abstract": "We introduce CollabToolBuilder, a flexible multiagent LLM framework with expert-in-the-loop (HITL) guidance that iteratively learns to create tools for a target goal, aligning with human intent and process, while minimizing time for task/domain adaptation effort and human feedback capture. The architecture generates and validates tools via four specialized agents (Coach, Coder, Critic, Capitalizer) using a reinforced dynamic prompt and systematic human feedback integration to reinforce each agent's role toward goals and constraints. This work is best viewed as a system-level integration and methodology combining multi-agent in-context learning, HITL controls, and reusable tool capitalization for complex iterative problems such as scientific document generation. We illustrate it with preliminary experiments (e.g., generating state-of-the-art research papers or patents given an abstract) and discuss its applicability to other iterative problem-solving.",
    "authors": [
      "Daull Xavier",
      "Patrice Bellot",
      "Emmanuel Bruno",
      "Vincent Martin",
      "Elisabeth Murisasco"
    ],
    "published": "2025-12-01T09:19:18+00:00",
    "url": "https://arxiv.org/pdf/2512.01434v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01427v1",
    "title": "Language-Guided Open-World Anomaly Segmentation",
    "abstract": "Open-world and anomaly segmentation methods seek to enable autonomous driving systems to detect and segment both known and unknown objects in real-world scenes. However, existing methods do not assign semantically meaningful labels to unknown regions, and distinguishing and learning representations for unknown classes remains difficult. While open-vocabulary segmentation methods show promise in generalizing to novel classes, they require a fixed inference vocabulary and thus cannot be directly applied to anomaly segmentation where unknown classes are unconstrained. We propose Clipomaly, the first CLIP-based open-world and anomaly segmentation method for autonomous driving. Our zero-shot approach requires no anomaly-specific training data and leverages CLIP's shared image-text embedding space to both segment unknown objects and assign human-interpretable names to them. Unlike open-vocabulary methods, our model dynamically extends its vocabulary at inference time without retraining, enabling robust detection and naming of anomalies beyond common class definitions such as those in Cityscapes. Clipomaly achieves state-of-the-art performance on established anomaly segmentation benchmarks while providing interpretability and flexibility essential for practical deployment.",
    "authors": [
      "Klara Reichard",
      "Nikolas Brasch",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "published": "2025-12-01T09:08:59+00:00",
    "url": "https://arxiv.org/pdf/2512.01427v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01426v1",
    "title": "ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers",
    "abstract": "Leveraging pre-trained Diffusion Transformers (DiTs) for high-resolution (HR) image synthesis often leads to spatial layout collapse and degraded texture fidelity. Prior work mitigates these issues with complex pipelines that first perform a base-resolution (i.e., training-resolution) denoising process to guide HR generation. We instead explore the intrinsic generative mechanisms of DiTs and propose ResDiT, a training-free method that scales resolution efficiently. We identify the core factor governing spatial layout, position embeddings (PEs), and show that the original PEs encode incorrect positional information when extrapolated to HR, which triggers layout collapse. To address this, we introduce a PE scaling technique that rectifies positional encoding under resolution changes. To further remedy low-fidelity details, we develop a local-enhancement mechanism grounded in base-resolution local attention. We design a patch-level fusion module that aggregates global and local cues, together with a Gaussian-weighted splicing strategy that eliminates grid artifacts. Comprehensive evaluations demonstrate that ResDiT consistently delivers high-fidelity, high-resolution image synthesis and integrates seamlessly with downstream tasks, including spatially controlled generation.",
    "authors": [
      "Yiyang Ma",
      "Feng Zhou",
      "Xuedan Yin",
      "Pu Cao",
      "Yonghao Dang",
      "Jianqin Yin"
    ],
    "published": "2025-12-01T09:08:01+00:00",
    "url": "https://arxiv.org/pdf/2512.01426v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01424v2",
    "title": "\\textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models",
    "abstract": "As multimodal large language models (MLLMs) frequently exhibit errors in complex video reasoning scenarios, correcting these errors is critical for uncovering their weaknesses and improving performance. However, existing benchmarks lack systematic evaluation of MLLMs' ability to identify and correct these video reasoning errors. To bridge this gap, we propose \\textit{ViRectify}, a comprehensive benchmark to evaluate their fine-grained correction capability. Through an AI-assisted annotation pipeline with human verification, we construct a dataset of over 30\\textit{K} instances spanning dynamic perception, scientific reasoning, and embodied decision-making domains. In \\textit{ViRectify}, we challenge MLLMs to perform step-wise error identification and generate rationales with key video evidence grounding. In addition, we further propose the trajectory evidence-driven correction framework, comprising step-wise error trajectory and reward modeling on visual evidence-grounded correction. It encourages the model to explicitly concentrate on error propagation and key timestamps for correction. Extensive evaluation across 16 advanced MLLMs demonstrates that our \\textit{ViRectify} serves as a challenging testbed, where GPT-5 achieves only 31.94\\% correction accuracy. Our framework enables a Qwen2.5-VL-7B to consistently outperform the variants of 72B on \\textit{ViRectify}, showing the effectiveness of our approach. Further analysis uncovers systematic asymmetries in error correction across models, and our dataset is also a valuable data resource to perform reflection learning. We believe \\textit{ViRectify} provides a new direction for comprehensively evaluating the advanced MLLMs in video reasoning.",
    "authors": [
      "Xusen Hei",
      "Jiali Chen",
      "Jinyu Yang",
      "Mengchen Zhao",
      "Yi Cai"
    ],
    "published": "2025-12-01T09:05:02+00:00",
    "url": "https://arxiv.org/pdf/2512.01424v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01422v1",
    "title": "MDiff4STR: Mask Diffusion Model for Scene Text Recognition",
    "abstract": "Mask Diffusion Models (MDMs) have recently emerged as a promising alternative to auto-regressive models (ARMs) for vision-language tasks, owing to their flexible balance of efficiency and accuracy. In this paper, for the first time, we introduce MDMs into the Scene Text Recognition (STR) task. We show that vanilla MDM lags behind ARMs in terms of accuracy, although it improves recognition efficiency. To bridge this gap, we propose MDiff4STR, a Mask Diffusion model enhanced with two key improvement strategies tailored for STR. Specifically, we identify two key challenges in applying MDMs to STR: noising gap between training and inference, and overconfident predictions during inference. Both significantly hinder the performance of MDMs. To mitigate the first issue, we develop six noising strategies that better align training with inference behavior. For the second, we propose a token-replacement noise mechanism that provides a non-mask noise type, encouraging the model to reconsider and revise overly confident but incorrect predictions. We conduct extensive evaluations of MDiff4STR on both standard and challenging STR benchmarks, covering diverse scenarios including irregular, artistic, occluded, and Chinese text, as well as whether the use of pretraining. Across these settings, MDiff4STR consistently outperforms popular STR models, surpassing state-of-the-art ARMs in accuracy, while maintaining fast inference with only three denoising steps. Code: https://github.com/Topdu/OpenOCR.",
    "authors": [
      "Yongkun Du",
      "Miaomiao Zhao",
      "Songlin Fan",
      "Zhineng Chen",
      "Caiyan Jia",
      "Yu-Gang Jiang"
    ],
    "published": "2025-12-01T08:57:51+00:00",
    "url": "https://arxiv.org/pdf/2512.01422v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01420v1",
    "title": "PromptBridge: Cross-Model Prompt Transfer for Large Language Models",
    "abstract": "Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.",
    "authors": [
      "Yaxuan Wang",
      "Quan Liu",
      "Zhenting Wang",
      "Zichao Li",
      "Wei Wei",
      "Yang Liu",
      "Yujia Bao"
    ],
    "published": "2025-12-01T08:55:45+00:00",
    "url": "https://arxiv.org/pdf/2512.01420v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01419v1",
    "title": "Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries",
    "abstract": "Vision-Language Models (VLMs) excel in multimodal tasks but often exhibit Western-centric biases, limiting their effectiveness in culturally diverse regions like Southeast Asia (SEA). To address this, we introduce RICE-VL, a novel benchmark evaluating VLM cultural understanding across 11 ASEAN countries. RICE-VL includes over 28,000 human-curated Visual Question Answering (VQA) samples -- covering True or False, Fill-in-the-Blank, and open-ended formats -- and 1,000 image-bounding box pairs for Visual Grounding, annotated by culturally informed experts across 14 sub-ground categories. We propose SEA-LAVE, an extension of the LAVE metric, assessing textual accuracy, cultural alignment, and country identification. Evaluations of six open- and closed-source VLMs reveal significant performance gaps in low-resource countries and abstract cultural domains. The Visual Grounding task tests models' ability to localize culturally significant elements in complex scenes, probing spatial and contextual accuracy. RICE-VL exposes limitations in VLMs' cultural comprehension and highlights the need for inclusive model development to better serve diverse global populations.",
    "authors": [
      "Tushar Pranav",
      "Eshan Pandey",
      "Austria Lyka Diane Bala",
      "Aman Chadha",
      "Indriyati Atmosukarto",
      "Donny Soh Cheng Lock"
    ],
    "published": "2025-12-01T08:55:41+00:00",
    "url": "https://arxiv.org/pdf/2512.01419v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01412v1",
    "title": "A Self-explainable Model of Long Time Series by Extracting Informative Structured Causal Patterns",
    "abstract": "Explainability is essential for neural networks that model long time series, yet most existing explainable AI methods only produce point-wise importance scores and fail to capture temporal structures such as trends, cycles, and regime changes. This limitation weakens human interpretability and trust in long-horizon models. To address these issues, we identify four key requirements for interpretable time-series modeling: temporal continuity, pattern-centric explanation, causal disentanglement, and faithfulness to the model's inference process. We propose EXCAP, a unified framework that satisfies all four requirements. EXCAP combines an attention-based segmenter that extracts coherent temporal patterns, a causally structured decoder guided by a pre-trained causal graph, and a latent aggregation mechanism that enforces representation stability. Our theoretical analysis shows that EXCAP provides smooth and stable explanations over time and is robust to perturbations in causal masks. Extensive experiments on classification and forecasting benchmarks demonstrate that EXCAP achieves strong predictive accuracy while generating coherent and causally grounded explanations. These results show that EXCAP offers a principled and scalable approach to interpretable modeling of long time series with relevance to high-stakes domains such as healthcare and finance.",
    "authors": [
      "Ziqian Wang",
      "Yuxiao Cheng",
      "Jinli Suo"
    ],
    "published": "2025-12-01T08:33:33+00:00",
    "url": "https://arxiv.org/pdf/2512.01412v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01410v1",
    "title": "DyFuLM: An Advanced Multimodal Framework for Sentiment Analysis",
    "abstract": "Understanding sentiment in complex textual expressions remains a fundamental challenge in affective computing. To address this, we propose a Dynamic Fusion Learning Model (DyFuLM), a multimodal framework designed to capture both hierarchical semantic representations and fine-grained emotional nuances. DyFuLM introduces two key moodules: a Hierarchical Dynamic Fusion module that adaptively integrates multi-level features, and a Gated Feature Aggregation module that regulates cross-layer information ffow to achieve balanced representation learning. Comprehensive experiments on multi-task sentiment datasets demonstrate that DyFuLM achieves 82.64% coarse-grained and 68.48% fine-grained accuracy, yielding the lowest regression errors (MAE = 0.0674, MSE = 0.0082) and the highest R^2 coefficient of determination (R^2= 0.6903). Furthermore, the ablation study validates the effectiveness of each module in DyFuLM. When all modules are removed, the accuracy drops by 0.91% for coarse-grained and 0.68% for fine-grained tasks. Keeping only the gated fusion module causes decreases of 0.75% and 0.55%, while removing the dynamic loss mechanism results in drops of 0.78% and 0.26% for coarse-grained and fine-grained sentiment classification, respectively. These results demonstrate that each module contributes significantly to feature interaction and task balance. Overall, the experimental findings further validate that DyFuLM enhances sentiment representation and overall performance through effective hierarchical feature fusion.",
    "authors": [
      "Ruohan Zhou",
      "Jiachen Yuan",
      "Churui Yang",
      "Wenzheng Huang",
      "Guoyan Zhang",
      "Shiyao Wei",
      "Jiazhen Hu",
      "Ning Xin",
      "Md Maruf Hasan"
    ],
    "published": "2025-12-01T08:30:10+00:00",
    "url": "https://arxiv.org/pdf/2512.01410v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01396v1",
    "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches",
    "abstract": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.   To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.",
    "authors": [
      "Zhiqing Zhong",
      "Jiaming Huang",
      "Pinjia He"
    ],
    "published": "2025-12-01T08:16:43+00:00",
    "url": "https://arxiv.org/pdf/2512.01396v1",
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.CR"
    ]
  },
  {
    "arxiv_id": "2512.01390v1",
    "title": "FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution",
    "abstract": "Real-image super-resolution (Real-ISR) seeks to recover HR images from LR inputs with mixed, unknown degradations. While diffusion models surpass GANs in perceptual quality, they under-reconstruct high-frequency (HF) details due to a low-frequency (LF) bias and a depth-wise \"low-first, high-later\" hierarchy. We introduce FRAMER, a plug-and-play training scheme that exploits diffusion priors without changing the backbone or inference. At each denoising step, the final-layer feature map teaches all intermediate layers. Teacher and student feature maps are decomposed into LF/HF bands via FFT masks to align supervision with the model's internal frequency hierarchy. For LF, an Intra Contrastive Loss (IntraCL) stabilizes globally shared structure. For HF, an Inter Contrastive Loss (InterCL) sharpens instance-specific details using random-layer and in-batch negatives. Two adaptive modulators, Frequency-based Adaptive Weight (FAW) and Frequency-based Alignment Modulation (FAM), reweight per-layer LF/HF signals and gate distillation by current similarity. Across U-Net and DiT backbones (e.g., Stable Diffusion 2, 3), FRAMER consistently improves PSNR/SSIM and perceptual metrics (LPIPS, NIQE, MANIQA, MUSIQ). Ablations validate the final-layer teacher and random-layer negatives.",
    "authors": [
      "Seungho Choi",
      "Jeahun Sung",
      "Jihyong Oh"
    ],
    "published": "2025-12-01T08:09:05+00:00",
    "url": "https://arxiv.org/pdf/2512.01390v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01389v1",
    "title": "Consistency Flow Model Achieves One-step Denoising Error Correction Codes",
    "abstract": "Error Correction Codes (ECC) are fundamental to reliable digital communication, yet designing neural decoders that are both accurate and computationally efficient remains challenging. Recent denoising diffusion decoders with transformer backbones achieve state-of-the-art performance, but their iterative sampling limits practicality in low-latency settings. We introduce the Error Correction Consistency Flow Model (ECCFM), an architecture-agnostic training framework for high-fidelity one-step decoding. By casting the reverse denoising process as a Probability Flow Ordinary Differential Equation (PF-ODE) and enforcing smoothness through a differential time regularization, ECCFM learns to map noisy signals along the decoding trajectory directly to the original codeword in a single inference step. Across multiple decoding benchmarks, ECCFM attains lower bit-error rates (BER) than autoregressive and diffusion-based baselines, with notable improvements on longer codes, while delivering inference speeds up from 30x to 100x faster than denoising diffusion decoders.",
    "authors": [
      "Haoyu Lei",
      "Chin Wa Lau",
      "Kaiwen Zhou",
      "Nian Guo",
      "Farzan Farnia"
    ],
    "published": "2025-12-01T08:07:51+00:00",
    "url": "https://arxiv.org/pdf/2512.01389v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01383v1",
    "title": "PointNet4D: A Lightweight 4D Point Cloud Video Backbone for Online and Offline Perception in Robotic Applications",
    "abstract": "Understanding dynamic 4D environments-3D space evolving over time-is critical for robotic and interactive systems. These applications demand systems that can process streaming point cloud video in real-time, often under resource constraints, while also benefiting from past and present observations when available. However, current 4D backbone networks rely heavily on spatiotemporal convolutions and Transformers, which are often computationally intensive and poorly suited to real-time applications. We propose PointNet4D, a lightweight 4D backbone optimized for both online and offline settings. At its core is a Hybrid Mamba-Transformer temporal fusion block, which integrates the efficient state-space modeling of Mamba and the bidirectional modeling power of Transformers. This enables PointNet4D to handle variable-length online sequences efficiently across different deployment scenarios. To enhance temporal understanding, we introduce 4DMAP, a frame-wise masked auto-regressive pretraining strategy that captures motion cues across frames. Our extensive evaluations across 9 tasks on 7 datasets, demonstrating consistent improvements across diverse domains. We further demonstrate PointNet4D's utility by building two robotic application systems: 4D Diffusion Policy and 4D Imitation Learning, achieving substantial gains on the RoboTwin and HandoverSim benchmarks.",
    "authors": [
      "Yunze Liu",
      "Zifan Wang",
      "Peiran Wu",
      "Jiayang Ao"
    ],
    "published": "2025-12-01T07:58:01+00:00",
    "url": "https://arxiv.org/pdf/2512.01383v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01382v1",
    "title": "Reversible Inversion for Training-Free Exemplar-guided Image Editing",
    "abstract": "Exemplar-guided Image Editing (EIE) aims to modify a source image according to a visual reference. Existing approaches often require large-scale pre-training to learn relationships between the source and reference images, incurring high computational costs. As a training-free alternative, inversion techniques can be used to map the source image into a latent space for manipulation. However, our empirical study reveals that standard inversion is sub-optimal for EIE, leading to poor quality and inefficiency. To tackle this challenge, we introduce \\textbf{Reversible Inversion ({ReInversion})} for effective and efficient EIE. Specifically, ReInversion operates as a two-stage denoising process, which is first conditioned on the source image and subsequently on the reference. Besides, we introduce a Mask-Guided Selective Denoising (MSD) strategy to constrain edits to target regions, preserving the structural consistency of the background. Both qualitative and quantitative comparisons demonstrate that our ReInversion method achieves state-of-the-art EIE performance with the lowest computational overhead.",
    "authors": [
      "Yuke Li",
      "Lianli Gao",
      "Ji Zhang",
      "Pengpeng Zeng",
      "Lichuan Xiang",
      "Hongkai Wen",
      "Heng Tao Shen",
      "Jingkuan Song"
    ],
    "published": "2025-12-01T07:56:06+00:00",
    "url": "https://arxiv.org/pdf/2512.01382v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01380v1",
    "title": "Textured Geometry Evaluation: Perceptual 3D Textured Shape Metric via 3D Latent-Geometry Network",
    "abstract": "Textured high-fidelity 3D models are crucial for games, AR/VR, and film, but human-aligned evaluation methods still fall behind despite recent advances in 3D reconstruction and generation. Existing metrics, such as Chamfer Distance, often fail to align with how humans evaluate the fidelity of 3D shapes. Recent learning-based metrics attempt to improve this by relying on rendered images and 2D image quality metrics. However, these approaches face limitations due to incomplete structural coverage and sensitivity to viewpoint choices. Moreover, most methods are trained on synthetic distortions, which differ significantly from real-world distortions, resulting in a domain gap. To address these challenges, we propose a new fidelity evaluation method that is based directly on 3D meshes with texture, without relying on rendering. Our method, named Textured Geometry Evaluation TGE, jointly uses the geometry and color information to calculate the fidelity of the input textured mesh with comparison to a reference colored shape. To train and evaluate our metric, we design a human-annotated dataset with real-world distortions. Experiments show that TGE outperforms rendering-based and geometry-only methods on real-world distortion dataset.",
    "authors": [
      "Tianyu Luan",
      "Xuelu Feng",
      "Zixin Zhu",
      "Phani Nuney",
      "Sheng Liu",
      "Xuan Gong",
      "David Doermann",
      "Chunming Qiao",
      "Junsong Yuan"
    ],
    "published": "2025-12-01T07:53:03+00:00",
    "url": "https://arxiv.org/pdf/2512.01380v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01374v3",
    "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
    "abstract": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.",
    "authors": [
      "Chujie Zheng",
      "Kai Dang",
      "Bowen Yu",
      "Mingze Li",
      "Huiqiang Jiang",
      "Junrong Lin",
      "Yuqiong Liu",
      "Hao Lin",
      "Chencan Wu",
      "Feng Hu",
      "An Yang",
      "Jingren Zhou",
      "Junyang Lin"
    ],
    "published": "2025-12-01T07:45:39+00:00",
    "url": "https://arxiv.org/pdf/2512.01374v3",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01373v1",
    "title": "SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation",
    "abstract": "3D generation and reconstruction techniques have been widely used in computer games, film, and other content creation areas. As the application grows, there is a growing demand for 3D shapes that look truly realistic. Traditional evaluation methods rely on a ground truth to measure mesh fidelity. However, in many practical cases, a shape's realism does not depend on having a ground truth reference. In this work, we propose a Shape-Realism Alignment Metric that leverages a large language model (LLM) as a bridge between mesh shape information and realism evaluation. To achieve this, we adopt a mesh encoding approach that converts 3D shapes into the language token space. A dedicated realism decoder is designed to align the language model's output with human perception of realism. Additionally, we introduce a new dataset, RealismGrading, which provides human-annotated realism scores without the need for ground truth shapes. Our dataset includes shapes generated by 16 different algorithms on over a dozen objects, making it more representative of practical 3D shape distributions. We validate our metric's performance and generalizability through k-fold cross-validation across different objects. Experimental results show that our metric correlates well with human perceptions and outperforms existing methods, and has good generalizability.",
    "authors": [
      "Sheng Liu",
      "Tianyu Luan",
      "Phani Nuney",
      "Xuelu Feng",
      "Junsong Yuan"
    ],
    "published": "2025-12-01T07:40:11+00:00",
    "url": "https://arxiv.org/pdf/2512.01373v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01372v1",
    "title": "Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation",
    "abstract": "Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.",
    "authors": [
      "Wei Yang",
      "Rui Zhong",
      "Yiqun Chen",
      "Chi Lu",
      "Peng Jiang"
    ],
    "published": "2025-12-01T07:39:28+00:00",
    "url": "https://arxiv.org/pdf/2512.01372v1",
    "categories": [
      "cs.IR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01370v1",
    "title": "Beyond Loss Guidance: Using PDE Residuals as Spectral Attention in Diffusion Neural Operators",
    "abstract": "Diffusion-based solvers for partial differential equations (PDEs) are often bottle-necked by slow gradient-based test-time optimization routines that use PDE residuals for loss guidance. They additionally suffer from optimization instabilities and are unable to dynamically adapt their inference scheme in the presence of noisy PDE residuals. To address these limitations, we introduce PRISMA (PDE Residual Informed Spectral Modulation with Attention), a conditional diffusion neural operator that embeds PDE residuals directly into the model's architecture via attention mechanisms in the spectral domain, enabling gradient-descent free inference. In contrast to previous methods that use PDE loss solely as external optimization targets, PRISMA integrates PDE residuals as integral architectural features, making it inherently fast, robust, accurate, and free from sensitive hyperparameter tuning. We show that PRISMA has competitive accuracy, at substantially lower inference costs, compared to previous methods across five benchmark PDEs, especially with noisy observations, while using 10x to 100x fewer denoising steps, leading to 15x to 250x faster inference.",
    "authors": [
      "Medha Sawhney",
      "Abhilash Neog",
      "Mridul Khurana",
      "Anuj Karpatne"
    ],
    "published": "2025-12-01T07:34:42+00:00",
    "url": "https://arxiv.org/pdf/2512.01370v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.NA",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.01369v1",
    "title": "MARSAD: A Multi-Functional Tool for Real-Time Social Media Analysis",
    "abstract": "MARSAD is a multifunctional natural language processing (NLP) platform designed for real-time social media monitoring and analysis, with a particular focus on the Arabic-speaking world. It enables researchers and non-technical users alike to examine both live and archived social media content, producing detailed visualizations and reports across various dimensions, including sentiment analysis, emotion analysis, propaganda detection, fact-checking, and hate speech detection. The platform also provides secure data-scraping capabilities through API keys for accessing public social media data. MARSAD's backend architecture integrates flexible document storage with structured data management, ensuring efficient processing of large and multimodal datasets. Its user-friendly frontend supports seamless data upload and interaction.",
    "authors": [
      "Md. Rafiul Biswas",
      "Firoj Alam",
      "Wajdi Zaghouani"
    ],
    "published": "2025-12-01T07:31:37+00:00",
    "url": "https://arxiv.org/pdf/2512.01369v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01366v1",
    "title": "BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud",
    "abstract": "Failing to be aware of speeding vehicles approaching from behind poses a huge threat to the road safety of pedestrians and cyclists. In this paper, we propose BlinkBud, which utilizes a single earbud and a paired phone to online detect hazardous objects approaching from behind of a user. The core idea is to accurately track visually identified objects utilizing a small number of sampled camera images taken from the earbud. To minimize the power consumption of the earbud and the phone while guaranteeing the best tracking accuracy, a novel 3D object tracking algorithm is devised, integrating both a Kalman filter based trajectory estimation scheme and an optimal image sampling strategy based on reinforcement learning. Moreover, the impact of constant user head movements on the tracking accuracy is significantly eliminated by leveraging the estimated pitch and yaw angles to correct the object depth estimation and align the camera coordinate system to the user's body coordinate system, respectively. We implement a prototype BlinkBud system and conduct extensive real-world experiments. Results show that BlinkBud is lightweight with ultra-low mean power consumptions of 29.8 mW and 702.6 mW on the earbud and smartphone, respectively, and can accurately detect hazards with a low average false positive ratio (FPR) and false negative ratio (FNR) of 4.90% and 1.47%, respectively.",
    "authors": [
      "Yunzhe Li",
      "Jiajun Yan",
      "Yuzhou Wei",
      "Kechen Liu",
      "Yize Zhao",
      "Chong Zhang",
      "Hongzi Zhu",
      "Li Lu",
      "Shan Chang",
      "Minyi Guo"
    ],
    "published": "2025-12-01T07:25:17+00:00",
    "url": "https://arxiv.org/pdf/2512.01366v1",
    "categories": [
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01357v1",
    "title": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity",
    "abstract": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.",
    "authors": [
      "Wenbin Zhu",
      "Zhaoyan Shen",
      "Zili Shao",
      "Hongjun Dai",
      "Feng Chen"
    ],
    "published": "2025-12-01T07:10:34+00:00",
    "url": "https://arxiv.org/pdf/2512.01357v1",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR"
    ]
  },
  {
    "arxiv_id": "2512.01354v2",
    "title": "The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness",
    "abstract": "Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.   This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.   The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.   Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.",
    "authors": [
      "Zhongjie Jiang"
    ],
    "published": "2025-12-01T07:09:38+00:00",
    "url": "https://arxiv.org/pdf/2512.01354v2",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "q-fin.TR"
    ]
  },
  {
    "arxiv_id": "2512.01352v1",
    "title": "OpenBox: Annotate Any Bounding Boxes in 3D",
    "abstract": "Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects' physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via cross-modal instance alignment. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset, the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.",
    "authors": [
      "In-Jae Lee",
      "Mungyeom Kim",
      "Kwonyoung Ryu",
      "Pierre Musacchio",
      "Jaesik Park"
    ],
    "published": "2025-12-01T07:04:48+00:00",
    "url": "https://arxiv.org/pdf/2512.01352v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01351v1",
    "title": "Benchmarking Overton Pluralism in LLMs",
    "abstract": "We introduce a novel framework for measuring Overton pluralism in LLMs--the extent to which diverse viewpoints are represented in model outputs. We (i) formalize Overton pluralism as a set coverage metric (OvertonScore), (ii) conduct a large-scale U.S.-representative human study (N = 1209; 60 questions; 8 LLMs), and (iii) develop an automated benchmark that closely reproduces human judgments. On average, models achieve OvertonScores of 0.35--0.41, with DeepSeek V3 performing best; yet all models remain far below the theoretical maximum of 1.0, revealing substantial headroom for improvement. Because repeated large-scale human studies are costly and slow, scalable evaluation tools are essential for model development. Hence, we propose an automated benchmark that achieves high rank correlation with human judgments ($\u03c1=0.88$), providing a practical proxy without replacing human assessment. By turning pluralistic alignment from a normative aim into a measurable benchmark, our work establishes a foundation for systematic progress toward more pluralistic LLMs.",
    "authors": [
      "Elinor Poole-Dayan",
      "Jiayi Wu",
      "Taylor Sorensen",
      "Jiaxin Pei",
      "Michiel A. Bakker"
    ],
    "published": "2025-12-01T07:04:20+00:00",
    "url": "https://arxiv.org/pdf/2512.01351v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01348v1",
    "title": "Handwritten Text Recognition for Low Resource Languages",
    "abstract": "Despite considerable progress in handwritten text recognition, paragraph-level handwritten text recognition, especially in low-resource languages, such as Hindi, Urdu and similar scripts, remains a challenging problem. These languages, often lacking comprehensive linguistic resources, require special attention to develop robust systems for accurate optical character recognition (OCR). This paper introduces BharatOCR, a novel segmentation-free paragraph-level handwritten Hindi and Urdu text recognition. We propose a ViT-Transformer Decoder-LM architecture for handwritten text recognition, where a Vision Transformer (ViT) extracts visual features, a Transformer decoder generates text sequences, and a pre-trained language model (LM) refines the output to improve accuracy, fluency, and coherence. Our model utilizes a Data-efficient Image Transformer (DeiT) model proposed for masked image modeling in this research work. In addition, we adopt a RoBERTa architecture optimized for masked language modeling (MLM) to enhance the linguistic comprehension and generative capabilities of the proposed model. The transformer decoder generates text sequences from visual embeddings. This model is designed to iteratively process a paragraph image line by line, called implicit line segmentation. The proposed model was evaluated using our custom dataset ('Parimal Urdu') and ('Parimal Hindi'), introduced in this research work, as well as two public datasets. The proposed model achieved benchmark results in the NUST-UHWR, PUCIT-OUHL, and Parimal-Urdu datasets, achieving character recognition rates of 96.24%, 92.05%, and 94.80%, respectively. The model also provided benchmark results using the Hindi dataset achieving a character recognition rate of 80.64%. The results obtained from our proposed model indicated that it outperformed several state-of-the-art Urdu text recognition methods.",
    "authors": [
      "Sayantan Dey",
      "Alireza Alaei",
      "Partha Pratim Roy"
    ],
    "published": "2025-12-01T07:01:52+00:00",
    "url": "https://arxiv.org/pdf/2512.01348v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01343v2",
    "title": "Intrinsic Structure as a Proxy for Saliency: SVD-Based Weight Preservation for Mixed-Precision Quantization in Large Language Models",
    "abstract": "As Large Language Models (LLMs) continue to scale in parameter count, deploying them on commodity hardware has become increasingly challenging. Post-Training Quantization (PTQ) addresses this by reducing the precision of model weights, typically to 4-bit or lower. However, uniform quantization often leads to significant performance degradation due to the presence of ``outlier features'' -- weights that, while few in number, are critical for maintaining model accuracy. Current state-of-the-art methods such as AWQ (Activation-aware Weight Quantization) and SpQR (Sparse Quantization Representations) rely on calibration data to identify these salient weights via activation magnitudes or Hessian sensitivity. In scenarios where data privacy is paramount or calibration data is unavailable, these methods are inapplicable.   In this work, we propose a data-free, structure-aware hypothesis: that the weights identified as Principal Components via Singular Value Decomposition (SVD) are intrinsically important to the model's downstream performance. We introduce a novel selection heuristic that preserves the top-$k$ weights aligned with the principal components in FP32, while aggressively quantizing the residual weights. We compare our method against activation-aware (AWQ) and second-order (SpQR) methods across GLUE benchmarks (MRPC, RTE, QNLI) using a DistilBERT backbone. Our experiments reveal that structural importance is highly correlated with functional importance. On the challenging RTE task, our SVD-based method achieves an accuracy of 66.06\\%, outperforming both AWQ (65.34\\%) and SpQR (65.34\\%) at high protection budgets, validating that intrinsic matrix structure can serve as a robust proxy for weight saliency without the need for forward passes or calibration data.",
    "authors": [
      "Shashank Landge",
      "Abhishek Patil",
      "Tejas kamble",
      "Bhushan Buddhivant",
      "Priyanka Joshi"
    ],
    "published": "2025-12-01T06:58:30+00:00",
    "url": "https://arxiv.org/pdf/2512.01343v2",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01342v1",
    "title": "InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision",
    "abstract": "Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.",
    "authors": [
      "Chenting Wang",
      "Yuhan Zhu",
      "Yicheng Xu",
      "Jiange Yang",
      "Ziang Yan",
      "Yali Wang",
      "Yi Wang",
      "Limin Wang"
    ],
    "published": "2025-12-01T06:57:39+00:00",
    "url": "https://arxiv.org/pdf/2512.01342v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01340v1",
    "title": "EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans",
    "abstract": "Speech-driven Talking Human (TH) generation, commonly known as \"Talker,\" currently faces limitations in multi-subject driving capabilities. Extending this paradigm to \"Multi-Talker,\" capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.",
    "authors": [
      "Yingjie Zhou",
      "Xilei Zhu",
      "Siyu Ren",
      "Ziyi Zhao",
      "Ziwen Wang",
      "Farong Wen",
      "Yu Zhou",
      "Jiezhang Cao",
      "Xiongkuo Min",
      "Fengjiao Chen",
      "Xiaoyu Li",
      "Xuezhi Cao",
      "Guangtao Zhai",
      "Xiaohong Liu"
    ],
    "published": "2025-12-01T06:56:40+00:00",
    "url": "https://arxiv.org/pdf/2512.01340v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01335v1",
    "title": "EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations",
    "abstract": "Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as \"(@_@)\" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.",
    "authors": [
      "Xinyun Zhou",
      "Xinfeng Li",
      "Yinan Peng",
      "Ming Xu",
      "Xuanwang Zhang",
      "Miao Yu",
      "Yidong Wang",
      "Xiaojun Jia",
      "Kun Wang",
      "Qingsong Wen",
      "XiaoFeng Wang",
      "Wei Dong"
    ],
    "published": "2025-12-01T06:53:49+00:00",
    "url": "https://arxiv.org/pdf/2512.01335v1",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01334v1",
    "title": "AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation",
    "abstract": "Text-guided image-to-video (TI2V) generation has recently achieved remarkable progress, particularly in maintaining subject consistency and temporal coherence. However, existing methods still struggle to adhere to fine-grained prompt semantics, especially when prompts entail substantial transformations of the input image (e.g., object addition, deletion, or modification), a shortcoming we term semantic negligence. In a pilot study, we find that applying a Gaussian blur to the input image improves semantic adherence. Analyzing attention maps, we observe clearer foreground-background separation. From an energy perspective, this corresponds to a lower-entropy cross-attention distribution. Motivated by this, we introduce AlignVid, a training-free framework with two components: (i) Attention Scaling Modulation (ASM), which directly reweights attention via lightweight Q or K scaling, and (ii) Guidance Scheduling (GS), which applies ASM selectively across transformer blocks and denoising steps to reduce visual quality degradation. This minimal intervention improves prompt adherence while limiting aesthetic degradation. In addition, we introduce OmitI2V to evaluate semantic negligence in TI2V generation, comprising 367 human-annotated samples that span addition, deletion, and modification scenarios. Extensive experiments demonstrate that AlignVid can enhance semantic fidelity.",
    "authors": [
      "Yexin Liu",
      "Wen-Jie Shu",
      "Zile Huang",
      "Haoze Zheng",
      "Yueze Wang",
      "Manyuan Zhang",
      "Ser-Nam Lim",
      "Harry Yang"
    ],
    "published": "2025-12-01T06:53:48+00:00",
    "url": "https://arxiv.org/pdf/2512.01334v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01333v1",
    "title": "Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI",
    "abstract": "Stroke is a major cause of death and permanent impairment, making it a major worldwide health concern. For prompt intervention and successful preventative tactics, early risk assessment is essential. To address this challenge, we used ensemble modeling and explainable AI (XAI) techniques to create an interpretable machine learning framework for stroke risk prediction. A thorough evaluation of 10 different machine learning models using 5-fold cross-validation across several datasets was part of our all-inclusive strategy, which also included feature engineering and data pretreatment (using Random Over-Sampling (ROS) to solve class imbalance). Our optimized ensemble model (Random Forest + ExtraTrees + XGBoost) performed exceptionally well, obtaining a strong 99.09% accuracy on the Stroke Prediction Dataset (SPD). We improved the model's transparency and clinical applicability by identifying three important clinical variables using LIME-based interpretability analysis: age, hypertension, and glucose levels. Through early prediction, this study highlights how combining ensemble learning with explainable AI (XAI) can deliver highly accurate and interpretable stroke risk assessment. By enabling data-driven prevention and personalized clinical decisions, our framework has the potential to transform stroke prediction and cardiovascular risk management.",
    "authors": [
      "A S M Ahsanul Sarkar Akib",
      "Raduana Khawla",
      "Abdul Hasib"
    ],
    "published": "2025-12-01T06:53:00+00:00",
    "url": "https://arxiv.org/pdf/2512.01333v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01331v1",
    "title": "A Fast Heuristic Search Approach for Energy-Optimal Profile Routing for Electric Vehicles",
    "abstract": "We study the energy-optimal shortest path problem for electric vehicles (EVs) in large-scale road networks, where recuperated energy along downhill segments introduces negative energy costs. While traditional point-to-point pathfinding algorithms for EVs assume a known initial energy level, many real-world scenarios involving uncertainty in available energy require planning optimal paths for all possible initial energy levels, a task known as energy-optimal profile search. Existing solutions typically rely on specialized profile-merging procedures within a label-correcting framework that results in searching over complex profiles. In this paper, we propose a simple yet effective label-setting approach based on multi-objective A* search, which employs a novel profile dominance rule to avoid generating and handling complex profiles. We develop four variants of our method and evaluate them on real-world road networks enriched with realistic energy consumption data. Experimental results demonstrate that our energy profile A* search achieves performance comparable to energy-optimal A* with a known initial energy level.",
    "authors": [
      "Saman Ahmadi",
      "Mahdi Jalili"
    ],
    "published": "2025-12-01T06:45:34+00:00",
    "url": "https://arxiv.org/pdf/2512.01331v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01329v1",
    "title": "TagSplat: Topology-Aware Gaussian Splatting for Dynamic Mesh Modeling and Tracking",
    "abstract": "Topology-consistent dynamic model sequences are essential for applications such as animation and model editing. However, existing 4D reconstruction methods face challenges in generating high-quality topology-consistent meshes. To address this, we propose a topology-aware dynamic reconstruction framework based on Gaussian Splatting. We introduce a Gaussian topological structure that explicitly encodes spatial connectivity. This structure enables topology-aware densification and pruning, preserving the manifold consistency of the Gaussian representation. Temporal regularization terms further ensure topological coherence over time, while differentiable mesh rasterization improves mesh quality. Experimental results demonstrate that our method reconstructs topology-consistent mesh sequences with significantly higher accuracy than existing approaches. Moreover, the resulting meshes enable precise 3D keypoint tracking. Project page: https://haza628.github.io/tagSplat/",
    "authors": [
      "Hanzhi Guo",
      "Dongdong Weng",
      "Mo Su",
      "Yixiao Chen",
      "Xiaonuo Dongye",
      "Chenyu Xu"
    ],
    "published": "2025-12-01T06:41:54+00:00",
    "url": "https://arxiv.org/pdf/2512.01329v1",
    "categories": [
      "cs.GR",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01326v1",
    "title": "Securing Large Language Models (LLMs) from Prompt Injection Attacks",
    "abstract": "Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.",
    "authors": [
      "Omar Farooq Khan Suri",
      "John McCrae"
    ],
    "published": "2025-12-01T06:34:20+00:00",
    "url": "https://arxiv.org/pdf/2512.01326v1",
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01324v1",
    "title": "Panda: Self-distillation of Reusable Sensor-level Representations for High Energy Physics",
    "abstract": "Liquid argon time projection chambers (LArTPCs) provide dense, high-fidelity 3D measurements of particle interactions and underpin current and future neutrino and rare-event experiments. Physics reconstruction typically relies on complex detector-specific pipelines that use tens of hand-engineered pattern recognition algorithms or cascades of task-specific neural networks that require extensive, labeled simulation that requires a careful, time-consuming calibration process. We introduce \\textbf{Panda}, a model that learns reusable sensor-level representations directly from raw unlabeled LArTPC data. Panda couples a hierarchical sparse 3D encoder with a multi-view, prototype-based self-distillation objective. On a simulated dataset, Panda substantially improves label efficiency and reconstruction quality, beating the previous state-of-the-art semantic segmentation model with 1,000$\\times$ fewer labels. We also show that a single set-prediction head 1/20th the size of the backbone with no physical priors trained on frozen outputs from Panda can result in particle identification that is comparable with state-of-the-art (SOTA) reconstruction tools. Full fine-tuning further improves performance across all tasks.",
    "authors": [
      "Samuel Young",
      "Kazuhiro Terao"
    ],
    "published": "2025-12-01T06:28:11+00:00",
    "url": "https://arxiv.org/pdf/2512.01324v1",
    "categories": [
      "hep-ex",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01321v1",
    "title": "Extending NGU to Multi-Agent RL: A Preliminary Study",
    "abstract": "The Never Give Up (NGU) algorithm has proven effective in reinforcement learning tasks with sparse rewards by combining episodic novelty and intrinsic motivation. In this work, we extend NGU to multi-agent environments and evaluate its performance in the simple_tag environment from the PettingZoo suite. Compared to a multi-agent DQN baseline, NGU achieves moderately higher returns and more stable learning dynamics. We investigate three design choices: (1) shared replay buffer versus individual replay buffers, (2) sharing episodic novelty among agents using different k thresholds, and (3) using heterogeneous values of the beta parameter. Our results show that NGU with a shared replay buffer yields the best performance and stability, highlighting that the gains come from combining NGU intrinsic exploration with experience sharing. Novelty sharing performs comparably when k = 1 but degrades learning for larger values. Finally, heterogeneous beta values do not improve over a small common value. These findings suggest that NGU can be effectively applied in multi-agent settings when experiences are shared and intrinsic exploration signals are carefully tuned.",
    "authors": [
      "Juan Hernandez",
      "Diego Fern\u00e1ndez",
      "Manuel Cifuentes",
      "Denis Parra",
      "Rodrigo Toro Icarte"
    ],
    "published": "2025-12-01T06:24:37+00:00",
    "url": "https://arxiv.org/pdf/2512.01321v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01319v1",
    "title": "Rethinking Intracranial Aneurysm Vessel Segmentation: A Perspective from Computational Fluid Dynamics Applications",
    "abstract": "The precise segmentation of intracranial aneurysms and their parent vessels (IA-Vessel) is a critical step for hemodynamic analyses, which mainly depends on computational fluid dynamics (CFD). However, current segmentation methods predominantly focus on image-based evaluation metrics, often neglecting their practical effectiveness in subsequent CFD applications. To address this deficiency, we present the Intracranial Aneurysm Vessel Segmentation (IAVS) dataset, the first comprehensive, multi-center collection comprising 641 3D MRA images with 587 annotations of aneurysms and IA-Vessels. In addition to image-mask pairs, IAVS dataset includes detailed hemodynamic analysis outcomes, addressing the limitations of existing datasets that neglect topological integrity and CFD applicability. To facilitate the development and evaluation of clinically relevant techniques, we construct two evaluation benchmarks including global localization of aneurysms (Stage I) and fine-grained segmentation of IA-Vessel (Stage II) and develop a simple and effective two-stage framework, which can be used as a out-of-the-box method and strong baseline. For comprehensive evaluation of applicability of segmentation results, we establish a standardized CFD applicability evaluation system that enables the automated and consistent conversion of segmentation masks into CFD models, offering an applicability-focused assessment of segmentation outcomes. The dataset, code, and model will be public available at https://github.com/AbsoluteResonance/IAVS.",
    "authors": [
      "Feiyang Xiao",
      "Yichi Zhang",
      "Xigui Li",
      "Yuanye Zhou",
      "Chen Jiang",
      "Xin Guo",
      "Limei Han",
      "Yuxin Li",
      "Fengping Zhu",
      "Yuan Cheng"
    ],
    "published": "2025-12-01T06:23:07+00:00",
    "url": "https://arxiv.org/pdf/2512.01319v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01317v1",
    "title": "Data-Driven Learnability Transition of Measurement-Induced Entanglement",
    "abstract": "Measurement-induced entanglement (MIE) captures how local measurements generate long-range quantum correlations and drive dynamical phase transitions in many-body systems. Yet estimating MIE experimentally remains challenging: direct evaluation requires extensive post-selection over measurement outcomes, raising the question of whether MIE is accessible with only polynomial resources. We address this challenge by reframing MIE detection as a data-driven learning problem that assumes no prior knowledge of state preparation. Using measurement records alone, we train a neural network in a self-supervised manner to predict the uncertainty metric for MIE--the gap between upper and lower bounds of the average post-measurement bipartite entanglement. Applied to random circuits with one-dimensional all-to-all connectivity and two-dimensional nearest-neighbor coupling, our method reveals a learnability transition with increasing circuit depth: below a threshold, the uncertainty is small and decreases with polynomial measurement data and model parameters, while above it the uncertainty remains large despite increasing resources. We further verify this transition experimentally on current noisy quantum devices, demonstrating its robustness to realistic noise. These results highlight the power of data-driven approaches for learning MIE and delineate the practical limits of its classical learnability.",
    "authors": [
      "Dongheng Qian",
      "Jing Wang"
    ],
    "published": "2025-12-01T06:18:08+00:00",
    "url": "https://arxiv.org/pdf/2512.01317v1",
    "categories": [
      "quant-ph",
      "cond-mat.dis-nn",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01316v1",
    "title": "Agreement-Constrained Probabilistic Minimum Bayes Risk Decoding",
    "abstract": "Minimum Bayes risk (MBR) decoding generates high-quality translations by maximizing the expected utility of output candidates, but it evaluates all pairwise scores over the candidate set; hence, it takes quadratic time with respect to the number of candidates. To reduce the number of utility function calls, probabilistic MBR (PMBR) decoding partially evaluates quality scores using sampled pairs of candidates and completes the missing scores with a matrix completion algorithm. Nevertheless, it degrades the translation quality as the number of utility function calls is reduced. Therefore, to improve the trade-off between quality and cost, we propose agreement-constrained PMBR (AC-PMBR) decoding, which leverages a knowledge distilled model to guide the completion of the score matrix. Our AC-PMBR decoding improved approximation errors of matrix completion by up to 3 times and achieved higher translation quality compared with PMBR decoding at a comparable computational cost on the WMT'23 En$\\leftrightarrow$De translation tasks.",
    "authors": [
      "Koki Natsumi",
      "Hiroyuki Deguchi",
      "Yusuke Sakai",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ],
    "published": "2025-12-01T06:16:47+00:00",
    "url": "https://arxiv.org/pdf/2512.01316v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01315v1",
    "title": "FOD-S2R: A FOD Dataset for Sim2Real Transfer Learning based Object Detection",
    "abstract": "Foreign Object Debris (FOD) within aircraft fuel tanks presents critical safety hazards including fuel contamination, system malfunctions, and increased maintenance costs. Despite the severity of these risks, there is a notable lack of dedicated datasets for the complex, enclosed environments found inside fuel tanks. To bridge this gap, we present a novel dataset, FOD-S2R, composed of real and synthetic images of the FOD within a simulated aircraft fuel tank. Unlike existing datasets that focus on external or open-air environments, our dataset is the first to systematically evaluate the effectiveness of synthetic data in enhancing the real-world FOD detection performance in confined, closed structures. The real-world subset consists of 3,114 high-resolution HD images captured in a controlled fuel tank replica, while the synthetic subset includes 3,137 images generated using Unreal Engine. The dataset is composed of various Field of views (FOV), object distances, lighting conditions, color, and object size. Prior research has demonstrated that synthetic data can reduce reliance on extensive real-world annotations and improve the generalizability of vision models. Thus, we benchmark several state-of-the-art object detection models and demonstrate that introducing synthetic data improves the detection accuracy and generalization to real-world conditions. These experiments demonstrate the effectiveness of synthetic data in enhancing the model performance and narrowing the Sim2Real gap, providing a valuable foundation for developing automated FOD detection systems for aviation maintenance.",
    "authors": [
      "Ashish Vashist",
      "Qiranul Saadiyean",
      "Suresh Sundaram",
      "Chandra Sekhar Seelamantula"
    ],
    "published": "2025-12-01T06:16:26+00:00",
    "url": "https://arxiv.org/pdf/2512.01315v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01314v1",
    "title": "TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance",
    "abstract": "In the digital economy era, digital watermarking serves as a critical basis for ownership proof of massive replicable content, including AI-generated and other virtual assets. Designing robust watermarks capable of withstanding various attacks and processing operations is even more paramount. We introduce TokenPure, a novel Diffusion Transformer-based framework designed for effective and consistent watermark removal. TokenPure solves the trade-off between thorough watermark destruction and content consistency by leveraging token-based conditional reconstruction. It reframes the task as conditional generation, entirely bypassing the initial watermark-carrying noise. We achieve this by decomposing the watermarked image into two complementary token sets: visual tokens for texture and structural tokens for geometry. These tokens jointly condition the diffusion process, enabling the framework to synthesize watermark-free images with fine-grained consistency and structural integrity. Comprehensive experiments show that TokenPure achieves state-of-the-art watermark removal and reconstruction fidelity, substantially outperforming existing baselines in both perceptual quality and consistency.",
    "authors": [
      "Pei Yang",
      "Yepeng Liu",
      "Kelly Peng",
      "Yuan Gao",
      "Yiren Song"
    ],
    "published": "2025-12-01T06:15:51+00:00",
    "url": "https://arxiv.org/pdf/2512.01314v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01312v1",
    "title": "IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval",
    "abstract": "In recent years, significant developments have been made in both video retrieval and video moment retrieval tasks, which respectively retrieve complete videos or moments for a given text query. These advancements have greatly improved user satisfaction during the search process. However, previous work has failed to establish meaningful \"interaction\" between the retrieval system and the user, and its one-way retrieval paradigm can no longer fully meet the personalization and dynamic needs of at least 80.8\\% of users. In this paper, we introduce the Interactive Video Corpus Retrieval (IVCR) task, a more realistic setting that enables multi-turn, conversational, and realistic interactions between the user and the retrieval system. To facilitate research on this challenging task, we introduce IVCR-200K, a high-quality, bilingual, multi-turn, conversational, and abstract semantic dataset that supports video retrieval and even moment retrieval. Furthermore, we propose a comprehensive framework based on multi-modal large language models (MLLMs) to help users interact in several modes with more explainable solutions. The extensive experiments demonstrate the effectiveness of our dataset and framework.",
    "authors": [
      "Ning Han",
      "Yawen Zeng",
      "Shaohua Long",
      "Chengqing Li",
      "Sijie Yang",
      "Dun Tan",
      "Jianfeng Dong",
      "Jingjing Chen"
    ],
    "published": "2025-12-01T06:12:59+00:00",
    "url": "https://arxiv.org/pdf/2512.01312v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01311v2",
    "title": "CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL",
    "abstract": "Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/tree/main/research/CuES.",
    "authors": [
      "Shinji Mai",
      "Yunpeng Zhai",
      "Ziqian Chen",
      "Cheng Chen",
      "Anni Zou",
      "Shuchang Tao",
      "Zhaoyang Liu",
      "Bolin Ding"
    ],
    "published": "2025-12-01T06:11:37+00:00",
    "url": "https://arxiv.org/pdf/2512.01311v2",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01310v1",
    "title": "Lost in Distortion: Uncovering the Domain Gap Between Computer Vision and Brain Imaging - A Study on Pretraining for Age Prediction",
    "abstract": "Large-scale brain imaging datasets provide unprecedented opportunities for developing domain foundation models through pretraining. However, unlike natural image datasets in computer vision, these neuroimaging data often exhibit high heterogeneity in quality, ranging from well-structured scans to severely distorted or incomplete brain volumes. This raises a fundamental question: can noise or low-quality scans contribute meaningfully to pretraining, or do they instead hinder model learning? In this study, we systematically explore the role of data quality level in pretraining and its impact on downstream tasks. Specifically, we perform pretraining on datasets with different quality levels and perform fine-tuning for brain age prediction on external cohorts. Our results show significant performance differences across quality levels, revealing both opportunities and limitations. We further discuss the gap between computer vision practices and clinical neuroimaging standards, emphasizing the necessity of domain-aware curation to ensure trusted and generalizable domain-specific foundation models.",
    "authors": [
      "Yanteng Zhang",
      "Songheng Li",
      "Zeyu Shen",
      "Qizhen Lan",
      "Lipei Zhang",
      "Yang Liu",
      "Vince Calhoun"
    ],
    "published": "2025-12-01T06:11:31+00:00",
    "url": "https://arxiv.org/pdf/2512.01310v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01306v1",
    "title": "Gaussian Swaying: Surface-Based Framework for Aerodynamic Simulation with 3D Gaussians",
    "abstract": "Branches swaying in the breeze, flags rippling in the wind, and boats rocking on the water all show how aerodynamics shape natural motion -- an effect crucial for realism in vision and graphics. In this paper, we present Gaussian Swaying, a surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike mesh-based methods that require costly meshing, or particle-based approaches that rely on discrete positional data, Gaussian Swaying models surfaces continuously with 3D Gaussians, enabling efficient and fine-grained aerodynamic interaction. Our framework unifies simulation and rendering on the same representation: Gaussian patches, which support force computation for dynamics while simultaneously providing normals for lightweight shading. Comprehensive experiments on both synthetic and real-world datasets across multiple metrics demonstrate that Gaussian Swaying achieves state-of-the-art performance and efficiency, offering a scalable approach for realistic aerodynamic scene simulation.",
    "authors": [
      "Hongru Yan",
      "Xiang Zhang",
      "Zeyuan Chen",
      "Fangyin Wei",
      "Zhuowen Tu"
    ],
    "published": "2025-12-01T06:03:47+00:00",
    "url": "https://arxiv.org/pdf/2512.01306v1",
    "categories": [
      "cs.CV",
      "cs.GR"
    ]
  },
  {
    "arxiv_id": "2512.01302v1",
    "title": "DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy",
    "abstract": "Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.",
    "authors": [
      "Jaewoo Song",
      "Jooyoung Choi",
      "Kanghyun Baek",
      "Sangyub Lee",
      "Daemin Park",
      "Sungroh Yoon"
    ],
    "published": "2025-12-01T05:52:55+00:00",
    "url": "https://arxiv.org/pdf/2512.01302v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01300v1",
    "title": "RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving",
    "abstract": "Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.",
    "authors": [
      "Dacheng Liao",
      "Mengshi Qi",
      "Peng Shu",
      "Zhining Zhang",
      "Yuxin Lin",
      "Liang Liu",
      "Huadong Ma"
    ],
    "published": "2025-12-01T05:44:06+00:00",
    "url": "https://arxiv.org/pdf/2512.01300v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01298v1",
    "title": "TBT-Former: Learning Temporal Boundary Distributions for Action Localization",
    "abstract": "Temporal Action Localization (TAL) remains a fundamental challenge in video understanding, aiming to identify the start time, end time, and category of all action instances within untrimmed videos. While recent single-stage, anchor-free models like ActionFormer have set a high standard by leveraging Transformers for temporal reasoning, they often struggle with two persistent issues: the precise localization of actions with ambiguous or \"fuzzy\" temporal boundaries and the effective fusion of multi-scale contextual information. In this paper, we introduce the Temporal Boundary Transformer (TBT-Former), a new architecture that directly addresses these limitations. TBT-Former enhances the strong ActionFormer baseline with three core contributions: (1) a higher-capacity scaled Transformer backbone with an increased number of attention heads and an expanded Multi-Layer Perceptron (MLP) dimension for more powerful temporal feature extraction; (2) a cross-scale feature pyramid network (FPN) that integrates a top-down pathway with lateral connections, enabling richer fusion of high-level semantics and low-level temporal details; and (3) a novel boundary distribution regression head. Inspired by the principles of Generalized Focal Loss (GFL), this new head recasts the challenging task of boundary regression as a more flexible probability distribution learning problem, allowing the model to explicitly represent and reason about boundary uncertainty. Within the paradigm of Transformer-based architectures, TBT-Former advances the formidable benchmark set by its predecessors, establishing a new level of performance on the highly competitive THUMOS14 and EPIC-Kitchens 100 datasets, while remaining competitive on the large-scale ActivityNet-1.3. Our code is available at https://github.com/aaivu/In21-S7-CS4681-AML-Research-Projects/tree/main/projects/210536K-Multi-Modal-Learning_Video-Understanding",
    "authors": [
      "Thisara Rathnayaka",
      "Uthayasanker Thayasivam"
    ],
    "published": "2025-12-01T05:38:13+00:00",
    "url": "https://arxiv.org/pdf/2512.01298v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01296v1",
    "title": "EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly",
    "abstract": "Real-time 3D reconstruction is a fundamental task in computer graphics. Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality. To address these challenges, we propose a novel real-time system EGG-Fusion, featuring robust sparse-to-dense camera tracking and a geometry-aware Gaussian surfel mapping module, introducing an information filter-based fusion method that explicitly accounts for sensor noise to achieve high-precision surface reconstruction. The proposed differentiable Gaussian surfel mapping effectively models multi-view consistent surfaces while enabling efficient parameter optimization. Extensive experimental results demonstrate that the proposed system achieves a surface reconstruction error of 0.6\\textit{cm} on standardized benchmark datasets including Replica and ScanNet++, representing over 20\\% improvement in accuracy compared to state-of-the-art (SOTA) GS-based methods. Notably, the system maintains real-time processing capabilities at 24 FPS, establishing it as one of the most accurate differentiable-rendering-based real-time reconstruction systems. Project Page: https://zju3dv.github.io/eggfusion/",
    "authors": [
      "Xiaokun Pan",
      "Zhenzhe Li",
      "Zhichao Ye",
      "Hongjia Zhai",
      "Guofeng Zhang"
    ],
    "published": "2025-12-01T05:32:17+00:00",
    "url": "https://arxiv.org/pdf/2512.01296v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01292v2",
    "title": "Diffusion Model in Latent Space for Medical Image Segmentation Task",
    "abstract": "Medical image segmentation is crucial for clinical diagnosis and treatment planning. Traditional methods typically produce a single segmentation mask, failing to capture inherent uncertainty. Recent generative models enable the creation of multiple plausible masks per image, mimicking the collaborative interpretation of several clinicians. However, these approaches remain computationally heavy. We propose MedSegLatDiff, a diffusion based framework that combines a variational autoencoder (VAE) with a latent diffusion model for efficient medical image segmentation. The VAE compresses the input into a low dimensional latent space, reducing noise and accelerating training, while the diffusion process operates directly in this compact representation. We further replace the conventional MSE loss with weighted cross entropy in the VAE mask reconstruction path to better preserve tiny structures such as small nodules. MedSegLatDiff is evaluated on ISIC-2018 (skin lesions), CVC-Clinic (polyps), and LIDC-IDRI (lung nodules). It achieves state of the art or highly competitive Dice and IoU scores while simultaneously generating diverse segmentation hypotheses and confidence maps. This provides enhanced interpretability and reliability compared to deterministic baselines, making the model particularly suitable for clinical deployment.",
    "authors": [
      "Huynh Trinh Ngoc",
      "Toan Nguyen Hai",
      "Ba Luong Son",
      "Long Tran Quoc"
    ],
    "published": "2025-12-01T05:26:43+00:00",
    "url": "https://arxiv.org/pdf/2512.01292v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01291v1",
    "title": "Supervised Contrastive Machine Unlearning of Background Bias in Sonar Image Classification with Fine-Grained Explainable AI",
    "abstract": "Acoustic sonar image analysis plays a critical role in object detection and classification, with applications in both civilian and defense domains. Despite the availability of real and synthetic datasets, existing AI models that achieve high accuracy often over-rely on seafloor features, leading to poor generalization. To mitigate this issue, we propose a novel framework that integrates two key modules: (i) a Targeted Contrastive Unlearning (TCU) module, which extends the traditional triplet loss to reduce seafloor-induced background bias and improve generalization, and (ii) the Unlearn to Explain Sonar Framework (UESF), which provides visual insights into what the model has deliberately forgotten while adapting the LIME explainer to generate more faithful and localized attributions for unlearning evaluation. Extensive experiments across both real and synthetic sonar datasets validate our approach, demonstrating significant improvements in unlearning effectiveness, model robustness, and interpretability.",
    "authors": [
      "Kamal Basha S",
      "Athira Nambiar"
    ],
    "published": "2025-12-01T05:25:34+00:00",
    "url": "https://arxiv.org/pdf/2512.01291v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01289v1",
    "title": "OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph Construction",
    "abstract": "Environmental, Social, and Governance (ESG) disclosure frameworks such as SASB, TCFD, and IFRS S2 require organizations to compute and report numerous metrics for compliance, yet these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates. We present OntoMetric, an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs. OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels (VR001-VR006). The framework preserves both segment-level and page-level provenance for audit traceability. Evaluated on five ESG standards (SASB Commercial Banks, SASB Semiconductors, TCFD, IFRS S2, AASB S2) totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity. Our results demonstrate that combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools.",
    "authors": [
      "Mingqin Yu",
      "Fethi Rabhi",
      "Boming Xia",
      "Zhengyi Yang",
      "Felix Tan",
      "Qinghua Lu"
    ],
    "published": "2025-12-01T05:21:22+00:00",
    "url": "https://arxiv.org/pdf/2512.01289v1",
    "categories": [
      "cs.AI",
      "cs.GR"
    ]
  },
  {
    "arxiv_id": "2512.01286v1",
    "title": "Generative Modeling with Continuous Flows: Sample Complexity of Flow Matching",
    "abstract": "Flow matching has recently emerged as a promising alternative to diffusion-based generative models, offering faster sampling and simpler training by learning continuous flows governed by ordinary differential equations. Despite growing empirical success, the theoretical understanding of flow matching remains limited, particularly in terms of sample complexity results. In this work, we provide the first analysis of the sample complexity for flow-matching based generative models without assuming access to the empirical risk minimizer (ERM) of the loss function for estimating the velocity field. Under standard assumptions on the loss function for velocity field estimation and boundedness of the data distribution, we show that a sufficiently expressive neural network can learn a velocity field such that with $\\mathcal{O}(\u03b5^{-4})$ samples, such that the Wasserstein-2 distance between the learned and the true distribution is less than $\\mathcal{O}(\u03b5)$. The key technical idea is to decompose the velocity field estimation error into neural-network approximation error, statistical error due to the finite sample size, and optimization error due to the finite number of optimization steps for estimating the velocity field. Each of these terms are then handled via techniques that may be of independent interest.",
    "authors": [
      "Mudit Gaur",
      "Prashant Trivedi",
      "Shuchin Aeron",
      "Amrit Singh Bedi",
      "George K. Atia",
      "Vaneet Aggarwal"
    ],
    "published": "2025-12-01T05:14:25+00:00",
    "url": "https://arxiv.org/pdf/2512.01286v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01282v2",
    "title": "Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning",
    "abstract": "As web platforms evolve towards greater personalization and emotional complexity, conversational agents must transcend superficial empathy to demonstrate identity-aware emotional reasoning. However, existing systems face two limitations: (1) reliance on situation-centric datasets lacking persistent user identity, which hampers the capture of personalized affective nuances; and (2) dependence on opaque, coarse reward signals that hinder development of verifiable empathetic reasoning. To address these gaps, we introduce KardiaBench, a large-scale user-grounded benchmark comprising 178,080 QA pairs across 22,080 multi-turn conversations anchored to 671 real-world profiles. The dataset is constructed via a model-in-the-loop pipeline with iterative rubric-guided refinement to ensure psychological plausibility and persona consistency. This progressive empathy pipeline that integrates user comprehension, contextual reasoning, and emotion perception into conversations, followed by iterative critique and rubric-based refinement to ensure psychological plausibility, emotional fidelity, and persona consistency. Building on this, we propose Kardia-R1, a framework that trains models for interpretable, stepwise empathetic cognition. Kardia-R1 leverages Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL), a GRPO-based method that uses explainable, human-aligned rubric rewards to tightly couple user understanding, emotional inference, and supportive response generation. Extensive experiments across four LLM backbones demonstrate that Kardia-R1 consistently outperforms othet methods in emotion accuracy, empathy, relevance, persona consistency, and safety. Our dataset and model will be released at https://github.com/JhCircle/Kardia-R1.",
    "authors": [
      "Jiahao Yuan",
      "Zhiqing Cui",
      "Hanqing Wang",
      "Yuansheng Gao",
      "Yucheng Zhou",
      "Usman Naseem"
    ],
    "published": "2025-12-01T04:54:03+00:00",
    "url": "https://arxiv.org/pdf/2512.01282v2",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01278v1",
    "title": "Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding",
    "abstract": "Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.   To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup.",
    "authors": [
      "Yilong Zhao",
      "Jiaming Tang",
      "Kan Zhu",
      "Zihao Ye",
      "Chi-Chih Chang",
      "Chaofan Lin",
      "Jongseok Park",
      "Guangxuan Xiao",
      "Mohamed S. Abdelfattah",
      "Mingyu Gao",
      "Baris Kasikci",
      "Song Han",
      "Ion Stoica"
    ],
    "published": "2025-12-01T04:50:55+00:00",
    "url": "https://arxiv.org/pdf/2512.01278v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01274v1",
    "title": "SUPERChem: A Multimodal Reasoning Benchmark in Chemistry",
    "abstract": "Current benchmarks for evaluating the chemical reasoning capabilities of Large Language Models (LLMs) are limited by oversimplified tasks, lack of process-level evaluation, and misalignment with expert-level chemistry skills. To address these issues, we introduce SUPERChem, a benchmark of 500 expert-curated reasoning-intensive chemistry problems, covering diverse subfields and provided in both multimodal and text-only formats. Original content and an iterative curation pipeline eliminate flawed items and mitigate data contamination. Each problem is paired with an expert-authored solution path, enabling Reasoning Path Fidelity (RPF) scoring to evaluate reasoning quality beyond final-answer accuracy. Evaluations against a human baseline of 40.3% accuracy show that even the best-performing model, GPT-5 (High), reaches only 38.5%, followed closely by Gemini 2.5 Pro (37.9%) and DeepSeek-V3.1-Think (37.3%). SUPERChem elicits multi-step, multimodal reasoning, reveals model-dependent effects of visual information, and distinguishes high-fidelity reasoners from heuristic ones. By providing a challenging benchmark and a reliable evaluation framework, SUPERChem aims to facilitate the advancement of LLMs toward expert-level chemical intelligence. The dataset of the benchmark is available at https://huggingface.co/datasets/ZehuaZhao/SUPERChem.",
    "authors": [
      "Zehua Zhao",
      "Zhixian Huang",
      "Junren Li",
      "Siyu Lin",
      "Junting Zhou",
      "Fengqi Cao",
      "Kun Zhou",
      "Rui Ge",
      "Tingting Long",
      "Yuexiang Zhu",
      "Yan Liu",
      "Jie Zheng",
      "Junnian Wei",
      "Rong Zhu",
      "Peng Zou",
      "Wenyu Li",
      "Zekai Cheng",
      "Tian Ding",
      "Yaxuan Wang",
      "Yizhao Yan",
      "Tingru Wei",
      "Haowei Ming",
      "Weijie Mao",
      "Chen Sun",
      "Yiming Liu",
      "Zichen Wang",
      "Zuo Zhang",
      "Tong Yang",
      "Hao Ma",
      "Zhen Gao",
      "Jian Pei"
    ],
    "published": "2025-12-01T04:46:35+00:00",
    "url": "https://arxiv.org/pdf/2512.01274v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01273v1",
    "title": "nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis",
    "abstract": "Retinal imaging is a critical, non-invasive modality for the early detection and monitoring of ocular and systemic diseases. Deep learning, particularly convolutional neural networks (CNNs), has significant progress in automated retinal analysis, supporting tasks such as fundus image classification, lesion detection, and vessel segmentation. As a representative lightweight network, nnMobileNet has demonstrated strong performance across multiple retinal benchmarks while remaining computationally efficient. However, purely convolutional architectures inherently struggle to capture long-range dependencies and model the irregular lesions and elongated vascular patterns that characterize on retinal images, despite the critical importance of vascular features for reliable clinical diagnosis. To further advance this line of work and extend the original vision of nnMobileNet, we propose nnMobileNet++, a hybrid architecture that progressively bridges convolutional and transformer representations. The framework integrates three key components: (i) dynamic snake convolution for boundary-aware feature extraction, (ii) stage-specific transformer blocks introduced after the second down-sampling stage for global context modeling, and (iii) retinal image pretraining to improve generalization. Experiments on multiple public retinal datasets for classification, together with ablation studies, demonstrate that nnMobileNet++ achieves state-of-the-art or highly competitive accuracy while maintaining low computational cost, underscoring its potential as a lightweight yet effective framework for retinal image analysis.",
    "authors": [
      "Xin Li",
      "Wenhui Zhu",
      "Xuanzhao Dong",
      "Hao Wang",
      "Yujian Xiong",
      "Oana Dumitrascu",
      "Yalin Wang"
    ],
    "published": "2025-12-01T04:45:39+00:00",
    "url": "https://arxiv.org/pdf/2512.01273v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01268v2",
    "title": "ViscNet: Vision-Based In-line Viscometry for Fluid Mixing Process",
    "abstract": "Viscosity measurement is essential for process monitoring and autonomous laboratory operation, yet conventional viscometers remain invasive and require controlled laboratory environments that differ substantially from real process conditions. We present a computer-vision-based viscometer that infers viscosity by exploiting how a fixed background pattern becomes optically distorted as light refracts through the mixing-driven, continuously deforming free surface. Under diverse lighting conditions, the system achieves a mean absolute error of 0.113 in log m2 s^-1 units for regression and reaches up to 81% accuracy in viscosity-class prediction. Although performance declines for classes with closely clustered viscosity values, a multi-pattern strategy improves robustness by providing enriched visual cues. To ensure sensor reliability, we incorporate uncertainty quantification, enabling viscosity predictions with confidence estimates. This stand-off viscometer offers a practical, automation-ready alternative to existing viscometry methods.",
    "authors": [
      "Jongwon Sohn",
      "Juhyeon Moon",
      "Hyunjoon Jung",
      "Jaewook Nam"
    ],
    "published": "2025-12-01T04:21:33+00:00",
    "url": "https://arxiv.org/pdf/2512.01268v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01262v1",
    "title": "Social Media Data Mining of Human Behaviour during Bushfire Evacuation",
    "abstract": "Traditional data sources on bushfire evacuation behaviour, such as quantitative surveys and manual observations have severe limitations. Mining social media data related to bushfire evacuations promises to close this gap by allowing the collection and processing of a large amount of behavioural data, which are low-cost, accurate, possibly including location information and rich contextual information. However, social media data have many limitations, such as being scattered, incomplete, informal, etc. Together, these limitations represent several challenges to their usefulness to better understand bushfire evacuation. To overcome these challenges and provide guidance on which and how social media data can be used, this scoping review of the literature reports on recent advances in relevant data mining techniques. In addition, future applications and open problems are discussed. We envision future applications such as evacuation model calibration and validation, emergency communication, personalised evacuation training, and resource allocation for evacuation preparedness. We identify open problems such as data quality, bias and representativeness, geolocation accuracy, contextual understanding, crisis-specific lexicon and semantics, and multimodal data interpretation.",
    "authors": [
      "Junfeng Wu",
      "Xiangmin Zhou",
      "Erica Kuligowski",
      "Dhirendra Singh",
      "Enrico Ronchi",
      "Max Kinateder"
    ],
    "published": "2025-12-01T04:13:29+00:00",
    "url": "https://arxiv.org/pdf/2512.01262v1",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01256v1",
    "title": "Sentiment Analysis and Emotion Classification using Machine Learning Techniques for Nagamese Language - A Low-resource Language",
    "abstract": "The Nagamese language, a.k.a Naga Pidgin, is an Assamese-lexified creole language developed primarily as a means of communication in trade between the people from Nagaland and people from Assam in the north-east India. Substantial amount of work in sentiment analysis has been done for resource-rich languages like English, Hindi, etc. However, no work has been done in Nagamese language. To the best of our knowledge, this is the first attempt on sentiment analysis and emotion classification for the Nagamese Language. The aim of this work is to detect sentiments in terms of polarity (positive, negative and neutral) and basic emotions contained in textual content of Nagamese language. We build sentiment polarity lexicon of 1,195 nagamese words and use these to build features along with additional features for supervised machine learning techniques using Na\"ive Bayes and Support Vector Machines.   Keywords: Nagamese, NLP, sentiment analysis, machine learning",
    "authors": [
      "Ekha Morang",
      "Surhoni A. Ngullie",
      "Sashienla Longkumer",
      "Teisovi Angami"
    ],
    "published": "2025-12-01T04:01:29+00:00",
    "url": "https://arxiv.org/pdf/2512.01256v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01255v1",
    "title": "Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation",
    "abstract": "Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects.   In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework.   We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.",
    "authors": [
      "Qingyuan Fei",
      "Xin Liu",
      "Song Li",
      "Shujiang Wu",
      "Jianwei Hou",
      "Ping Chen",
      "Zifeng Kang"
    ],
    "published": "2025-12-01T04:00:06+00:00",
    "url": "https://arxiv.org/pdf/2512.01255v1",
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.SE"
    ]
  },
  {
    "arxiv_id": "2512.01252v1",
    "title": "Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe",
    "abstract": "Recent efforts on Diffusion Mixture-of-Experts (MoE) models have primarily focused on developing more sophisticated routing mechanisms. However, we observe that the underlying architectural configuration space remains markedly under-explored. Inspired by the MoE design paradigms established in large language models (LLMs), we identify a set of crucial architectural factors for building effective Diffusion MoE models--including DeepSeek-style expert modules, alternative intermediate widths, varying expert counts, and enhanced attention positional encodings. Our systematic study reveals that carefully tuning these configurations is essential for unlocking the full potential of Diffusion MoE models, often yielding gains that exceed those achieved by routing innovations alone. Through extensive experiments, we present novel architectures that can be efficiently applied to both latent and pixel-space diffusion frameworks, which provide a practical and efficient training recipe that enables Diffusion MoE models to surpass strong baselines while using equal or fewer activated parameters. All code and models are publicly available at: https://github.com/yhlleo/EfficientMoE.",
    "authors": [
      "Yahui Liu",
      "Yang Yue",
      "Jingyuan Zhang",
      "Chenxi Sun",
      "Yang Zhou",
      "Wencong Zeng",
      "Ruiming Tang",
      "Guorui Zhou"
    ],
    "published": "2025-12-01T03:52:31+00:00",
    "url": "https://arxiv.org/pdf/2512.01252v1",
    "categories": [
      "cs.LG",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01249v1",
    "title": "Pascal-Weighted Genetic Algorithms: A Binomially-Structured Recombination Framework",
    "abstract": "This paper introduces a new family of multi-parent recombination operators for Genetic Algorithms (GAs), based on normalized Pascal (binomial) coefficients. Unlike classical two-parent crossover operators, Pascal-Weighted Recombination (PWR) forms offsprings as structured convex combination of multiple parents, using binomially shaped weights that emphasize central inheritance while suppressing disruptive variance. We develop a mathematical framework for PWR, derive variance-transfer properties, and analyze its effect on schema survival. The operator is extended to real-valued, binary/logit, and permutation representations.   We evaluate the proposed method on four representative benchmarks: (i) PID controller tuning evaluated using the ITAE metric, (ii) FIR low-pass filter design under magnitude-response constraints, (iii) wireless power-modulation optimization under SINR coupling, and (iv) the Traveling Salesman Problem (TSP). We demonstrate how, across these benchmarks, PWR consistently yields smoother convergence, reduced variance, and achieves 9-22% performance gains over standard recombination operators. The approach is simple, algorithm-agnostic, and readily integrable into diverse GA architectures.",
    "authors": [
      "Otman A. Basir"
    ],
    "published": "2025-12-01T03:51:29+00:00",
    "url": "https://arxiv.org/pdf/2512.01249v1",
    "categories": [
      "cs.NE",
      "cs.AI",
      "eess.SY"
    ]
  },
  {
    "arxiv_id": "2512.01248v1",
    "title": "TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition",
    "abstract": "Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia",
    "authors": [
      "Junyuan Zhang",
      "Bin Wang",
      "Qintong Zhang",
      "Fan Wu",
      "Zichen Wen",
      "Jialin Lu",
      "Junjie Shan",
      "Ziqi Zhao",
      "Shuya Yang",
      "Ziling Wang",
      "Ziyang Miao",
      "Huaping Zhong",
      "Yuhang Zang",
      "Xiaoyi Dong",
      "Ka-Ho Chow",
      "Conghui He"
    ],
    "published": "2025-12-01T03:49:00+00:00",
    "url": "https://arxiv.org/pdf/2512.01248v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01242v1",
    "title": "Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation",
    "abstract": "We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.",
    "authors": [
      "Zirui Zhao",
      "Boye Niu",
      "David Hsu",
      "Wee Sun Lee"
    ],
    "published": "2025-12-01T03:38:44+00:00",
    "url": "https://arxiv.org/pdf/2512.01242v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01241v1",
    "title": "First, do NOHARM: towards clinically safe large language models",
    "abstract": "Large language models (LLMs) are routinely used by physicians and patients for medical advice, yet their clinical safety profiles remain poorly characterized. We present NOHARM (Numerous Options Harm Assessment for Risk in Medicine), a benchmark using 100 real primary-care-to-specialist consultation cases to measure harm frequency and severity from LLM-generated medical recommendations. NOHARM covers 10 specialties, with 12,747 expert annotations for 4,249 clinical management options. Across 31 LLMs, severe harm occurs in up to 22.2% (95% CI 21.6-22.8%) of cases, with harms of omission accounting for 76.6% (95% CI 76.4-76.8%) of errors. Safety performance is only moderately correlated (r = 0.61-0.64) with existing AI and medical knowledge benchmarks. The best models outperform generalist physicians on safety (mean difference 9.7%, 95% CI 7.0-12.5%), and a diverse multi-agent approach reduces harm compared to solo models (mean difference 8.0%, 95% CI 4.0-12.1%). Therefore, despite strong performance on existing evaluations, widely used AI models can produce severely harmful medical advice at nontrivial rates, underscoring clinical safety as a distinct performance dimension necessitating explicit measurement.",
    "authors": [
      "David Wu",
      "Fateme Nateghi Haredasht",
      "Saloni Kumar Maharaj",
      "Priyank Jain",
      "Jessica Tran",
      "Matthew Gwiazdon",
      "Arjun Rustagi",
      "Jenelle Jindal",
      "Jacob M. Koshy",
      "Vinay Kadiyala",
      "Anup Agarwal",
      "Bassman Tappuni",
      "Brianna French",
      "Sirus Jesudasen",
      "Christopher V. Cosgriff",
      "Rebanta Chakraborty",
      "Jillian Caldwell",
      "Susan Ziolkowski",
      "David J. Iberri",
      "Robert Diep",
      "Rahul S. Dalal",
      "Kira L. Newman",
      "Kristin Galetta",
      "J. Carl Pallais",
      "Nancy Wei",
      "Kathleen M. Buchheit",
      "David I. Hong",
      "Ernest Y. Lee",
      "Allen Shih",
      "Vartan Pahalyants",
      "Tamara B. Kaplan",
      "Vishnu Ravi",
      "Sarita Khemani",
      "April S. Liang",
      "Daniel Shirvani",
      "Advait Patil",
      "Nicholas Marshall",
      "Kanav Chopra",
      "Joel Koh",
      "Adi Badhwar",
      "Liam G. McCoy",
      "David J. H. Wu",
      "Yingjie Weng",
      "Sumant Ranji",
      "Kevin Schulman",
      "Nigam H. Shah",
      "Jason Hom",
      "Arnold Milstein",
      "Adam Rodman",
      "Jonathan H. Chen",
      "Ethan Goh"
    ],
    "published": "2025-12-01T03:33:16+00:00",
    "url": "https://arxiv.org/pdf/2512.01241v1",
    "categories": [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01236v1",
    "title": "PSR: Scaling Multi-Subject Personalized Image Generation with Pairwise Subject-Consistency Rewards",
    "abstract": "Personalized generation models for a single subject have demonstrated remarkable effectiveness, highlighting their significant potential. However, when extended to multiple subjects, existing models often exhibit degraded performance, particularly in maintaining subject consistency and adhering to textual prompts. We attribute these limitations to the absence of high-quality multi-subject datasets and refined post-training strategies. To address these challenges, we propose a scalable multi-subject data generation pipeline that leverages powerful single-subject generation models to construct diverse and high-quality multi-subject training data. Through this dataset, we first enable single-subject personalization models to acquire knowledge of synthesizing multi-image and multi-subject scenarios. Furthermore, to enhance both subject consistency and text controllability, we design a set of Pairwise Subject-Consistency Rewards and general-purpose rewards, which are incorporated into a refined reinforcement learning stage. To comprehensively evaluate multi-subject personalization, we introduce a new benchmark that assesses model performance using seven subsets across three dimensions. Extensive experiments demonstrate the effectiveness of our approach in advancing multi-subject personalized image generation. Github Link: https://github.com/wang-shulei/PSR",
    "authors": [
      "Shulei Wang",
      "Longhui Wei",
      "Xin He",
      "Jianbo Ouyang",
      "Hui Lu",
      "Zhou Zhao",
      "Qi Tian"
    ],
    "published": "2025-12-01T03:25:49+00:00",
    "url": "https://arxiv.org/pdf/2512.01236v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01234v2",
    "title": "Proactive Agentic Whiteboards: Enhancing Diagrammatic Learning",
    "abstract": "Educators frequently rely on diagrams to explain complex concepts during lectures, yet creating clear and complete visual representations in real time while simultaneously speaking can be cognitively demanding. Incomplete or unclear diagrams may hinder student comprehension, as learners must mentally reconstruct missing information while following the verbal explanation. Inspired by advances in code completion tools, we introduce DrawDash, an AI-powered whiteboard assistant that proactively completes and refines educational diagrams through multimodal understanding. DrawDash adopts a TAB-completion interaction model: it listens to spoken explanations, detects intent, and dynamically suggests refinements that can be accepted with a single keystroke. We demonstrate DrawDash across four diverse teaching scenarios, spanning topics from computer science and web development to biology. This work represents an early exploration into reducing instructors' cognitive load and improving diagram-based pedagogy through real-time, speech-driven visual assistance, and concludes with a discussion of current limitations and directions for formal classroom evaluation.",
    "authors": [
      "Suveen Ellawela",
      "Sashenka Gamage",
      "Dinithi Dissanayake"
    ],
    "published": "2025-12-01T03:20:12+00:00",
    "url": "https://arxiv.org/pdf/2512.01234v2",
    "categories": [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01232v1",
    "title": "LLM-as-a-Judge for Scalable Test Coverage Evaluation: Accuracy, Operational Reliability, and Cost",
    "abstract": "Assessing software test coverage at scale remains a bottleneck in QA pipelines. We present LLM-as-a-Judge (LAJ), a production-ready, rubric-driven framework for evaluating Gherkin acceptance tests with structured JSON outputs. Across 20 model configurations (GPT-4, GPT-5 with varying reasoning effort, and open-weight models) on 100 expert-annotated scripts over 5 runs (500 evaluations), we provide the first comprehensive analysis spanning accuracy, operational reliability, and cost. We introduce the Evaluation Completion Rate (ECR@1) to quantify first-attempt success, revealing reliability from 85.4% to 100.0% with material cost implications via retries. Results show that smaller models can outperform larger ones: GPT-4o Mini attains the best accuracy (6.07 MAAE), high reliability (96.6% ECR@1), and low cost ($1.01 per 1K), yielding a 78x cost reduction vs. GPT-5 (high reasoning) while improving accuracy. Reasoning effort is model-family dependent: GPT-5 benefits from increased reasoning (with predictable accuracy-cost tradeoffs), whereas open-weight models degrade across all dimensions as reasoning increases. Overall, cost spans 175x ($0.45-$78.96 per 1K). We release the dataset, framework, and code to support reproducibility and deployment.",
    "authors": [
      "Donghao Huang",
      "Shila Chew",
      "Anna Dutkiewicz",
      "Zhaoxia Wang"
    ],
    "published": "2025-12-01T03:19:33+00:00",
    "url": "https://arxiv.org/pdf/2512.01232v1",
    "categories": [
      "cs.SE",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01223v1",
    "title": "S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance",
    "abstract": "3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.",
    "authors": [
      "Beining Xu",
      "Siting Zhu",
      "Zhao Jin",
      "Junxian Li",
      "Hesheng Wang"
    ],
    "published": "2025-12-01T03:08:34+00:00",
    "url": "https://arxiv.org/pdf/2512.01223v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01222v1",
    "title": "Unsupervised decoding of encoded reasoning using language model interpretability",
    "abstract": "As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.",
    "authors": [
      "Ching Fang",
      "Samuel Marks"
    ],
    "published": "2025-12-01T03:05:20+00:00",
    "url": "https://arxiv.org/pdf/2512.01222v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01219v1",
    "title": "Neural Network Optimal Power Flow via Energy Gradient Flow and Unified Dynamics",
    "abstract": "Optimal Power Flow (OPF) is a core optimization problem in power system operation and planning, aiming to minimize generation costs while satisfying physical constraints such as power flow equations, generator limits, and voltage limits. Traditional OPF solving methods typically employ iterative optimization algorithms (such as interior point methods, sequential quadratic programming, etc.), with limitations including low computational efficiency, initial value sensitivity, and low batch computation efficiency. Most existing deep learning-based OPF methods rely on supervised learning, requiring pre-solving large numbers of cases, and have difficulty guaranteeing physical consistency. This paper proposes an Optimal Power Flow solving method based on neural network dynamics and energy gradient flow, transforming OPF problems into energy minimization problems. By constructing an energy function to measure the degree of deviation from the constraint manifold, and guiding networks to learn optimal solutions that simultaneously satisfy power flow constraints and minimize costs through gradient flow. Neural networks are trained unsupervised by directly minimizing physical residuals, requiring no labeled data, achieving true \"end-to-end\" physics-constrained learning.",
    "authors": [
      "Xuezhi Liu"
    ],
    "published": "2025-12-01T02:59:47+00:00",
    "url": "https://arxiv.org/pdf/2512.01219v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01218v1",
    "title": "How do trout regulate patterns of muscle contraction to optimize propulsive efficiency during steady swimming",
    "abstract": "Understanding efficient fish locomotion offers insights for biomechanics, fluid dynamics, and engineering. Traditional studies often miss the link between neuromuscular control and whole-body movement. To explore energy transfer in carangiform swimming, we created a bio-inspired digital trout. This model combined multibody dynamics, Hill-type muscle modeling, and a high-fidelity fluid-structure interaction algorithm, accurately replicating a real trout's form and properties. Using deep reinforcement learning, the trout's neural system achieved hierarchical spatiotemporal control of muscle activation. We systematically examined how activation strategies affect speed and energy use. Results show that axial myomere coupling-with activation spanning over 0.5 body lengths-is crucial for stable body wave propagation. Moderate muscle contraction duration ([0.1,0.3] of a tail-beat cycle) lets the body and fluid act as a passive damping system, cutting energy use. Additionally, the activation phase lag of myomeres shapes the body wave; if too large, it causes antagonistic contractions that hinder thrust. These findings advance bio-inspired locomotion understanding and aid energy-efficient underwater system design.",
    "authors": [
      "Tao Li",
      "Chunze Zhang",
      "Weiwei Yao",
      "Junzhao He",
      "Ji Hou",
      "Qin Zhou",
      "Lu Zhang"
    ],
    "published": "2025-12-01T02:57:02+00:00",
    "url": "https://arxiv.org/pdf/2512.01218v1",
    "categories": [
      "physics.flu-dyn",
      "cs.AI",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.01214v1",
    "title": "M4-BLIP: Advancing Multi-Modal Media Manipulation Detection through Face-Enhanced Local Analysis",
    "abstract": "In the contemporary digital landscape, multi-modal media manipulation has emerged as a significant societal threat, impacting the reliability and integrity of information dissemination. Current detection methodologies in this domain often overlook the crucial aspect of localized information, despite the fact that manipulations frequently occur in specific areas, particularly in facial regions. In response to this critical observation, we propose the M4-BLIP framework. This innovative framework utilizes the BLIP-2 model, renowned for its ability to extract local features, as the cornerstone for feature extraction. Complementing this, we incorporate local facial information as prior knowledge. A specially designed alignment and fusion module within M4-BLIP meticulously integrates these local and global features, creating a harmonious blend that enhances detection accuracy. Furthermore, our approach seamlessly integrates with Large Language Models (LLM), significantly improving the interpretability of the detection outcomes. Extensive quantitative and visualization experiments validate the effectiveness of our framework against the state-of-the-art competitors.",
    "authors": [
      "Hang Wu",
      "Ke Sun",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ],
    "published": "2025-12-01T02:54:03+00:00",
    "url": "https://arxiv.org/pdf/2512.01214v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01213v1",
    "title": "Closing the Approximation Gap of Partial AUC Optimization: A Tale of Two Formulations",
    "abstract": "As a variant of the Area Under the ROC Curve (AUC), the partial AUC (PAUC) focuses on a specific range of false positive rate (FPR) and/or true positive rate (TPR) in the ROC curve. It is a pivotal evaluation metric in real-world scenarios with both class imbalance and decision constraints. However, selecting instances within these constrained intervals during its calculation is NP-hard, and thus typically requires approximation techniques for practical resolution. Despite the progress made in PAUC optimization over the last few years, most existing methods still suffer from uncontrollable approximation errors or a limited scalability when optimizing the approximate PAUC objectives. In this paper, we close the approximation gap of PAUC optimization by presenting two simple instance-wise minimax reformulations: one with an asymptotically vanishing gap, the other with the unbiasedness at the cost of more variables. Our key idea is to first establish an equivalent instance-wise problem to lower the time complexity, simplify the complicated sample selection procedure by threshold learning, and then apply different smoothing techniques. Equipped with an efficient solver, the resulting algorithms enjoy a linear per-iteration computational complexity w.r.t. the sample size and a convergence rate of $O(\u03b5^{-1/3})$ for typical one-way and two-way PAUCs. Moreover, we provide a tight generalization bound of our minimax reformulations. The result explicitly demonstrates the impact of the TPR/FPR constraints $\u03b1$/$\u03b2$ on the generalization and exhibits a sharp order of $\\tilde{O}(\u03b1^{-1}\\n_+^{-1} + \u03b2^{-1}\\n_-^{-1})$. Finally, extensive experiments on several benchmark datasets validate the strength of our proposed methods.",
    "authors": [
      "Yangbangyan Jiang",
      "Qianqian Xu",
      "Huiyang Shao",
      "Zhiyong Yang",
      "Shilong Bao",
      "Xiaochun Cao",
      "Qingming Huang"
    ],
    "published": "2025-12-01T02:52:33+00:00",
    "url": "https://arxiv.org/pdf/2512.01213v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01210v2",
    "title": "Knowledge Graph Augmented Large Language Models for Disease Prediction",
    "abstract": "Electronic health records (EHRs) support powerful clinical prediction models, but existing methods typically provide coarse, post hoc explanations that offer limited value for patient-level decision making. We introduce a knowledge graph (KG)-guided chain-of-thought (CoT) framework that generates clinically grounded and temporally consistent reasoning for visit-level disease prediction in MIMIC-III. ICD-9 codes are mapped to PrimeKG, from which disease-relevant nodes and multi-hop reasoning paths are extracted and used as scaffolds for CoT generation; only explanations whose conclusions match observed outcomes are retained. Lightweight LLaMA-3.1-Instruct-8B and Gemma-7B models are then fine-tuned on this supervision corpus. Across ten PrimeKG-mapped diseases and limited training cohorts (400 and 1000 cases), KG-guided models outperform strong classical baselines, achieving AUROC values of 0.66 to 0.70 and macro-AUPR values of 0.40 to 0.47. The models also transfer zero-shot to the CRADLE cohort, improving accuracy from approximately 0.40 to 0.51 up to 0.72 to 0.77. A blinded clinician evaluation shows consistent preference for KG-guided CoT explanations in clarity, relevance, and clinical correctness.",
    "authors": [
      "Ruiyu Wang",
      "Tuan Vinh",
      "Ran Xu",
      "Yuyin Zhou",
      "Jiaying Lu",
      "Carl Yang",
      "Francisco Pasquel"
    ],
    "published": "2025-12-01T02:49:17+00:00",
    "url": "https://arxiv.org/pdf/2512.01210v2",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01208v1",
    "title": "Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking",
    "abstract": "Standard Transformers suffer from a \"Semantic Alignment Tax\", a prohibitive optimization cost required to organize a chaotic initialization into a coherent geometric map via local gradient diffusion. We hypothesize that this reliance on diffusive learning creates \"Catastrophic Rigidity\", rendering models unable to adapt to novel concepts without destroying their pre-trained reasoning capabilities. To isolate this phenomenon, we introduce Iterative Semantic Map Refinement (ISMR), a diagnostic protocol revealing that alignment is a fixed geometric barrier that scaling cannot solve; a 20-layer model overcomes this barrier no faster than a 1-layer model. We introduce the Phase-Resonant Intelligent Spectral Model (PRISM). PRISM encodes semantic identity as resonant frequencies in the complex domain (C^d) and replaces quadratic self-attention with linearithmic O(N log N) Gated Harmonic Convolutions. We validate PRISM on the WMT14 translation task. While the Standard Transformer maintains a slight edge in general competence on static benchmarks (23.88 vs 21.40 BLEU), it fails the \"Plasticity-Stability\" stress test completely. When injected with novel concepts, the Transformer suffers Catastrophic Forgetting, degrading by -10.55 BLEU points while achieving only 60% acquisition. In contrast, PRISM demonstrates Lossless Plasticity, achieving 96% 5-shot acquisition with negligible degradation (-0.84 BLEU). These results suggest that harmonic representations effectively decouple memory from reasoning, offering a structural solution to the plasticity-stability dilemma in real-time knowledge adaptation.",
    "authors": [
      "Alper Y\u0131ld\u0131r\u0131m",
      "\u0130brahim Y\u00fcceda\u011f"
    ],
    "published": "2025-12-01T02:46:15+00:00",
    "url": "https://arxiv.org/pdf/2512.01208v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01207v1",
    "title": "Physics-Constrained Neural Dynamics: A Unified Manifold Framework for Large-Scale Power Flow Computation",
    "abstract": "Power flow analysis is a fundamental tool for power system analysis, planning, and operational control. Traditional Newton-Raphson methods suffer from limitations such as initial value sensitivity and low efficiency in batch computation, while existing deep learning-based power flow solvers mostly rely on supervised learning, requiring pre-solving of numerous cases and struggling to guarantee physical consistency. This paper proposes a neural physics power flow solving method based on manifold geometry and gradient flow, by describing the power flow equations as a constraint manifold, and constructing an energy function \\(V(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{F}(\\mathbf{x})\\|^2\\) and gradient flow \\(\\frac{d\\mathbf{x}}{dt} = -\\nabla V(\\mathbf{x})\\), transforming power flow solving into an equilibrium point finding problem for dynamical systems. Neural networks are trained in an unsupervised manner by directly minimizing physical residuals, requiring no labeled data, achieving true \"end-to-end\" physics-constrained learning.",
    "authors": [
      "Xuezhi Liu"
    ],
    "published": "2025-12-01T02:45:23+00:00",
    "url": "https://arxiv.org/pdf/2512.01207v1",
    "categories": [
      "eess.SY",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01204v2",
    "title": "TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image",
    "abstract": "Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI--especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.",
    "authors": [
      "Ziqian Wang",
      "Yonghao He",
      "Licheng Yang",
      "Wei Zou",
      "Hongxuan Ma",
      "Liu Liu",
      "Wei Sui",
      "Yuxin Guo",
      "Hu Su"
    ],
    "published": "2025-12-01T02:38:52+00:00",
    "url": "https://arxiv.org/pdf/2512.01204v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01198v1",
    "title": "Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework",
    "abstract": "Traditional Chinese Medicine (TCM) theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop (HITL) framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis (IPA). Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient, and replicable HITL methodological pathway for the translation of ancient, concept-dense texts such as TCM.",
    "authors": [
      "Jiatong Han"
    ],
    "published": "2025-12-01T02:27:44+00:00",
    "url": "https://arxiv.org/pdf/2512.01198v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01191v1",
    "title": "Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks",
    "abstract": "Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.",
    "authors": [
      "Krithik Vishwanath",
      "Mrigayu Ghosh",
      "Anton Alyakin",
      "Daniel Alexander Alber",
      "Yindalon Aphinyanaphongs",
      "Eric Karl Oermann"
    ],
    "published": "2025-12-01T02:14:43+00:00",
    "url": "https://arxiv.org/pdf/2512.01191v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01189v1",
    "title": "fMRI2GES: Co-speech Gesture Reconstruction from fMRI Signal with Dual Brain Decoding Alignment",
    "abstract": "Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired \\{brain, speech, gesture\\} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, \\textbf{fMRI2GES}, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using \\textbf{Dual Brain Decoding Alignment}. This method relies on two key components: (i) observed texts that elicit brain responses, and (ii) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.",
    "authors": [
      "Chunzheng Zhu",
      "Jialin Shao",
      "Jianxin Lin",
      "Yijun Wang",
      "Jing Wang",
      "Jinhui Tang",
      "Kenli Li"
    ],
    "published": "2025-12-01T02:09:44+00:00",
    "url": "https://arxiv.org/pdf/2512.01189v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01188v1",
    "title": "Real-World Reinforcement Learning of Active Perception Behaviors",
    "abstract": "A robot's instantaneous sensory observations do not always reveal task-relevant state information. Under such partial observability, optimal behavior typically involves explicitly acting to gain the missing information. Today's standard robot learning techniques struggle to produce such active perception behaviors. We propose a simple real-world robot learning recipe to efficiently train active perception policies. Our approach, asymmetric advantage weighted regression (AAWR), exploits access to \"privileged\" extra sensors at training time. The privileged sensors enable training high-quality privileged value functions that aid in estimating the advantage of the target policy. Bootstrapping from a small number of potentially suboptimal demonstrations and an easy-to-obtain coarse policy initialization, AAWR quickly acquires active perception behaviors and boosts task performance. In evaluations on 8 manipulation tasks on 3 robots spanning varying degrees of partial observability, AAWR synthesizes reliable active perception behaviors that outperform all prior approaches. When initialized with a \"generalist\" robot policy that struggles with active perception tasks, AAWR efficiently generates information-gathering behaviors that allow it to operate under severe partial observability for manipulation tasks. Website: https://penn-pal-lab.github.io/aawr/",
    "authors": [
      "Edward S. Hu",
      "Jie Wang",
      "Xingfang Yuan",
      "Fiona Luo",
      "Muyao Li",
      "Gaspard Lambrechts",
      "Oleh Rybkin",
      "Dinesh Jayaraman"
    ],
    "published": "2025-12-01T02:05:20+00:00",
    "url": "https://arxiv.org/pdf/2512.01188v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01187v1",
    "title": "Teaching by Failure: Counter-Example-Driven Curricula for Transformer Self-Improvement",
    "abstract": "Transformer models often exhibit brittle extrapolation, failing on inputs that are longer or structurally more complex than those seen during training. We introduce Counter-Example-Driven Curricula (CEDC), an automated framework that improves model robustness by iteratively focusing on its own failures. At each step, CEDC uses the current model to generate a diverse set of candidate problems, employs a fast, executable verifier to identify incorrect predictions (counter-examples), and then fine-tunes the model on a dataset enriched with these discovered failures. We evaluate CEDC on a suite of algorithmic and natural language tasks, including integer addition, sorting, Dyck-2 language recognition, and three text classification benchmarks. Compared to static training and standard curriculum learning baselines, CEDC achieves up to 30x greater length extrapolation, is 3.75x more computationally efficient than uniform data augmentation, and requires no manual difficulty heuristics. We provide a detailed analysis of the counter-examples, showing how the curriculum naturally adapts to target progressively more complex error modes. Our findings establish verifier-guided, failure-driven learning as a simple, powerful, and efficient paradigm for enhancing the generalization capabilities of Transformer models.",
    "authors": [
      "Harshil Vejendla"
    ],
    "published": "2025-12-01T02:00:41+00:00",
    "url": "https://arxiv.org/pdf/2512.01187v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01183v1",
    "title": "TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness",
    "abstract": "The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.",
    "authors": [
      "Yongxin Zhou",
      "Philippe Mulhem",
      "Didier Schwab"
    ],
    "published": "2025-12-01T01:46:36+00:00",
    "url": "https://arxiv.org/pdf/2512.01183v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01181v1",
    "title": "First On-Orbit Demonstration of a Geospatial Foundation Model",
    "abstract": "Geospatial foundation models (GeoFMs) promise broad generalisation capacity for Earth observation (EO) tasks, particularly under data-limited conditions. However, their large size poses a barrier to deployment on resource-constrained space hardware. To address this, we present compact variants of a Vision Transformer (ViT)-based GeoFM that preserve downstream task performance while enabling onboard execution. Evaluation across five downstream tasks and validation in two representative flight environments show that model compression and domain adaptation are critical to reducing size and resource demands while maintaining high performance under operational conditions. We further demonstrate reliable on-orbit inference with the IMAGIN-e payload aboard the International Space Station. These results establish a pathway from large GeoFMs to flight-ready, resource-efficient deployments, expanding the feasibility of onboard AI for EO missions.",
    "authors": [
      "Andrew Du",
      "Roberto Del Prete",
      "Alejandro Mousist",
      "Nick Manser",
      "Fabrice Marre",
      "Andrew Barton",
      "Carl Seubert",
      "Gabriele Meoni",
      "Tat-Jun Chin"
    ],
    "published": "2025-12-01T01:43:03+00:00",
    "url": "https://arxiv.org/pdf/2512.01181v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01179v1",
    "title": "Toward a benchmark for CTR prediction in online advertising: datasets, evaluation protocols and perspectives",
    "abstract": "This research designs a unified architecture of CTR prediction benchmark (Bench-CTR) platform that offers flexible interfaces with datasets and components of a wide range of CTR prediction models. Moreover, we construct a comprehensive system of evaluation protocols encompassing real-world and synthetic datasets, a taxonomy of metrics, standardized procedures and experimental guidelines for calibrating the performance of CTR prediction models. Furthermore, we implement the proposed benchmark platform and conduct a comparative study to evaluate a wide range of state-of-the-art models from traditional multivariate statistical to modern large language model (LLM)-based approaches on three public datasets and two synthetic datasets. Experimental results reveal that, (1) high-order models largely outperform low-order models, though such advantage varies in terms of metrics and on different datasets; (2) LLM-based models demonstrate a remarkable data efficiency, i.e., achieving the comparable performance to other models while using only 2% of the training data; (3) the performance of CTR prediction models has achieved significant improvements from 2015 to 2016, then reached a stage with slow progress, which is consistent across various datasets. This benchmark is expected to facilitate model development and evaluation and enhance practitioners' understanding of the underlying mechanisms of models in the area of CTR prediction. Code is available at https://github.com/NuriaNinja/Bench-CTR.",
    "authors": [
      "Shan Gao",
      "Yanwu Yang"
    ],
    "published": "2025-12-01T01:36:55+00:00",
    "url": "https://arxiv.org/pdf/2512.01179v1",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01178v1",
    "title": "VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering",
    "abstract": "Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance's SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus",
    "authors": [
      "Zihua Liu",
      "Hiroki Sakuma",
      "Masatoshi Okutomi"
    ],
    "published": "2025-12-01T01:28:35+00:00",
    "url": "https://arxiv.org/pdf/2512.01178v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01174v1",
    "title": "DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks",
    "abstract": "As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench",
    "authors": [
      "Hyunjun Kim",
      "Sooyoung Ryu"
    ],
    "published": "2025-12-01T01:18:21+00:00",
    "url": "https://arxiv.org/pdf/2512.01174v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01171v1",
    "title": "Conversion rate prediction in online advertising: modeling techniques, performance evaluation and future directions",
    "abstract": "Conversion and conversion rate (CVR) prediction play a critical role in efficient advertising decision-making. In past decades, although researchers have developed plenty of models for CVR prediction, the methodological evolution and relationships between different techniques have been precluded. In this paper, we conduct a comprehensive literature review on CVR prediction in online advertising, and classify state-of-the-art CVR prediction models into six categories with respect to the underlying techniques and elaborate on connections between these techniques. For each category of models, we present the framework of underlying techniques, their advantages and disadvantages, and discuss how they are utilized for CVR prediction. Moreover, we summarize the performance of various CVR prediction models on public and proprietary datasets. Finally, we identify research trends, major challenges, and promising future directions. We observe that results of performance evaluation reported in prior studies are not unanimous; semantics-enriched, attribution-enhanced, debiased CVR prediction and jointly modeling CTR and CVR prediction would be promising directions to explore in the future. This review is expected to provide valuable references and insights for future researchers and practitioners in this area.",
    "authors": [
      "Tao Xue",
      "Yanwu Yang",
      "Panyu Zhai"
    ],
    "published": "2025-12-01T01:02:35+00:00",
    "url": "https://arxiv.org/pdf/2512.01171v1",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01170v1",
    "title": "Data assimilation and discrepancy modeling with shallow recurrent decoders",
    "abstract": "The requirements of modern sensing are rapidly evolving, driven by increasing demands for data efficiency, real-time processing, and deployment under limited sensing coverage. Complex physical systems are often characterized through the integration of a limited number of point sensors in combination with scientific computations which approximate the dominant, full-state dynamics. Simulation models, however, inevitably neglect small-scale or hidden processes, are sensitive to perturbations, or oversimplify parameter correlations, leading to reconstructions that often diverge from the reality measured by sensors. This creates a critical need for data assimilation, the process of integrating observational data with predictive simulation models to produce coherent and accurate estimates of the full state of complex physical systems. We propose a machine learning framework for Data Assimilation with a SHallow REcurrent Decoder (DA-SHRED) which bridges the simulation-to-real (SIM2REAL) gap between computational modeling and experimental sensor data. For real-world physics systems modeling high-dimensional spatiotemporal fields, where the full state cannot be directly observed and must be inferred from sparse sensor measurements, we leverage the latent space learned from a reduced simulation model via SHRED, and update these latent variables using real sensor data to accurately reconstruct the full system state. Furthermore, our algorithm incorporates a sparse identification of nonlinear dynamics based regression model in the latent space to identify functionals corresponding to missing dynamics in the simulation model. We demonstrate that DA-SHRED successfully closes the SIM2REAL gap and additionally recovers missing dynamics in highly complex systems, demonstrating that the combination of efficient temporal encoding and physics-informed correction enables robust data assimilation.",
    "authors": [
      "Yuxuan Bao",
      "J. Nathan Kutz"
    ],
    "published": "2025-12-01T01:01:48+00:00",
    "url": "https://arxiv.org/pdf/2512.01170v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.AP",
      "nlin.CD"
    ]
  },
  {
    "arxiv_id": "2512.01167v1",
    "title": "A TinyML Reinforcement Learning Approach for Energy-Efficient Light Control in Low-Cost Greenhouse Systems",
    "abstract": "This study presents a reinforcement learning (RL)-based control strategy for adaptive lighting regulation in controlled environments using a low-power microcontroller. A model-free Q-learning algorithm was implemented to dynamically adjust the brightness of a Light-Emitting Diode (LED) based on real-time feedback from a light-dependent resistor (LDR) sensor. The system was trained to stabilize at 13 distinct light intensity levels (L1 to L13), with each target corresponding to a specific range within the 64-state space derived from LDR readings. A total of 130 trials were conducted, covering all target levels with 10 episodes each. Performance was evaluated in terms of convergence speed, steps taken, and time required to reach target states. Box plots and histograms were generated to analyze the distribution of training time and learning efficiency across targets. Experimental validation demonstrated that the agent could effectively learn to stabilize at varying light levels with minimal overshooting and smooth convergence, even in the presence of environmental perturbations. This work highlights the feasibility of lightweight, on-device RL for energy-efficient lighting control and sets the groundwork for multi-modal environmental control applications in resource-constrained agricultural systems.",
    "authors": [
      "Mohamed Abdallah Salem",
      "Manuel Cuevas Perez",
      "Ahmed Harb Rabia"
    ],
    "published": "2025-12-01T00:58:05+00:00",
    "url": "https://arxiv.org/pdf/2512.01167v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ]
  },
  {
    "arxiv_id": "2512.01165v1",
    "title": "Real-Time On-the-Go Annotation Framework Using YOLO for Automated Dataset Generation",
    "abstract": "Efficient and accurate annotation of datasets remains a significant challenge for deploying object detection models such as You Only Look Once (YOLO) in real-world applications, particularly in agriculture where rapid decision-making is critical. Traditional annotation techniques are labor-intensive, requiring extensive manual labeling post data collection. This paper presents a novel real-time annotation approach leveraging YOLO models deployed on edge devices, enabling immediate labeling during image capture. To comprehensively evaluate the efficiency and accuracy of our proposed system, we conducted an extensive comparative analysis using three prominent YOLO architectures (YOLOv5, YOLOv8, YOLOv12) under various configurations: single-class versus multi-class annotation and pretrained versus scratch-based training. Our analysis includes detailed statistical tests and learning dynamics, demonstrating significant advantages of pretrained and single-class configurations in terms of model convergence, performance, and robustness. Results strongly validate the feasibility and effectiveness of our real-time annotation framework, highlighting its capability to drastically reduce dataset preparation time while maintaining high annotation quality.",
    "authors": [
      "Mohamed Abdallah Salem",
      "Ahmed Harb Rabia"
    ],
    "published": "2025-12-01T00:54:57+00:00",
    "url": "https://arxiv.org/pdf/2512.01165v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.01163v1",
    "title": "2D-ThermAl: Physics-Informed Framework for Thermal Analysis of Circuits using Generative AI",
    "abstract": "Thermal analysis is increasingly critical in modern integrated circuits, where non-uniform power dissipation and high transistor densities can cause rapid temperature spikes and reliability concerns. Traditional methods, such as FEM-based simulations offer high accuracy but computationally prohibitive for early-stage design, often requiring multiple iterative redesign cycles to resolve late-stage thermal failures. To address these challenges, we propose 'ThermAl', a physics-informed generative AI framework which effectively identifies heat sources and estimates full-chip transient and steady-state thermal distributions directly from input activity profiles. ThermAl employs a hybrid U-Net architecture enhanced with positional encoding and a Boltzmann regularizer to maintain physical fidelity. Our model is trained on an extensive dataset of heat dissipation maps, ranging from simple logic gates (e.g., inverters, NAND, XOR) to complex designs, generated via COMSOL. Experimental results demonstrate that ThermAl delivers precise temperature mappings for large circuits, with a root mean squared error (RMSE) of only 0.71\u00b0C, and outperforms conventional FEM tools by running up to ~200 times faster. We analyze performance across diverse layouts and workloads, and discuss its applicability to large-scale EDA workflows. While thermal reliability assessments often extend beyond 85\u00b0C for post-layout signoff, our focus here is on early-stage hotspot detection and thermal pattern learning. To ensure generalization beyond the nominal operating range 25-55\u00b0C, we additionally performed cross-validation on an extended dataset spanning 25-95\u00b0C maintaining a high accuracy (<2.2% full-scale RMSE) even under elevated temperature conditions representative of peak power and stress scenarios.",
    "authors": [
      "Soumyadeep Chandra",
      "Sayeed Shafayet Chowdhury",
      "Kaushik Roy"
    ],
    "published": "2025-12-01T00:45:26+00:00",
    "url": "https://arxiv.org/pdf/2512.01163v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01155v2",
    "title": "Beyond Greenfield: The D3 Framework for AI-Driven Productivity in Brownfield Engineering",
    "abstract": "Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and 83% of participants spent less time fixing or rewriting code due to better initial planning with AI. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.",
    "authors": [
      "Krishna Kumaar Sharma"
    ],
    "published": "2025-12-01T00:26:41+00:00",
    "url": "https://arxiv.org/pdf/2512.01155v2",
    "categories": [
      "cs.SE",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01153v1",
    "title": "DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling",
    "abstract": "Adversarially guided diffusion sampling often achieves the target class, but sample quality degrades as deviations between the adversarially controlled and nominal trajectories accumulate. We formalize this degradation as a path-space Kullback-Leibler divergence(path-KL) between controlled and nominal (uncontrolled) diffusion processes, thereby showing via Girsanov's theorem that it exactly equals the control energy. Building on this stochastic optimal control (SOC) view, we theoretically establish that minimizing this path-KL simultaneously tightens upper bounds on both the 2-Wasserstein distance and Fr\u00e9chet Inception Distance (FID), revealing a principled connection between adversarial control energy and perceptual fidelity. From a variational perspective, we derive a first-order optimality condition for the control: among all directions that yield the same classification gain, the component tangent to iso-(log-)density surfaces (i.e., orthogonal to the score) minimizes path-KL, whereas the normal component directly increases distributional drift. This leads to DPAC (Distribution-Preserving Adversarial Control), a diffusion guidance rule that projects adversarial gradients onto the tangent space defined by the generative score geometry. We further show that in discrete solvers, the tangent projection cancels the O(\u0394t) leading error term in the Wasserstein distance, achieving an O(\u0394t^2) quality gap; moreover, it remains second-order robust to score or metric approximation. Empirical studies on ImageNet-100 validate the theoretical predictions, confirming that DPAC achieves lower FID and estimated path-KL at matched attack success rates.",
    "authors": [
      "Han-Jin Lee",
      "Han-Ju Lee",
      "Jin-Seong Kim",
      "Seok-Hwan Choi"
    ],
    "published": "2025-12-01T00:15:05+00:00",
    "url": "https://arxiv.org/pdf/2512.01153v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01152v1",
    "title": "Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution",
    "abstract": "As we deploy machine learning systems in the real world, a core challenge is to maintain a model that is performant even as the data shifts. Such shifts can take many forms: new classes may emerge that were absent during training, a problem known as open-set recognition, and the distribution of known categories may change. Guarantees on open-set recognition are mostly derived under the assumption that the distribution of known classes, which we call \\emph{the background distribution}, is fixed. In this paper we develop \\ours{}, a method that is guaranteed to solve open-set recognition even in the challenging case where the background distribution shifts. We prove that the method works under benign assumptions that the novel class is separable from the non-novel classes, and provide theoretical guarantees that it outperforms a representative baseline in a simplified overparameterized setting. We develop techniques to make \\ours{} scalable and robust, and perform comprehensive empirical evaluations on image and text data. The results show that \\ours{} significantly outperforms existing open-set recognition methods under background shift. Moreover, we provide new insights into how factors such as the size of the novel class influences performance, an aspect that has not been extensively explored in prior work.",
    "authors": [
      "Shravan Chaudhari",
      "Yoav Wald",
      "Suchi Saria"
    ],
    "published": "2025-12-01T00:08:18+00:00",
    "url": "https://arxiv.org/pdf/2512.01152v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01149v1",
    "title": "A Benchmark of Causal vs Correlation AI for Predictive Maintenance",
    "abstract": "Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study evaluates eight predictive models, ranging from baseline statistical approaches to formal causal inference methods, on a dataset of 10,000 CNC machines with a 3.3% failure prevalence. The formal causal inference model (L5) achieved estimated annual cost savings of 1.16 million USD (a 70.2 percent reduction), outperforming the best correlation-based decision tree model (L3) by approximately 80,000 USD per year. The causal model matched the highest observed recall (87.9 percent) while reducing false alarms by 97 percent (from 165 to 5) and attained a precision of 92.1 percent, with a train-test performance gap of only 2.6 percentage points. These results indicate that causal AI methods, when combined with domain knowledge, can yield superior financial outcomes and more interpretable predictions compared to correlation-based approaches in predictive maintenance applications.",
    "authors": [
      "Krishna Taduri",
      "Shaunak Dhande",
      "Giacinto Paolo",
      "Saggese",
      "Paul Smith"
    ],
    "published": "2025-11-30T23:59:37+00:00",
    "url": "https://arxiv.org/pdf/2512.01149v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01148v1",
    "title": "SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models",
    "abstract": "Understanding social interactions from visual cues is a fundamental challenge for a socially competent AI. While powerful pre-trained vision-language models (VLMs) have shown remarkable general capabilities, they surprisingly struggle to unify and learn multiple social perception tasks simultaneously, often exhibiting negative transfer. We identify that this negative transfer stems from a critical issue we term \"social degradation,\" whereby the general visual-linguistic pre-training process of VLMs impairs the visual encoder's ability to represent nuanced social information. We investigate this behavior further under two lenses: decodability through linear representation probing and compatibility through gradient conflict analysis, revealing that both play a role in the degradation, especially the former, which is significantly compromised in the VLM pre-training process. To address these issues, we propose SocialFusion, a unified framework that learns a minimal connection between a frozen visual encoder and a language model. Compared with existing VLMs, it exhibits positive transfer across all five social tasks, leveraging synergies between them to enhance overall performance and achieves comparable performance to task-specific state-of-the-art models on various benchmarks. Our findings suggest that current VLM pre-training strategies may be detrimental to acquiring general social competence and highlight the need for more socially-aware training paradigms.",
    "authors": [
      "Hamza Tahboub",
      "Weiyan Shi",
      "Gang Hua",
      "Huaizu Jiang"
    ],
    "published": "2025-11-30T23:54:54+00:00",
    "url": "https://arxiv.org/pdf/2512.01148v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01145v1",
    "title": "Weakly Supervised Continuous Micro-Expression Intensity Estimation Using Temporal Deep Neural Network",
    "abstract": "Micro-facial expressions are brief and involuntary facial movements that reflect genuine emotional states. While most prior work focuses on classifying discrete micro-expression categories, far fewer studies address the continuous evolution of intensity over time. Progress in this direction is limited by the lack of frame-level intensity labels, which makes fully supervised regression impractical.   We propose a unified framework for continuous micro-expression intensity estimation using only weak temporal labels (onset, apex, offset). A simple triangular prior converts sparse temporal landmarks into dense pseudo-intensity trajectories, and a lightweight temporal regression model that combines a ResNet18 encoder with a bidirectional GRU predicts frame-wise intensity directly from image sequences. The method requires no frame-level annotation effort and is applied consistently across datasets through a single preprocessing and temporal alignment pipeline.   Experiments on SAMM and CASME II show strong temporal agreement with the pseudo-intensity trajectories. On SAMM, the model reaches a Spearman correlation of 0.9014 and a Kendall correlation of 0.7999, outperforming a frame-wise baseline. On CASME II, it achieves up to 0.9116 and 0.8168, respectively, when trained without the apex-ranking term. Ablation studies confirm that temporal modeling and structured pseudo labels are central to capturing the rise-apex-fall dynamics of micro-facial movements.   To our knowledge, this is the first unified approach for continuous micro-expression intensity estimation using only sparse temporal annotations.",
    "authors": [
      "Riyadh Mohammed Almushrafy"
    ],
    "published": "2025-11-30T23:47:47+00:00",
    "url": "https://arxiv.org/pdf/2512.01145v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01128v1",
    "title": "OmniFD: A Unified Model for Versatile Face Forgery Detection",
    "abstract": "Face forgery detection encompasses multiple critical tasks, including identifying forged images and videos and localizing manipulated regions and temporal segments. Current approaches typically employ task-specific models with independent architectures, leading to computational redundancy and ignoring potential correlations across related tasks. We introduce OmniFD, a unified framework that jointly addresses four core face forgery detection tasks within a single model, i.e., image and video classification, spatial localization, and temporal localization. Our architecture consists of three principal components: (1) a shared Swin Transformer encoder that extracts unified 4D spatiotemporal representations from both images and video inputs, (2) a cross-task interaction module with learnable queries that dynamically captures inter-task dependencies through attention-based reasoning, and (3) lightweight decoding heads that transform refined representations into corresponding predictions for all FFD tasks. Extensive experiments demonstrate OmniFD's advantage over task-specific models. Its unified design leverages multi-task learning to capture generalized representations across tasks, especially enabling fine-grained knowledge transfer that facilitates other tasks. For example, video classification accuracy improves by 4.63% when image data are incorporated. Furthermore, by unifying images, videos and the four tasks within one framework, OmniFD achieves superior performance across diverse benchmarks with high efficiency and scalability, e.g., reducing 63% model parameters and 50% training time. It establishes a practical and generalizable solution for comprehensive face forgery detection in real-world applications. The source code is made available at https://github.com/haotianll/OmniFD.",
    "authors": [
      "Haotian Liu",
      "Haoyu Chen",
      "Chenhui Pan",
      "You Hu",
      "Guoying Zhao",
      "Xiaobai Li"
    ],
    "published": "2025-11-30T22:36:42+00:00",
    "url": "https://arxiv.org/pdf/2512.01128v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01127v1",
    "title": "Mode-Conditioning Unlocks Superior Test-Time Scaling",
    "abstract": "Parallel sampling promises substantial gains in test-time scaling, but its effectiveness is sharply limited by diversity collapse, where models concentrate on a few modes and repeated samples produce the same mistakes. We propose the mode-conditioning (ModC) framework, which explicitly allocates test-time compute across reasoning modes using either specialist models or mode-specific prefixes. ModC consistently improves scaling across controlled graph-search tasks and large-scale reasoning benchmarks, spanning model families and sizes from 0.5B to 7B. On OpenThoughts, fine-tuning Qwen2.5-7B with ModC achieves a 4x efficiency gain over standard training while also improving the maximum attainable Pass@k. We further show that gradient clustering enables ModC without explicit mode labels, yielding up to 10% gains on datasets such as NuminaMath. Finally, we show that ModC improves reinforcement learning (RL) and can further boost diversity-inducing RL methods. These results demonstrate that standard training underutilizes the diversity in data, and that ModC provides a simple, effective remedy for unlocking the full benefits of diversity in test-time scaling.",
    "authors": [
      "Chen Henry Wu",
      "Sachin Goyal",
      "Aditi Raghunathan"
    ],
    "published": "2025-11-30T22:36:20+00:00",
    "url": "https://arxiv.org/pdf/2512.01127v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01119v1",
    "title": "World Model Robustness via Surprise Recognition",
    "abstract": "AI systems deployed in the real world must contend with distractions and out-of-distribution (OOD) noise that can destabilize their policies and lead to unsafe behavior. While robust training can reduce sensitivity to some forms of noise, it is infeasible to anticipate all possible OOD conditions. To mitigate this issue, we develop an algorithm that leverages a world model's inherent measure of surprise to reduce the impact of noise in world model--based reinforcement learning agents. We introduce both multi-representation and single-representation rejection sampling, enabling robustness to settings with multiple faulty sensors or a single faulty sensor. While the introduction of noise typically degrades agent performance, we show that our techniques preserve performance relative to baselines under varying types and levels of noise across multiple environments within self-driving simulation domains (CARLA and Safety Gymnasium). Furthermore, we demonstrate that our methods enhance the stability of two state-of-the-art world models with markedly different underlying architectures: Cosmos and DreamerV3. Together, these results highlight the robustness of our approach across world modeling domains. We release our code at https://github.com/Bluefin-Tuna/WISER .",
    "authors": [
      "Geigh Zollicoffer",
      "Tanush Chopra",
      "Mingkuan Yan",
      "Xiaoxu Ma",
      "Kenneth Eaton",
      "Mark Riedl"
    ],
    "published": "2025-11-30T22:25:45+00:00",
    "url": "https://arxiv.org/pdf/2512.01119v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01116v1",
    "title": "Structural Prognostic Event Modeling for Multimodal Cancer Survival Analysis",
    "abstract": "The integration of histology images and gene profiles has shown great promise for improving survival prediction in cancer. However, current approaches often struggle to model intra- and inter-modal interactions efficiently and effectively due to the high dimensionality and complexity of the inputs. A major challenge is capturing critical prognostic events that, though few, underlie the complexity of the observed inputs and largely determine patient outcomes. These events, manifested as high-level structural signals such as spatial histologic patterns or pathway co-activations, are typically sparse, patient-specific, and unannotated, making them inherently difficult to uncover. To address this, we propose SlotSPE, a slot-based framework for structural prognostic event modeling. Specifically, inspired by the principle of factorial coding, we compress each patient's multimodal inputs into compact, modality-specific sets of mutually distinctive slots using slot attention. By leveraging these slot representations as encodings for prognostic events, our framework enables both efficient and effective modeling of complex intra- and inter-modal interactions, while also facilitating seamless incorporation of biological priors that enhance prognostic relevance. Extensive experiments on ten cancer benchmarks show that SlotSPE outperforms existing methods in 8 out of 10 cohorts, achieving an overall improvement of 2.9%. It remains robust under missing genomic data and delivers markedly improved interpretability through structured event decomposition.",
    "authors": [
      "Yilan Zhang",
      "Li Nanbo",
      "Changchun Yang",
      "J\u00fcrgen Schmidhuber",
      "Xin Gao"
    ],
    "published": "2025-11-30T22:24:09+00:00",
    "url": "https://arxiv.org/pdf/2512.01116v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01113v1",
    "title": "Efficiently Learning Branching Networks for Multitask Algorithmic Reasoning",
    "abstract": "Algorithmic reasoning -- the ability to perform step-by-step logical inference -- has become a core benchmark for evaluating reasoning in graph neural networks (GNNs) and large language models (LLMs). Ideally, one would like to design a single model capable of performing well on multiple algorithmic reasoning tasks simultaneously. However, this is challenging when the execution steps of algorithms differ from one another, causing negative interference when they are trained together.   We propose branching neural networks, a principled architecture for multitask algorithmic reasoning. Searching for the optimal $k$-ary tree with $L$ layers over $n$ algorithmic tasks is combinatorial, requiring exploration of up to $k^{nL}$ possible structures. We develop AutoBRANE, an efficient algorithm that reduces this search to $O(nL)$ time by solving a convex relaxation at each layer to approximate an optimal task partition. The method clusters tasks using gradient-based affinity scores and can be used on top of any base model, including GNNs and LLMs.   We validate AutoBRANE on a broad suite of graph-algorithmic and text-based reasoning benchmarks. We show that gradient features estimate true task performance within 5% error across four GNNs and four LLMs (up to 34B parameters). On the CLRS benchmark, it outperforms the strongest single multitask GNN by 3.7% and the best baseline by 1.2%, while reducing runtime by 48% and memory usage by 26%. The learned branching structures reveal an intuitively reasonable hierarchical clustering of related algorithms. On three text-based graph reasoning benchmarks, AutoBRANE improves over the best non-branching multitask baseline by 3.2%. Finally, on a large graph dataset with 21M edges and 500 tasks, AutoBRANE achieves a 28% accuracy gain over existing multitask and branching architectures, along with a 4.5$\\times$ reduction in runtime.",
    "authors": [
      "Dongyue Li",
      "Zhenshuo Zhang",
      "Minxuan Duan",
      "Edgar Dobriban",
      "Hongyang R. Zhang"
    ],
    "published": "2025-11-30T22:19:55+00:00",
    "url": "https://arxiv.org/pdf/2512.01113v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DS"
    ]
  },
  {
    "arxiv_id": "2512.02080v1",
    "title": "The 4/$\u03b4$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee",
    "abstract": "The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($\u03b4$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $\u03b4> 0$, with an expected iteration count bounded by $\\mathbb{E}[n] \\leq 4/\u03b4$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.",
    "authors": [
      "PIerre Dantas",
      "Lucas Cordeiro",
      "Youcheng Sun",
      "Waldir Junior"
    ],
    "published": "2025-11-30T22:19:09+00:00",
    "url": "https://arxiv.org/pdf/2512.02080v1",
    "categories": [
      "cs.AI",
      "cs.FL",
      "cs.LG",
      "cs.SE"
    ]
  },
  {
    "arxiv_id": "2512.01109v1",
    "title": "How do we measure privacy in text? A survey of text anonymization metrics",
    "abstract": "In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.",
    "authors": [
      "Yaxuan Ren",
      "Krithika Ramesh",
      "Yaxing Yao",
      "Anjalie Field"
    ],
    "published": "2025-11-30T22:12:30+00:00",
    "url": "https://arxiv.org/pdf/2512.01109v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01107v1",
    "title": "Foundation Priors",
    "abstract": "Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.   We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.",
    "authors": [
      "Sanjog Misra"
    ],
    "published": "2025-11-30T22:09:37+00:00",
    "url": "https://arxiv.org/pdf/2512.01107v1",
    "categories": [
      "cs.AI",
      "econ.EM",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.01105v1",
    "title": "Supporting Productivity Skill Development in College Students through Social Robot Coaching: A Proof-of-Concept",
    "abstract": "College students often face academic challenges that hamper their productivity and well-being. Although self-help books and productivity apps are popular, they often fall short. Books provide generalized, non-interactive guidance, and apps are not inherently educational and can hinder the development of key organizational skills. Traditional productivity coaching offers personalized support, but is resource-intensive and difficult to scale. In this study, we present a proof-of-concept for a socially assistive robot (SAR) as an educational coach and a potential solution to the limitations of existing productivity tools and coaching approaches. The SAR delivers six different lessons on time management and task prioritization. Users interact via a chat interface, while the SAR responds through speech (with a toggle option). An integrated dashboard monitors progress, mood, engagement, confidence per lesson, and time spent per lesson. It also offers personalized productivity insights to foster reflection and self-awareness. We evaluated the system with 15 college students, achieving a System Usability Score of 79.2 and high ratings for overall experience and engagement. Our findings suggest that SAR-based productivity coaching can offer an effective and scalable solution to improve productivity among college students.",
    "authors": [
      "Himanshi Lalwani",
      "Hanan Salam"
    ],
    "published": "2025-11-30T22:08:02+00:00",
    "url": "https://arxiv.org/pdf/2512.01105v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ]
  },
  {
    "arxiv_id": "2512.01104v1",
    "title": "Estimation of Kinematic Motion from Dashcam Footage",
    "abstract": "The goal of this paper is to explore the accuracy of dashcam footage to predict the actual kinematic motion of a car-like vehicle. Our approach uses ground truth information from the vehicle's on-board data stream, through the controller area network, and a time-synchronized dashboard camera, mounted to a consumer-grade vehicle, for 18 hours of footage and driving. The contributions of the paper include neural network models that allow us to quantify the accuracy of predicting the vehicle speed and yaw, as well as the presence of a lead vehicle, and its relative distance and speed. In addition, the paper describes how other researchers can gather their own data to perform similar experiments, using open-source tools and off-the-shelf technology.",
    "authors": [
      "Evelyn Zhang",
      "Alex Richardson",
      "Jonathan Sprinkle"
    ],
    "published": "2025-11-30T22:07:40+00:00",
    "url": "https://arxiv.org/pdf/2512.01104v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01103v1",
    "title": "Learning Eigenstructures of Unstructured Data Manifolds",
    "abstract": "We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.",
    "authors": [
      "Roy Velich",
      "Arkadi Piven",
      "David Bensa\u00efd",
      "Daniel Cremers",
      "Thomas Dag\u00e8s",
      "Ron Kimmel"
    ],
    "published": "2025-11-30T22:06:49+00:00",
    "url": "https://arxiv.org/pdf/2512.01103v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01099v1",
    "title": "Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems",
    "abstract": "As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.",
    "authors": [
      "Daria Smirnova",
      "Hamid Nasiri",
      "Marta Adamska",
      "Zhengxin Yu",
      "Peter Garraghan"
    ],
    "published": "2025-11-30T21:46:54+00:00",
    "url": "https://arxiv.org/pdf/2512.01099v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01097v1",
    "title": "Discriminative classification with generative features: bridging Naive Bayes and logistic regression",
    "abstract": "We introduce Smart Bayes, a new classification framework that bridges generative and discriminative modeling by integrating likelihood-ratio-based generative features into a logistic-regression-style discriminative classifier. From the generative perspective, Smart Bayes relaxes the fixed unit weights of Naive Bayes by allowing data-driven coefficients on density-ratio features. From a discriminative perspective, it constructs transformed inputs as marginal log-density ratios that explicitly quantify how much more likely each feature value is under one class than another, thereby providing predictors with stronger class separation than the raw covariates. To support this framework, we develop a spline-based estimator for univariate log-density ratios that is flexible, robust, and computationally efficient. Through extensive simulations and real-data studies, Smart Bayes often outperforms both logistic regression and Naive Bayes. Our results highlight the potential of hybrid approaches that exploit generative structure to enhance discriminative performance.",
    "authors": [
      "Zachary Terner",
      "Alexander Petersen",
      "Yuedong Wang"
    ],
    "published": "2025-11-30T21:34:24+00:00",
    "url": "https://arxiv.org/pdf/2512.01097v1",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.CO",
      "stat.ME"
    ]
  },
  {
    "arxiv_id": "2512.01095v1",
    "title": "CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions",
    "abstract": "We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions. CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes. CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition. We conduct extensive experiments with current state-of-the-art VLMs, both open-source and proprietary, and reveal their limitations in generalizing to cyclical dynamics such as linear and orbital motion, as well as time-dependent changes in visual attributes like color and scale. Our results demonstrate that present-day VLMs struggle to reliably detect and exploit cyclic patterns, lack a notion of temporal understanding, and are unable to extract quantitative insights from scenes, such as the number of objects in motion, highlighting a significant technical gap that needs to be addressed. More specifically, we find no single model consistently leads in performance: neither size nor architecture correlates strongly with outcomes, and no model succeeds equally well across all tasks. By providing a targeted challenge and a comprehensive evaluation framework, CycliST paves the way for visual reasoning models that surpass the state-of-the-art in understanding periodic patterns.",
    "authors": [
      "Simon Kohaut",
      "Daniel Ochs",
      "Shun Zhang",
      "Benedict Flade",
      "Julian Eggert",
      "Kristian Kersting",
      "Devendra Singh Dhami"
    ],
    "published": "2025-11-30T21:28:41+00:00",
    "url": "https://arxiv.org/pdf/2512.01095v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01094v1",
    "title": "Accelerating Inference of Masked Image Generators via Reinforcement Learning",
    "abstract": "Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.",
    "authors": [
      "Pranav Subbaraman",
      "Shufan Li",
      "Siyan Zhao",
      "Aditya Grover"
    ],
    "published": "2025-11-30T21:28:00+00:00",
    "url": "https://arxiv.org/pdf/2512.01094v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01089v1",
    "title": "CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents",
    "abstract": "Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.",
    "authors": [
      "Peter Jansen",
      "Samiah Hassan",
      "Pragnya Narasimha"
    ],
    "published": "2025-11-30T21:19:10+00:00",
    "url": "https://arxiv.org/pdf/2512.01089v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01085v1",
    "title": "Generalized Medical Phrase Grounding",
    "abstract": "Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.",
    "authors": [
      "Wenjun Zhang",
      "Shekhar S. Chandra",
      "Aaron Nicolson"
    ],
    "published": "2025-11-30T21:09:41+00:00",
    "url": "https://arxiv.org/pdf/2512.01085v1",
    "categories": [
      "cs.CV",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01081v1",
    "title": "Testing the Machine Consciousness Hypothesis",
    "abstract": "The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.",
    "authors": [
      "Stephen Fitz"
    ],
    "published": "2025-11-30T21:05:48+00:00",
    "url": "https://arxiv.org/pdf/2512.01081v1",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA",
      "cs.NE",
      "q-bio.NC"
    ]
  },
  {
    "arxiv_id": "2512.01078v1",
    "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds",
    "abstract": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.",
    "authors": [
      "Jiawei Ren",
      "Yan Zhuang",
      "Xiaokang Ye",
      "Lingjun Mao",
      "Xuhong He",
      "Jianzhi Shen",
      "Mrinaal Dogra",
      "Yiming Liang",
      "Ruixuan Zhang",
      "Tianai Yue",
      "Yiqing Yang",
      "Eric Liu",
      "Ryan Wu",
      "Kevin Benavente",
      "Rajiv Mandya Nagaraju",
      "Muhammad Faayez",
      "Xiyan Zhang",
      "Dhruv Vivek Sharma",
      "Xianrui Zhong",
      "Ziqiao Ma",
      "Tianmin Shu",
      "Zhiting Hu",
      "Lianhui Qin"
    ],
    "published": "2025-11-30T20:58:13+00:00",
    "url": "https://arxiv.org/pdf/2512.01078v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01077v1",
    "title": "ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages",
    "abstract": "We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.",
    "authors": [
      "Neha Joshi",
      "Pamir Gogoi",
      "Aasim Mirza",
      "Aayush Jansari",
      "Aditya Yadavalli",
      "Ayushi Pandey",
      "Arunima Shukla",
      "Deepthi Sudharsan",
      "Kalika Bali",
      "Vivek Seshadri"
    ],
    "published": "2025-11-30T20:51:20+00:00",
    "url": "https://arxiv.org/pdf/2512.01077v1",
    "categories": [
      "cs.CL",
      "cs.HC"
    ]
  },
  {
    "arxiv_id": "2512.01067v1",
    "title": "On The Finetuning of MLIPs Through the Lens of Iterated Maps With BPTT",
    "abstract": "Vital to the creation of advanced materials is performing structural relaxations. Traditional approaches built on physics-derived first-principles calculations are computationally expensive, motivating the creation of machine-learning interatomic potentials (MLIPs). Traditional approaches to training MLIPs for structural relaxations involves training models to faithfully reproduce first-principles computed forces. We propose a fine-tuning method to be used on a pretrained MLIP in which we create a fully-differentiable end-to-end simulation loop that optimizes the predicted final structures directly. Trajectories are unrolled and gradients are tracked through the entire relaxation. We show that this method achieves substantial performance gains when applied to pretrained models, leading to a nearly $50\\%$ reduction in test error across the sample datasets. Interestingly, we show the process is robust to substantial variation in the relaxation setup, achieving negligibly different results across varied hyperparameter and procedural modifications. Experimental results indicate this is due to a ``preference'' of BPTT to modify the MLIP rather than the other trainable parameters. Of particular interest to practitioners is that this approach lowers the data requirements for producing an effective domain-specific MLIP, addressing a common bottleneck in practical deployment.",
    "authors": [
      "Evan Dramko",
      "Yizhi Zhu",
      "Aleksandar Krivokapic",
      "Geoffroy Hautier",
      "Thomas Reps",
      "Christopher Jermaine",
      "Anastasios Kyrillidis"
    ],
    "published": "2025-11-30T20:34:37+00:00",
    "url": "https://arxiv.org/pdf/2512.01067v1",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01062v1",
    "title": "PIANO: Physics-informed Dual Neural Operator for Precipitation Nowcasting",
    "abstract": "Precipitation nowcasting, key for early warning of disasters, currently relies on computationally expensive and restrictive methods that limit access to many countries. To overcome this challenge, we propose precipitation nowcasting using satellite imagery with physics constraints for improved accuracy and physical consistency. We use a novel physics-informed dual neural operator (PIANO) structure to enforce the fundamental equation of advection-diffusion during training to predict satellite imagery using a PINN loss. Then, we use a generative model to convert satellite images to radar images, which are used for precipitation nowcasting. Compared to baseline models, our proposed model shows a notable improvement in moderate (4mm/h) precipitation event prediction alongside short-term heavy (8mm/h) precipitation event prediction. It also demonstrates low seasonal variability in predictions, indicating robustness for generalization. This study suggests the potential of the PIANO and serves as a good baseline for physics-informed precipitation nowcasting.",
    "authors": [
      "Seokhyun Chin",
      "Junghwan Park",
      "Woojin Cho"
    ],
    "published": "2025-11-30T20:17:14+00:00",
    "url": "https://arxiv.org/pdf/2512.01062v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01061v1",
    "title": "Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer",
    "abstract": "Recent progress in GPU-accelerated, photorealistic simulation has opened a scalable data-generation path for robot learning, where massive physics and visual randomization allow policies to generalize beyond curated environments. Building on these advances, we develop a teacher-student-bootstrap learning framework for vision-based humanoid loco-manipulation, using articulated-object interaction as a representative high-difficulty benchmark. Our approach introduces a staged-reset exploration strategy that stabilizes long-horizon privileged-policy training, and a GRPO-based fine-tuning procedure that mitigates partial observability and improves closed-loop consistency in sim-to-real RL. Trained entirely on simulation data, the resulting policy achieves robust zero-shot performance across diverse door types and outperforms human teleoperators by up to 31.7% in task completion time under the same whole-body control stack. This represents the first humanoid sim-to-real policy capable of diverse articulated loco-manipulation using pure RGB perception.",
    "authors": [
      "Haoru Xue",
      "Tairan He",
      "Zi Wang",
      "Qingwei Ben",
      "Wenli Xiao",
      "Zhengyi Luo",
      "Xingye Da",
      "Fernando Casta\u00f1eda",
      "Guanya Shi",
      "Shankar Sastry",
      "Linxi \"Jim\" Fan",
      "Yuke Zhu"
    ],
    "published": "2025-11-30T20:07:13+00:00",
    "url": "https://arxiv.org/pdf/2512.01061v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01059v1",
    "title": "Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction",
    "abstract": "Although scaling laws and many empirical results suggest that increasing the size of Vision Transformers often improves performance, model accuracy and training behavior are not always monotonically increasing with scale. Focusing on ViT-B/16 trained on ImageNet-1K, we study two simple parameter-reduction strategies applied to the MLP blocks, each removing 32.7\\% of the baseline parameters. Our \\emph{GroupedMLP} variant shares MLP weights between adjacent transformer blocks and achieves 81.47\\% top-1 accuracy while maintaining the baseline computational cost. Our \\emph{ShallowMLP} variant halves the MLP hidden dimension and reaches 81.25\\% top-1 accuracy with a 38\\% increase in inference throughput. Both models outperform the 86.6M-parameter baseline (81.05\\%) and exhibit substantially improved training stability, reducing peak-to-final accuracy degradation from 0.47\\% to the range 0.03\\% to 0.06\\%. These results suggest that, for ViT-B/16 on ImageNet-1K with a standard training recipe, the model operates in an overparameterized regime in which MLP capacity can be reduced without harming performance and can even slightly improve it. More broadly, our findings suggest that architectural constraints such as parameter sharing and reduced width may act as useful inductive biases, and highlight the importance of how parameters are allocated when designing Vision Transformers. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps.",
    "authors": [
      "Anantha Padmanaban Krishna Kumar"
    ],
    "published": "2025-11-30T20:04:30+00:00",
    "url": "https://arxiv.org/pdf/2512.01059v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01054v1",
    "title": "Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs",
    "abstract": "Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages.   We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs.   We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set.   Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.",
    "authors": [
      "MohammadParsa Dini",
      "Human Jafari",
      "Sajjad Amini",
      "MohammadMahdi Mojahedian"
    ],
    "published": "2025-11-30T19:57:49+00:00",
    "url": "https://arxiv.org/pdf/2512.01054v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01048v1",
    "title": "TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models",
    "abstract": "Vision-language models (VLMs) have made great strides in addressing temporal understanding tasks, which involve characterizing visual changes across a sequence of images. However, recent works have suggested that when making predictions, VLMs may rely on static feature biases, such as background or object features, rather than dynamic visual changes. Static feature biases are a type of shortcut and can contribute to systematic prediction errors on downstream tasks; as a result, identifying and characterizing error-inducing static feature biases is critical prior to real-world model deployment. In this work, we introduce TRoVe, an automated approach for discovering error-inducing static feature biases learned by temporal VLMs. Given a trained VLM and an annotated validation dataset associated with a downstream classification task, TRoVe extracts candidate static features from the dataset and scores each feature by (i) the effect of the feature on classification errors as well as (ii) the extent to which the VLM relies on the feature when making predictions. In order to quantitatively evaluate TRoVe, we introduce an evaluation framework consisting of 101 trained temporal VLMs paired with ground-truth annotations for learned static feature biases. We use this framework to demonstrate that TRoVe can accurately identify error-inducing static feature biases in VLMs, achieving a 28.6% improvement over the closest baseline. Finally, we apply TRoVe to 7 off-the-shelf VLMs and 2 temporal understanding tasks, surfacing previously-unknown static feature biases and demonstrating that knowledge of learned biases can aid in improving model performance at test time. Our code is available at https://github.com/Stanford-AIMI/TRoVe.",
    "authors": [
      "Maya Varma",
      "Jean-Benoit Delbrouck",
      "Sophie Ostmeier",
      "Akshay Chaudhari",
      "Curtis Langlotz"
    ],
    "published": "2025-11-30T19:36:46+00:00",
    "url": "https://arxiv.org/pdf/2512.01048v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01047v1",
    "title": "Automating the Refinement of Reinforcement Learning Specifications",
    "abstract": "Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \\textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \\textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \\textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \\textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \\textsc{AutoSpec} are utilized.",
    "authors": [
      "Tanmay Ambadkar",
      "\u0110or\u0111e \u017dikeli\u0107",
      "Abhinav Verma"
    ],
    "published": "2025-11-30T19:32:33+00:00",
    "url": "https://arxiv.org/pdf/2512.01047v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01046v1",
    "title": "Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids",
    "abstract": "Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.",
    "authors": [
      "Hadi Nekoei",
      "Alexandre Blondin Mass\u00e9",
      "Rachid Hassani",
      "Sarath Chandar",
      "Vincent Mai"
    ],
    "published": "2025-11-30T19:28:34+00:00",
    "url": "https://arxiv.org/pdf/2512.01046v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01045v1",
    "title": "Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal",
    "abstract": "The scarcity of high-quality, logically annotated video datasets remains a primary bottleneck in advancing Multi-Modal Large Language Models (MLLMs) for the medical domain. Traditional manual annotation is prohibitively expensive and non-scalable, while existing synthetic methods often suffer from stochastic hallucinations and a lack of logical interpretability. To address these challenges, we introduce \\textbf{\\PipelineName}, a novel neuro-symbolic data engineering framework that formalizes benchmark synthesis as a deterministic graph traversal process. Unlike black-box generative approaches, Med-CRAFT extracts structured visual primitives (e.g., surgical instruments, anatomical boundaries) from raw video streams and instantiates them into a dynamic Spatiotemporal Knowledge Graph. By anchoring query generation to valid paths within this graph, we enforce a rigorous Chain-of-Thought (CoT) provenance for every synthesized benchmark item. We instantiate this pipeline to produce M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity. Comprehensive evaluations demonstrate that our automated pipeline generates query workloads with complexity comparable to expert-curated datasets. Furthermore, a logic alignment analysis reveals a high correlation between the prescribed graph topology and the reasoning steps of state-of-the-art MLLMs, validating the system's capability to encode verifiable logic into visual-linguistic benchmarks. This work paves the way for scalable, low-cost construction of robust evaluation protocols in critical domains.",
    "authors": [
      "Shenxi Liu",
      "Kan Li",
      "Mingyang Zhao",
      "Yuhang Tian",
      "Shoujun Zhou",
      "Bin Li"
    ],
    "published": "2025-11-30T19:24:10+00:00",
    "url": "https://arxiv.org/pdf/2512.01045v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01038v1",
    "title": "FMTK: A Modular Toolkit for Composable Time Series Foundation Model Pipelines",
    "abstract": "Foundation models (FMs) have opened new avenues for machine learning applications due to their ability to adapt to new and unseen tasks with minimal or no further training. Time-series foundation models (TSFMs) -- FMs trained on time-series data -- have shown strong performance on classification, regression, and imputation tasks. Recent pipelines combine TSFMs with task-specific encoders, decoders, and adapters to improve performance; however, assembling such pipelines typically requires ad hoc, model-specific implementations that hinder modularity and reproducibility. We introduce FMTK, an open-source, lightweight and extensible toolkit for constructing and fine-tuning TSFM pipelines via standardized backbone and component abstractions. FMTK enables flexible composition across models and tasks, achieving correctness and performance with an average of seven lines of code. https://github.com/umassos/FMTK",
    "authors": [
      "Hetvi Shastri",
      "Pragya Sharma",
      "Walid A. Hanafy",
      "Mani Srivastava",
      "Prashant Shenoy"
    ],
    "published": "2025-11-30T19:14:04+00:00",
    "url": "https://arxiv.org/pdf/2512.01038v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01037v1",
    "title": "When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals",
    "abstract": "Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce \"semantic confusion,\" a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.",
    "authors": [
      "Riad Ahmed Anonto",
      "Md Labid Al Nahiyan",
      "Md Tanvir Hassan",
      "Ch. Md. Rakin Haider"
    ],
    "published": "2025-11-30T19:11:45+00:00",
    "url": "https://arxiv.org/pdf/2512.01037v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01035v1",
    "title": "Goal-Oriented Multi-Agent Semantic Networking: Unifying Intents, Semantics, and Intelligence",
    "abstract": "6G services are evolving toward goal-oriented and AI-native communication, which are expected to deliver transformative societal benefits across various industries and promote energy sustainability. Yet today's networking architectures, built on complete decoupling of the applications and the network, cannot expose or exploit high-level goals, limiting their ability to adapt intelligently to service needs. This work introduces Goal-Oriented Multi-Agent Semantic Networking (GoAgentNet), a new architecture that elevates communication from data exchange to goal fulfilment. GoAgentNet enables applications and the network to collaborate by abstracting their functions into multiple collaborative agents, and jointly orchestrates multi-agent sensing, networking, computation, and control through semantic computation and cross-layer semantic networking, allowing the entire architecture to pursue unified application goals. We first outline the limitations of legacy network designs in supporting 6G services, based on which we highlight key enablers of our GoAgentNet design. Then, through three representative 6G usage scenarios, we demonstrate how GoAgentNet can unlock more efficient and intelligent services. We further identify unique challenges faced by GoAgentNet deployment and corresponding potential solutions. A case study on robotic fault detection and recovery shows that our GoAgentNet architecture improves energy efficiency by up to 99% and increases the task success rate by up to 72%, compared with the existing networking architectures without GoAgentNet, which underscores its potential to support scalable and sustainable 6G systems.",
    "authors": [
      "Shutong Chen",
      "Qi Liao",
      "Adnan Aijaz",
      "Yansha Deng"
    ],
    "published": "2025-11-30T19:04:17+00:00",
    "url": "https://arxiv.org/pdf/2512.01035v1",
    "categories": [
      "cs.NI",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01034v1",
    "title": "AltNet: Addressing the Plasticity-Stability Dilemma in Reinforcement Learning",
    "abstract": "Neural networks have shown remarkable success in supervised learning when trained on a single task using a fixed dataset. However, when neural networks are trained on a reinforcement learning task, their ability to continue learning from new experiences declines over time. This decline in learning ability is known as plasticity loss. To restore plasticity, prior work has explored periodically resetting the parameters of the learning network, a strategy that often improves overall performance. However, such resets come at the cost of a temporary drop in performance, which can be dangerous in real-world settings. To overcome this instability, we introduce AltNet, a reset-based approach that restores plasticity without performance degradation by leveraging twin networks. The use of twin networks anchors performance during resets through a mechanism that allows networks to periodically alternate roles: one network learns as it acts in the environment, while the other learns off-policy from the active network's interactions and a replay buffer. At fixed intervals, the active network is reset and the passive network, having learned from prior experiences, becomes the new active network. AltNet restores plasticity, improving sample efficiency and achieving higher performance, while avoiding performance drops that pose risks in safety-critical settings. We demonstrate these advantages in several high-dimensional control tasks from the DeepMind Control Suite, where AltNet outperforms various relevant baseline methods, as well as state-of-the-art reset-based techniques.",
    "authors": [
      "Mansi Maheshwari",
      "John C. Raisbeck",
      "Bruno Castro da Silva"
    ],
    "published": "2025-11-30T19:02:20+00:00",
    "url": "https://arxiv.org/pdf/2512.01034v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01033v1",
    "title": "Associative Syntax and Maximal Repetitions reveal context-dependent complexity in fruit bat communication",
    "abstract": "This study presents an unsupervised method to infer discreteness, syntax and temporal structures of fruit-bats vocalizations, as a case study of graded vocal systems, and evaluates the complexity of communication patterns in relation with behavioral context. The method improved the baseline for unsupervised labeling of vocal units (i.e. syllables) through manifold learning, by investigating how dimen- sionality reduction on mel-spectrograms affects labeling, and comparing it with unsupervised labels based on acoustic similarity. We then encoded vocalizations as syllabic sequences to analyze the type of syntax, and extracted the Maximal Repetitions (MRs) to evaluate syntactical structures. We found evidence for: i) associative syntax, rather than combinatorial (context classification is unaffected by permutation of sequences, F 1 > 0.9); ii) context-dependent use of syllables (Wilcoxon rank-sum tests, p-value < 0.05); iii) heavy-tail distribution of MRs (truncated power-law, exponent \u03b1 < 2), indicative of mechanism encoding com- binatorial complexity. Analysis of MRs and syllabic transition networks revealed that mother-pupil interactions were characterized by repetitions, while commu- nication in conflict-contexts exhibited higher complexity (longer MRs and more interconnected vocal sequences) than non-agonistic contexts. We propose that communicative complexity is higher in scenarios of disagreement, reflecting lower compressibility of information.",
    "authors": [
      "Luigi Assom"
    ],
    "published": "2025-11-30T19:01:59+00:00",
    "url": "https://arxiv.org/pdf/2512.01033v1",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.IT",
      "q-bio.QM"
    ]
  },
  {
    "arxiv_id": "2512.01031v1",
    "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
    "abstract": "Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash",
    "authors": [
      "Jiaming Tang",
      "Yufei Sun",
      "Yilong Zhao",
      "Shang Yang",
      "Yujun Lin",
      "Zhuoyang Zhang",
      "James Hou",
      "Yao Lu",
      "Zhijian Liu",
      "Song Han"
    ],
    "published": "2025-11-30T18:59:24+00:00",
    "url": "https://arxiv.org/pdf/2512.01031v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.01030v1",
    "title": "Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model",
    "abstract": "Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.",
    "authors": [
      "Jing He",
      "Haodong Li",
      "Mingzhi Sheng",
      "Ying-Cong Chen"
    ],
    "published": "2025-11-30T18:57:25+00:00",
    "url": "https://arxiv.org/pdf/2512.01030v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01025v1",
    "title": "Operator-Theoretic Framework for Gradient-Free Federated Learning",
    "abstract": "Federated learning must address heterogeneity, strict communication and computation limits, and privacy while ensuring performance. We propose an operator-theoretic framework that maps the $L^2$-optimal solution into a reproducing kernel Hilbert space (RKHS) via a forward operator, approximates it using available data, and maps back with the inverse operator, yielding a gradient-free scheme. Finite-sample bounds are derived using concentration inequalities over operator norms, and the framework identifies a data-dependent hypothesis space with guarantees on risk, error, robustness, and approximation. Within this space we design efficient kernel machines leveraging the space folding property of Kernel Affine Hull Machines. Clients transfer knowledge via a scalar space folding measure, reducing communication and enabling a simple differentially private protocol: summaries are computed from noise-perturbed data matrices in one step, avoiding per-round clipping and privacy accounting. The induced global rule requires only integer minimum and equality-comparison operations per test point, making it compatible with fully homomorphic encryption (FHE). Across four benchmarks, the gradient-free FL method with fixed encoder embeddings matches or outperforms strong gradient-based fine-tuning, with gains up to 23.7 points. In differentially private experiments, kernel smoothing mitigates accuracy loss in high-privacy regimes. The global rule admits an FHE realization using $Q \\times C$ encrypted minimum and $C$ equality-comparison operations per test point, with operation-level benchmarks showing practical latencies. Overall, the framework provides provable guarantees with low communication, supports private knowledge transfer via scalar summaries, and yields an FHE-compatible prediction rule offering a mathematically grounded alternative to gradient-based federated learning under heterogeneity.",
    "authors": [
      "Mohit Kumar",
      "Mathias Brucker",
      "Alexander Valentinitsch",
      "Adnan Husakovic",
      "Ali Abbas",
      "Manuela Gei\u00df",
      "Bernhard A. Moser"
    ],
    "published": "2025-11-30T18:49:00+00:00",
    "url": "https://arxiv.org/pdf/2512.01025v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01020v1",
    "title": "Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics",
    "abstract": "Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.",
    "authors": [
      "Jinu Lee",
      "Kyoung-Woon On",
      "Simeng Han",
      "Arman Cohan",
      "Julia Hockenmaier"
    ],
    "published": "2025-11-30T18:32:43+00:00",
    "url": "https://arxiv.org/pdf/2512.01020v1",
    "categories": [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.01017v1",
    "title": "ChartAnchor: Chart Grounding with Structural-Semantic Fidelity",
    "abstract": "Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension.Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios.Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.",
    "authors": [
      "Xinhang Li",
      "Jingbo Zhou",
      "Pengfei Luo",
      "Yixiong Xiao",
      "Tong Xu"
    ],
    "published": "2025-11-30T18:28:09+00:00",
    "url": "https://arxiv.org/pdf/2512.01017v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.01010v1",
    "title": "Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis",
    "abstract": "Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\\times10^{-3}$ %), with a $\\sim$33.4 % faster runtime and a $\\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.",
    "authors": [
      "Vansh Sharma",
      "Venkat Raman"
    ],
    "published": "2025-11-30T18:16:50+00:00",
    "url": "https://arxiv.org/pdf/2512.01010v1",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "cs.SE",
      "physics.comp-ph",
      "physics.flu-dyn"
    ]
  },
  {
    "arxiv_id": "2512.01009v1",
    "title": "FOM-Nav: Frontier-Object Maps for Object Goal Navigation",
    "abstract": "This paper addresses the Object Goal Navigation problem, where a robot must efficiently find a target object in an unknown environment. Existing implicit memory-based methods struggle with long-term memory retention and planning, while explicit map-based approaches lack rich semantic information. To address these challenges, we propose FOM-Nav, a modular framework that enhances exploration efficiency through Frontier-Object Maps and vision-language models. Our Frontier-Object Maps are built online and jointly encode spatial frontiers and fine-grained object information. Using this representation, a vision-language model performs multimodal scene understanding and high-level goal prediction, which is executed by a low-level planner for efficient trajectory generation. To train FOM-Nav, we automatically construct large-scale navigation datasets from real-world scanned environments. Extensive experiments validate the effectiveness of our model design and constructed dataset. FOM-Nav achieves state-of-the-art performance on the MP3D and HM3D benchmarks, particularly in navigation efficiency metric SPL, and yields promising results on a real robot.",
    "authors": [
      "Thomas Chabal",
      "Shizhe Chen",
      "Jean Ponce",
      "Cordelia Schmid"
    ],
    "published": "2025-11-30T18:16:09+00:00",
    "url": "https://arxiv.org/pdf/2512.01009v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.01008v1",
    "title": "LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency",
    "abstract": "Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.",
    "authors": [
      "Zhongbin Guo",
      "Jiahe Liu",
      "Wenyu Gao",
      "Yushan Li",
      "Chengzhi Li",
      "Ping Jian"
    ],
    "published": "2025-11-30T18:02:14+00:00",
    "url": "https://arxiv.org/pdf/2512.01008v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00999v1",
    "title": "Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints",
    "abstract": "Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.",
    "authors": [
      "Mohsin Rasheed",
      "Abdullah Al-Mamun"
    ],
    "published": "2025-11-30T17:48:55+00:00",
    "url": "https://arxiv.org/pdf/2512.00999v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00997v1",
    "title": "IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch",
    "abstract": "We introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 312 formal Lean 4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Through category-based retrieval, iterative compiler feedback, and multi-model ensembles, our pipeline generates candidate formalizations that experts efficiently validate via an interactive dashboard with automated quality summaries. Evaluation across multiple frontier models demonstrates that autoformalization remains challenging, with substantial gaps between syntactic validity and semantic correctness, while theorem proving success rates remain low even with iterative refinement, demonstrating that \\benchmark~presents a challenging testbed for mathematical reasoning. IndiMathBench is available at https://github.com/prmbiy/IndiMathBench.",
    "authors": [
      "Param Biyani",
      "Shashank Kirtania",
      "Yasharth Bajpai",
      "Sumit Gulwani",
      "Ashish Tiwari"
    ],
    "published": "2025-11-30T17:40:13+00:00",
    "url": "https://arxiv.org/pdf/2512.00997v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00995v1",
    "title": "S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud",
    "abstract": "Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.",
    "authors": [
      "Han Su",
      "Tianyu Huang",
      "Zichen Wan",
      "Xiaohe Wu",
      "Wangmeng Zuo"
    ],
    "published": "2025-11-30T17:32:54+00:00",
    "url": "https://arxiv.org/pdf/2512.00995v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00993v1",
    "title": "PhotoFramer: Multi-modal Image Composition Instruction",
    "abstract": "Composition matters during the photo-taking process, yet many casual users struggle to frame well-composed images. To provide composition guidance, we introduce PhotoFramer, a multi-modal composition instruction framework. Given a poorly composed image, PhotoFramer first describes how to improve the composition in natural language and then generates a well-composed example image. To train such a model, we curate a large-scale dataset. Inspired by how humans take photos, we organize composition guidance into a hierarchy of sub-tasks: shift, zoom-in, and view-change tasks. Shift and zoom-in data are sampled from existing cropping datasets, while view-change data are obtained via a two-stage pipeline. First, we sample pairs with varying viewpoints from multi-view datasets, and train a degradation model to transform well-composed photos into poorly composed ones. Second, we apply this degradation model to expert-taken photos to synthesize poor images to form training pairs. Using this dataset, we finetune a model that jointly processes and generates both text and images, enabling actionable textual guidance with illustrative examples. Extensive experiments demonstrate that textual instructions effectively steer image composition, and coupling them with exemplars yields consistent improvements over exemplar-only baselines. PhotoFramer offers a practical step toward composition assistants that make expert photographic priors accessible to everyday users. Codes, model weights, and datasets have been released in https://zhiyuanyou.github.io/photoframer.",
    "authors": [
      "Zhiyuan You",
      "Ke Wang",
      "He Zhang",
      "Xin Cai",
      "Jinjin Gu",
      "Tianfan Xue",
      "Chao Dong",
      "Zhoutong Zhang"
    ],
    "published": "2025-11-30T17:26:47+00:00",
    "url": "https://arxiv.org/pdf/2512.00993v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00991v1",
    "title": "Advancing Academic Chatbots: Evaluation of Non Traditional Outputs",
    "abstract": "Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.",
    "authors": [
      "Nicole Favero",
      "Francesca Salute",
      "Daniel Hardt"
    ],
    "published": "2025-11-30T17:25:23+00:00",
    "url": "https://arxiv.org/pdf/2512.00991v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00986v1",
    "title": "Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent",
    "abstract": "The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.",
    "authors": [
      "Zhihan Guo",
      "Feiyang Xu",
      "Yifan Li",
      "Muzhi Li",
      "Shuai Zou",
      "Jiele Wu",
      "Han Shi",
      "Haoli Bai",
      "Ho-fung Leung",
      "Irwin King"
    ],
    "published": "2025-11-30T17:16:47+00:00",
    "url": "https://arxiv.org/pdf/2512.00986v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.02076v1",
    "title": "FDRMFL:Multi-modal Federated Feature Extraction Model Based on Information Maximization and Contrastive Learning",
    "abstract": "This study focuses on the feature extraction problem in multi-modal data regression. To address three core challenges in real-world scenarios: limited and non-IID data, effective extraction and fusion of multi-modal information, and susceptibility to catastrophic forgetting in model learning, a task-driven supervised multi-modal federated feature extraction method is proposed. The method integrates multi-modal information extraction and contrastive learning mechanisms, and can adapt to different neural network structures as the latent mapping functions for data of each modality. It supports each client to independently learn low-dimensional representations of multi-modal data, and can flexibly control the degree of retention of effective information about the response variable in the predictive variables within the low-dimensional features through parameter tuning. The multi-constraint learning framework constructed by the method guarantees regression accuracy using Mean Squared Error loss. Through the synergistic effect of mutual information preservation constraint, symmetric Kullback-Leibler divergence constraint, and inter-model contrastive constraint, it achieves the retention of task-related information, the extraction, fusion, and alignment of multi-modal features, and the mitigation of representation drift and catastrophic forgetting in non-IID scenarios, respectively. This ensures that the feature extraction process always centers on improving the performance of downstream regression tasks. Experimental results from simulations and real-world data analysis demonstrate that the proposed method achieves more significant performance improvement on downstream regression tasks compared with classical feature extraction techniques.",
    "authors": [
      "Haozhe Wu"
    ],
    "published": "2025-11-30T17:13:35+00:00",
    "url": "https://arxiv.org/pdf/2512.02076v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00979v1",
    "title": "An Approach to Variable Clustering: K-means in Transposed Data and its Relationship with Principal Component Analysis",
    "abstract": "Principal Component Analysis (PCA) and K-means constitute fundamental techniques in multivariate analysis. Although they are frequently applied independently or sequentially to cluster observations, the relationship between them, especially when K-means is used to cluster variables rather than observations, has been scarcely explored. This study seeks to address this gap by proposing an innovative method that analyzes the relationship between clusters of variables obtained by applying K-means on transposed data and the principal components of PCA. Our approach involves applying PCA to the original data and K-means to the transposed data set, where the original variables are converted into observations. The contribution of each variable cluster to each principal component is then quantified using measures based on variable loadings. This process provides a tool to explore and understand the clustering of variables and how such clusters contribute to the principal dimensions of variation identified by PCA.",
    "authors": [
      "Victor Saquicela",
      "Kenneth Palacio-Baus",
      "Mario Chifla"
    ],
    "published": "2025-11-30T16:53:07+00:00",
    "url": "https://arxiv.org/pdf/2512.00979v1",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00975v1",
    "title": "MM-ACT: Learn from Multimodal Parallel Generation to Act",
    "abstract": "A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.",
    "authors": [
      "Haotian Liang",
      "Xinyi Chen",
      "Bin Wang",
      "Mingkang Chen",
      "Yitian Liu",
      "Yuhao Zhang",
      "Zanxin Chen",
      "Tianshuo Yang",
      "Yilun Chen",
      "Jiangmiao Pang",
      "Dong Liu",
      "Xiaokang Yang",
      "Yao Mu",
      "Wenqi Shao",
      "Ping Luo"
    ],
    "published": "2025-11-30T16:46:35+00:00",
    "url": "https://arxiv.org/pdf/2512.00975v1",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.00969v1",
    "title": "Integrating Causal Foundation Model in Prescriptive Maintenance Framework for Optimizing Production Line OEE",
    "abstract": "The transition to prescriptive maintenance in manufacturing is critically constrained by a dependence on predictive models. These models tend to rely on spurious correlations rather than identifying the true causal drivers of failures, often leading to costly misdiagnoses and ineffective interventions. This fundamental limitation results in a key-challenge: while we can predict that a failure may occur, we lack a systematic method to understand why a failure occurs, thereby providing the basis for identifying the most effective intervention. This paper proposes a model based on causal machine learning to bridge this gap. Our objective is to move beyond diagnosis to active prescription by simulating and evaluating potential fixes toward optimizing KPIs such as Overall Equipment Effectiveness (OEE). For this purpose a pre-trained causal foundation model is used as a \"what-if\" model to estimate the effects of potential fixes. By measuring the causal effect of each intervention on system-level KPIs, it provides a data-driven ranking of actions to recommend at the production line. This process not only identifies root causes but also quantifies their operational impact. The model is evaluated using semi-synthetic manufacturing data and compared with a baseline machine learning model. This paper sets the technical basis for a robust prescriptive maintenance framework, allowing engineers to test potential solutions in a causal environment to make more effective operational decisions and reduce costly downtimes.",
    "authors": [
      "Felix Saretzky",
      "Lucas Andersen",
      "Thomas Engel",
      "Fazel Ansari"
    ],
    "published": "2025-11-30T16:33:30+00:00",
    "url": "https://arxiv.org/pdf/2512.00969v1",
    "categories": [
      "cs.AI",
      "eess.SY"
    ]
  },
  {
    "arxiv_id": "2512.00968v1",
    "title": "Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search",
    "abstract": "Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive experiments on industrial datasets, along with online A/B tests, demonstrate the effectiveness of our approach.",
    "authors": [
      "Ziyang Zeng",
      "Heming Jing",
      "Jindong Chen",
      "Xiangli Li",
      "Hongyu Liu",
      "Yixuan He",
      "Zhengyu Li",
      "Yige Sun",
      "Zheyong Xie",
      "Yuqing Yang",
      "Shaosheng Cao",
      "Jun Fan",
      "Yi Wu",
      "Yao Hu"
    ],
    "published": "2025-11-30T16:31:16+00:00",
    "url": "https://arxiv.org/pdf/2512.00968v1",
    "categories": [
      "cs.IR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00960v1",
    "title": "Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction",
    "abstract": "Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/",
    "authors": [
      "Boran Wen",
      "Ye Lu",
      "Keyan Wan",
      "Sirui Wang",
      "Jiahong Zhou",
      "Junxuan Liang",
      "Xinpeng Liu",
      "Bang Xiao",
      "Dingbang Huang",
      "Ruiyang Liu",
      "Yong-Lu Li"
    ],
    "published": "2025-11-30T16:21:47+00:00",
    "url": "https://arxiv.org/pdf/2512.00960v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00953v1",
    "title": "Adaptive Evidential Learning for Temporal-Semantic Robustness in Moment Retrieval",
    "abstract": "In the domain of moment retrieval, accurately identifying temporal segments within videos based on natural language queries remains challenging. Traditional methods often employ pre-trained models that struggle with fine-grained information and deterministic reasoning, leading to difficulties in aligning with complex or ambiguous moments. To overcome these limitations, we explore Deep Evidential Regression (DER) to construct a vanilla Evidential baseline. However, this approach encounters two major issues: the inability to effectively handle modality imbalance and the structural differences in DER's heuristic uncertainty regularizer, which adversely affect uncertainty estimation. This misalignment results in high uncertainty being incorrectly associated with accurate samples rather than challenging ones. Our observations indicate that existing methods lack the adaptability required for complex video scenarios. In response, we propose Debiased Evidential Learning for Moment Retrieval (DEMR), a novel framework that incorporates a Reflective Flipped Fusion (RFF) block for cross-modal alignment and a query reconstruction task to enhance text sensitivity, thereby reducing bias in uncertainty estimation. Additionally, we introduce a Geom-regularizer to refine uncertainty predictions, enabling adaptive alignment with difficult moments and improving retrieval accuracy. Extensive testing on standard datasets and debiased datasets ActivityNet-CD and Charades-CD demonstrates significant enhancements in effectiveness, robustness, and interpretability, positioning our approach as a promising solution for temporal-semantic robustness in moment retrieval. The code is publicly available at https://github.com/KaijingOfficial/DEMR.",
    "authors": [
      "Haojian Huang",
      "Kaijing Ma",
      "Jin Chen",
      "Haodong Chen",
      "Zhou Wu",
      "Xianghao Zang",
      "Han Fang",
      "Chao Ban",
      "Hao Sun",
      "Mulin Chen",
      "Zhongjiang He"
    ],
    "published": "2025-11-30T16:13:20+00:00",
    "url": "https://arxiv.org/pdf/2512.00953v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00949v1",
    "title": "Multi-Modal AI for Remote Patient Monitoring in Cancer Care",
    "abstract": "For patients undergoing systemic cancer therapy, the time between clinic visits is full of uncertainties and risks of unmonitored side effects. To bridge this gap in care, we developed and prospectively trialed a multi-modal AI framework for remote patient monitoring (RPM). This system integrates multi-modal data from the HALO-X platform, such as demographics, wearable sensors, daily surveys, and clinical events. Our observational trial is one of the largest of its kind and has collected over 2.1 million data points (6,080 patient-days) of monitoring from 84 patients. We developed and adapted a multi-modal AI model to handle the asynchronous and incomplete nature of real-world RPM data, forecasting a continuous risk of future adverse events. The model achieved an accuracy of 83.9% (AUROC=0.70). Notably, the model identified previous treatments, wellness check-ins, and daily maximum heart rate as key predictive features. A case study demonstrated the model's ability to provide early warnings by outputting escalating risk profiles prior to the event. This work establishes the feasibility of multi-modal AI RPM for cancer care and offers a path toward more proactive patient support.(Accepted at Europe NeurIPS 2025 Multimodal Representation Learning for Healthcare Workshop)",
    "authors": [
      "Yansong Liu",
      "Ronnie Stafford",
      "Pramit Khetrapal",
      "Huriye Kocadag",
      "Gra\u00e7a Carvalho",
      "Patricia de Winter",
      "Maryam Imran",
      "Amelia Snook",
      "Adamos Hadjivasiliou",
      "D. Vijay Anand",
      "Weining Lin",
      "John Kelly",
      "Yukun Zhou",
      "Ivana Drobnjak"
    ],
    "published": "2025-11-30T16:01:50+00:00",
    "url": "https://arxiv.org/pdf/2512.00949v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00947v1",
    "title": "Table as a Modality for Large Language Models",
    "abstract": "To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.",
    "authors": [
      "Liyao Li",
      "Chao Ye",
      "Wentao Ye",
      "Yifei Sun",
      "Zhe Jiang",
      "Haobo Wang",
      "Jiaming Tian",
      "Yiming Zhang",
      "Ningtao Wang",
      "Xing Fu",
      "Gang Chen",
      "Junbo Zhao"
    ],
    "published": "2025-11-30T15:59:56+00:00",
    "url": "https://arxiv.org/pdf/2512.00947v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00946v1",
    "title": "Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data",
    "abstract": "Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.",
    "authors": [
      "Alvaro Paredes Amorin",
      "Andre Python",
      "Christoph Weisser"
    ],
    "published": "2025-11-30T15:58:22+00:00",
    "url": "https://arxiv.org/pdf/2512.00946v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00944v1",
    "title": "Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation",
    "abstract": "3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.",
    "authors": [
      "An Yang",
      "Chenyu Liu",
      "Jun Du",
      "Jianqing Gao",
      "Jia Pan",
      "Jinshui Hu",
      "Baocai Yin",
      "Bing Yin",
      "Cong Liu"
    ],
    "published": "2025-11-30T15:51:30+00:00",
    "url": "https://arxiv.org/pdf/2512.00944v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00939v1",
    "title": "Constant-Time Motion Planning with Manipulation Behaviors",
    "abstract": "Recent progress in contact-rich robotic manipulation has been striking, yet most deployed systems remain confined to simple, scripted routines. One of the key barriers is the lack of motion planning algorithms that can provide verifiable guarantees for safety, efficiency and reliability. To address this, a family of algorithms called Constant-Time Motion Planning (CTMP) was introduced, which leverages a preprocessing phase to enable collision-free motion queries in a fixed, user-specified time budget (e.g., 10 milliseconds). However, existing CTMP methods do not explicitly incorporate the manipulation behaviors essential for object handling. To bridge this gap, we introduce the \\textit{Behavioral Constant-Time Motion Planner} (B-CTMP), an algorithm that extends CTMP to solve a broad class of two-step manipulation tasks: (1) a collision-free motion to a behavior initiation state, followed by (2) execution of a manipulation behavior (such as grasping or insertion) to reach the goal. By precomputing compact data structures, B-CTMP guarantees constant-time query in mere milliseconds while ensuring completeness and successful task execution over a specified set of states. We evaluate B-CTMP on two canonical manipulation tasks in simulation, shelf picking and plug insertion,and demonstrate its effectiveness on a real robot. Our results show that B-CTMP unifies collision-free planning and object manipulation within a single constant-time framework, providing provable guarantees of speed and success for manipulation in semi-structured environments.",
    "authors": [
      "Nayesha Gandotra",
      "Itamar Mishani",
      "Maxim Likhachev"
    ],
    "published": "2025-11-30T15:42:35+00:00",
    "url": "https://arxiv.org/pdf/2512.00939v1",
    "categories": [
      "cs.RO",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00938v1",
    "title": "DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics",
    "abstract": "Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.   We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.   The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the \"what,\" \"how,\" and \"why\" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.",
    "authors": [
      "Ahmed Mustafa Younes"
    ],
    "published": "2025-11-30T15:39:28+00:00",
    "url": "https://arxiv.org/pdf/2512.00938v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00936v1",
    "title": "SceneProp: Combining Neural Network and Markov Random Field for Scene-Graph Grounding",
    "abstract": "Grounding complex, compositional visual queries with multiple objects and relationships is a fundamental challenge for vision-language models. While standard phrase grounding methods excel at localizing single objects, they lack the structural inductive bias to parse intricate relational descriptions, often failing as queries become more descriptive. To address this structural deficit, we focus on scene-graph grounding, a powerful but less-explored formulation where the query is an explicit graph of objects and their relationships. However, existing methods for this task also struggle, paradoxically showing decreased performance as the query graph grows -- failing to leverage the very information that should make grounding easier. We introduce SceneProp, a novel method that resolves this issue by reformulating scene-graph grounding as a Maximum a Posteriori (MAP) inference problem in a Markov Random Field (MRF). By performing global inference over the entire query graph, SceneProp finds the optimal assignment of image regions to nodes that jointly satisfies all constraints. This is achieved within an end-to-end framework via a differentiable implementation of the Belief Propagation algorithm. Experiments on four benchmarks show that our dedicated focus on the scene-graph grounding formulation allows SceneProp to significantly outperform prior work. Critically, its accuracy consistently improves with the size and complexity of the query graph, demonstrating for the first time that more relational context can, and should, lead to better grounding. Codes are available at https://github.com/keitaotani/SceneProp.",
    "authors": [
      "Keita Otani",
      "Tatsuya Harada"
    ],
    "published": "2025-11-30T15:35:38+00:00",
    "url": "https://arxiv.org/pdf/2512.00936v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00931v1",
    "title": "Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study",
    "abstract": "Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.",
    "authors": [
      "Imane Jaaouine",
      "Ross D. King"
    ],
    "published": "2025-11-30T15:19:41+00:00",
    "url": "https://arxiv.org/pdf/2512.00931v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00927v1",
    "title": "LAHNet: Local Attentive Hashing Network for Point Cloud Registration",
    "abstract": "Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.",
    "authors": [
      "Wentao Qu",
      "Xiaoshui Huang",
      "Liang Xiao"
    ],
    "published": "2025-11-30T15:12:31+00:00",
    "url": "https://arxiv.org/pdf/2512.00927v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00920v1",
    "title": "Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios",
    "abstract": "Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering \"How accurate is the RM's preference perception for given samples?\", it employs scientific auditing to answer: \"Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?\". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.",
    "authors": [
      "Jianxiang Zang",
      "Yongda Wei",
      "Ruxue Bai",
      "Shiyu Jiang",
      "Nijia Mo",
      "Binhong Li",
      "Qiang Sun",
      "Hui Liu"
    ],
    "published": "2025-11-30T14:54:12+00:00",
    "url": "https://arxiv.org/pdf/2512.00920v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00918v1",
    "title": "Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models",
    "abstract": "Large Vision-Language Models (LVLMs) have shown impressive multimodal understanding capabilities, yet their robustness is poorly understood. In this paper, we investigate the structural vulnerabilities of LVLMs to identify any critical neurons whose removal triggers catastrophic collapse. In this context, we propose CAN, a method to detect Consistently Activated Neurons and to locate critical neurons by progressive masking. Experiments on LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b reveal that masking only a tiny portion of the language model's feed-forward networks (just as few as four neurons in extreme cases) suffices to trigger catastrophic collapse. Notably, critical neurons are predominantly localized in the language model rather than in the vision components, and the down-projection layer is a particularly vulnerable structure. We also observe a consistent two-stage collapse pattern: initial expressive degradation followed by sudden, complete collapse. Our findings provide important insights for safety research in LVLMs.",
    "authors": [
      "Cen Lu",
      "Yung-Chen Tang",
      "Andrea Cavallaro"
    ],
    "published": "2025-11-30T14:52:11+00:00",
    "url": "https://arxiv.org/pdf/2512.00918v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02074v1",
    "title": "Dialect Identification Using Resource-Efficient Fine-Tuning Approaches",
    "abstract": "Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.",
    "authors": [
      "Zirui Lin",
      "Haris Gulzar",
      "Monnika Roslianna Busto",
      "Akiko Masaki",
      "Takeharu Eda",
      "Kazuhiro Nakadai"
    ],
    "published": "2025-11-30T14:40:27+00:00",
    "url": "https://arxiv.org/pdf/2512.02074v1",
    "categories": [
      "cs.CL",
      "cs.SD"
    ]
  },
  {
    "arxiv_id": "2512.00912v1",
    "title": "ForamDeepSlice: A High-Accuracy Deep Learning Framework for Foraminifera Species Classification from 2D Micro-CT Slices",
    "abstract": "This study presents a comprehensive deep learning pipeline for the automated classification of 12 foraminifera species using 2D micro-CT slices derived from 3D scans. We curated a scientifically rigorous dataset comprising 97 micro-CT scanned specimens across 27 species, selecting 12 species with sufficient representation for robust machine learning. To ensure methodological integrity and prevent data leakage, we employed specimen-level data splitting, resulting in 109,617 high-quality 2D slices (44,103 for training, 14,046 for validation, and 51,468 for testing). We evaluated seven state-of-the-art 2D convolutional neural network (CNN) architectures using transfer learning. Our final ensemble model, combining ConvNeXt-Large and EfficientNetV2-Small, achieved a test accuracy of 95.64%, with a top-3 accuracy of 99.6% and an area under the ROC curve (AUC) of 0.998 across all species. To facilitate practical deployment, we developed an interactive advanced dashboard that supports real-time slice classification and 3D slice matching using advanced similarity metrics, including SSIM, NCC, and the Dice coefficient. This work establishes new benchmarks for AI-assisted micropaleontological identification and provides a fully reproducible framework for foraminifera classification research, bridging the gap between deep learning and applied geosciences.",
    "authors": [
      "Abdelghafour Halimi",
      "Ali Alibrahim",
      "Didier Barradas-Bautista",
      "Ronell Sicat",
      "Abdulkader M. Afifi"
    ],
    "published": "2025-11-30T14:30:16+00:00",
    "url": "https://arxiv.org/pdf/2512.00912v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00911v1",
    "title": "Dual-Projection Fusion for Accurate Upright Panorama Generation in Robotic Vision",
    "abstract": "Panoramic cameras, capable of capturing a 360-degree field of view, are crucial in robotic vision, particularly in environments with sparse features. However, non-upright panoramas due to unstable robot postures hinder downstream tasks. Traditional IMU-based correction methods suffer from drift and external disturbances, while vision-based approaches offer a promising alternative. This study presents a dual-stream angle-aware generation network that jointly estimates camera inclination angles and reconstructs upright panoramic images. The network comprises a CNN branch that extracts local geometric structures from equirectangular projections and a ViT branch that captures global contextual cues from cubemap projections. These are integrated through a dual-projection adaptive fusion module that aligns spatial features across both domains. To further enhance performance, we introduce a high-frequency enhancement block, circular padding, and channel attention mechanisms to preserve 360\u00b0 continuity and improve geometric sensitivity. Experiments on the SUN360 and M3D datasets demonstrate that our method outperforms existing approaches in both inclination estimation and upright panorama generation. Ablation studies further validate the contribution of each module and highlight the synergy between the two tasks. The code and related datasets can be found at: https://github.com/YuhaoShine/DualProjectionFusion.",
    "authors": [
      "Yuhao Shan",
      "Qianyi Yuan",
      "Jingguo Liu",
      "Shigang Li",
      "Jianfeng Li",
      "Tong Chen"
    ],
    "published": "2025-11-30T14:28:21+00:00",
    "url": "https://arxiv.org/pdf/2512.00911v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00909v1",
    "title": "TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model",
    "abstract": "Recent advancements in diffusion models have significantly improved the realism and generalizability of character-driven animation, enabling the synthesis of high-quality motion from just a single RGB image and a set of driving poses. Nevertheless, generating temporally coherent long-form content remains challenging. Existing approaches are constrained by computational and memory limitations, as they are typically trained on short video segments, thus performing effectively only over limited frame lengths and hindering their potential for extended coherent generation. To address these constraints, we propose TalkingPose, a novel diffusion-based framework specifically designed for producing long-form, temporally consistent human upper-body animations. TalkingPose leverages driving frames to precisely capture expressive facial and hand movements, transferring these seamlessly to a target actor through a stable diffusion backbone. To ensure continuous motion and enhance temporal coherence, we introduce a feedback-driven mechanism built upon image-based diffusion models. Notably, this mechanism does not incur additional computational costs or require secondary training stages, enabling the generation of animations with unlimited duration. Additionally, we introduce a comprehensive, large-scale dataset to serve as a new benchmark for human upper-body animation.",
    "authors": [
      "Alireza Javanmardi",
      "Pragati Jaiswal",
      "Tewodros Amberbir Habtegebrial",
      "Christen Millerdurai",
      "Shaoxiang Wang",
      "Alain Pagani",
      "Didier Stricker"
    ],
    "published": "2025-11-30T14:26:24+00:00",
    "url": "https://arxiv.org/pdf/2512.00909v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00908v1",
    "title": "Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.",
    "authors": [
      "Xinzhu Chen",
      "Xuesheng Li",
      "Zhongxiang Sun",
      "Weijie Yu"
    ],
    "published": "2025-11-30T14:19:36+00:00",
    "url": "https://arxiv.org/pdf/2512.00908v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00904v1",
    "title": "Hierarchical Semantic Alignment for Image Clustering",
    "abstract": "Image clustering is a classic problem in computer vision, which categorizes images into different groups. Recent studies utilize nouns as external semantic knowledge to improve clus- tering performance. However, these methods often overlook the inherent ambiguity of nouns, which can distort semantic representations and degrade clustering quality. To address this issue, we propose a hierarChical semAntic alignmEnt method for image clustering, dubbed CAE, which improves cluster- ing performance in a training-free manner. In our approach, we incorporate two complementary types of textual seman- tics: caption-level descriptions, which convey fine-grained attributes of image content, and noun-level concepts, which represent high-level object categories. We first select relevant nouns from WordNet and descriptions from caption datasets to construct a semantic space aligned with image features. Then, we align image features with selected nouns and captions via optimal transport to obtain a more discriminative semantic space. Finally, we combine the enhanced semantic and image features to perform clustering. Extensive experiments across 8 datasets demonstrate the effectiveness of our method, notably surpassing the state-of-the-art training-free approach with a 4.2% improvement in accuracy and a 2.9% improvement in adjusted rand index (ARI) on the ImageNet-1K dataset.",
    "authors": [
      "Xingyu Zhu",
      "Beier Zhu",
      "Yunfan Li",
      "Junfeng Fang",
      "Shuo Wang",
      "Kesen Zhao",
      "Hanwang Zhang"
    ],
    "published": "2025-11-30T14:14:51+00:00",
    "url": "https://arxiv.org/pdf/2512.00904v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00903v1",
    "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
    "abstract": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
    "authors": [
      "Chaojun Ni",
      "Cheng Chen",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Wenzhao Zheng",
      "Boyuan Wang",
      "Tianrun Chen",
      "Guosheng Zhao",
      "Haoyun Li",
      "Zhehao Dong",
      "Qiang Zhang",
      "Yun Ye",
      "Yang Wang",
      "Guan Huang",
      "Wenjun Mei"
    ],
    "published": "2025-11-30T14:10:28+00:00",
    "url": "https://arxiv.org/pdf/2512.00903v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.00891v1",
    "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
    "abstract": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}.",
    "authors": [
      "Yiyu Wang",
      "Xuyang Liu",
      "Xiyan Gui",
      "Xinying Lin",
      "Boxue Yang",
      "Chenfei Liao",
      "Tailai Chen",
      "Linfeng Zhang"
    ],
    "published": "2025-11-30T13:44:28+00:00",
    "url": "https://arxiv.org/pdf/2512.00891v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00888v1",
    "title": "Light-Weight Benchmarks Reveal the Hidden Hardware Cost of Zero-Shot Tabular Foundation Models",
    "abstract": "Zero-shot foundation models (FMs) promise training-free prediction on tabular data, yet their hardware footprint remains poorly characterized. We present a fully reproducible benchmark that reports test accuracy together with wall-clock latency, peak CPU RAM, and peak GPU VRAM on four public datasets: Adult-Income, Higgs-100k, Wine-Quality, and California-Housing. Two open FMs (TabPFN-1.0 and TabICL-base) are compared against tuned XGBoost, LightGBM, and Random Forest baselines on a single NVIDIA T4 GPU. The tree ensembles equal or surpass FM accuracy on three datasets while completing full-test batches in <= 0.40 s and <= 150 MB RAM, using zero VRAM. TabICL achieves a 0.8 percentage-point gain on Higgs but requires roughly 40,000 times more latency (960 s) and 9 GB VRAM. TabPFN matches tree-model accuracy on Wine and Housing but peaks at 4 GB VRAM and cannot process the full 100k-row Higgs table. These results quantify the substantial hardware-versus-accuracy trade-offs in current tabular FMs and provide an open baseline for future efficiency-oriented research.",
    "authors": [
      "Aayam Bansal",
      "Ishaan Gangwani"
    ],
    "published": "2025-11-30T13:17:08+00:00",
    "url": "https://arxiv.org/pdf/2512.00888v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00887v2",
    "title": "Multilingual Training-Free Remote Sensing Image Captioning",
    "abstract": "Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.",
    "authors": [
      "Carlos Rebelo",
      "Gil Rocha",
      "Jo\u00e3o Daniel Silva",
      "Bruno Martins"
    ],
    "published": "2025-11-30T13:16:42+00:00",
    "url": "https://arxiv.org/pdf/2512.00887v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00885v1",
    "title": "HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics",
    "abstract": "Hand-object interaction (HOI) inherently involves dynamics where human manipulations produce distinct spatio-temporal effects on objects. However, existing semantic HOI benchmarks focused either on manipulation or on the resulting effects at a coarse level, lacking fine-grained spatio-temporal reasoning to capture the underlying dynamics in HOI. We introduce HanDyVQA, a fine-grained video question-answering benchmark that comprehensively covers both the manipulation and effect aspects of HOI. HanDyVQA comprises six complementary question types (Action, Process, Objects, Location, State Change, and Object Parts), totalling 11.1K multiple-choice QA pairs. Collected QA pairs recognizing manipulation styles, hand/object motions, and part-level state changes. HanDyVQA also includes 10.3K segmentation masks for Objects and Object Parts questions, enabling the evaluation of object/part-level reasoning in video object segmentation. We evaluated recent video foundation models on our benchmark and found that even the best-performing model, Gemini-2.5-Pro, reached only 73% average accuracy, which is far from human performance (97%). Further analysis shows the remaining challenges in spatial relationship, motion, and part-level geometric understanding. We also found that integrating explicit HOI-related cues into visual features improves performance, offering insights for developing future models with a deeper understanding of HOI dynamics.",
    "authors": [
      "Masatoshi Tateno",
      "Gido Kato",
      "Hirokatsu Kataoka",
      "Yoichi Sato",
      "Takuma Yagi"
    ],
    "published": "2025-11-30T13:15:02+00:00",
    "url": "https://arxiv.org/pdf/2512.00885v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00884v1",
    "title": "Towards Active Synthetic Data Generation for Finetuning Language Models",
    "abstract": "A common and effective means for improving language model capabilities involves finetuning a ``student'' language model's parameters on generations from a more proficient ``teacher'' model. Termed ``synthetic data'', these generations are often produced before any student finetuning, but some work has considered generating new synthetic samples as training progresses. This paper studies and advocates for the latter case, where data are generated in an iterative, closed-loop fashion that is guided by the current state of the student model. For a fixed budget of generated samples, or a budget in terms of compute spent querying a teacher, we show that this curation of finetuning data affords improved student performance over static generation. Further, while there have been several LLM-specific methods proposed that operate in this regime, we find that simple, inexpensive selection criteria from the active learning literature tend to be most performant. We validate these claims across four mathematical and logical reasoning datasets using four different small language models.",
    "authors": [
      "Samuel Kessler",
      "Menglin Xia",
      "Daniel Madrigal Diaz",
      "Dongge Han",
      "Helia Heshemi",
      "Saravan Rajmohan",
      "Victor Ruehle",
      "Jordan T. Ash"
    ],
    "published": "2025-11-30T13:13:00+00:00",
    "url": "https://arxiv.org/pdf/2512.00884v1",
    "categories": [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00883v1",
    "title": "Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound",
    "abstract": "World models simulate environmental dynamics to enable agents to plan and reason about future states. While existing approaches have primarily focused on visual observations, real-world perception inherently involves multiple sensory modalities. Audio provides crucial spatial and temporal cues such as sound source localization and acoustic scene properties, yet its integration into world models remains largely unexplored. No prior work has formally defined what constitutes an audio-visual world model or how to jointly capture binaural spatial audio and visual dynamics under precise action control with task reward prediction. This work presents the first formal framework for Audio-Visual World Models (AVWM), formulating multimodal environment simulation as a partially observable Markov decision process with synchronized audio-visual observations, fine-grained actions, and task rewards. To address the lack of suitable training data, we construct AVW-4k, a dataset comprising 30 hours of binaural audio-visual trajectories with action annotations and reward signals across 76 indoor environments. We propose AV-CDiT, an Audio-Visual Conditional Diffusion Transformer with a novel modality expert architecture that balances visual and auditory learning, optimized through a three-stage training strategy for effective multimodal integration. Extensive experiments demonstrate that AV-CDiT achieves high-fidelity multimodal prediction across visual and auditory modalities with reward. Furthermore, we validate its practical utility in continuous audio-visual navigation tasks, where AVWM significantly enhances the agent's performance.",
    "authors": [
      "Jiahua Wang",
      "Shannan Yan",
      "Leqi Zheng",
      "Jialong Wu",
      "Yaoxin Mao"
    ],
    "published": "2025-11-30T13:11:56+00:00",
    "url": "https://arxiv.org/pdf/2512.00883v1",
    "categories": [
      "cs.MM",
      "cs.CV",
      "cs.SD"
    ]
  },
  {
    "arxiv_id": "2512.00882v3",
    "title": "Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints",
    "abstract": "Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to \"Reasoning-Driven Hallucination\" where linguistic priors override visual perception. A key bottleneck is the \"Modality Gap\": visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose \"Look, Recite, Then Answer,\" a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.52% over Qwen2-VL-72B and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval",
    "authors": [
      "Xisheng Feng"
    ],
    "published": "2025-11-30T13:04:43+00:00",
    "url": "https://arxiv.org/pdf/2512.00882v3",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00881v1",
    "title": "Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing",
    "abstract": "Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.",
    "authors": [
      "Li Yuan",
      "Qingfei Huang",
      "Bingshan Zhu",
      "Yi Cai",
      "Qingbao Huang",
      "Changmeng Zheng",
      "Zikun Deng",
      "Tao Wang"
    ],
    "published": "2025-11-30T12:58:15+00:00",
    "url": "https://arxiv.org/pdf/2512.00881v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00880v1",
    "title": "Quantum-Inspired Spectral Geometry for Neural Operator Equivalence and Structured Pruning",
    "abstract": "The rapid growth of multimodal intelligence on resource-constrained and heterogeneous domestic hardware exposes critical bottlenecks: multimodal feature heterogeneity, real-time requirements in dynamic scenarios, and hardware-specific operator redundancy. This work introduces a quantum-inspired geometric framework for neural operators that represents each operator by its normalized singular value spectrum on the Bloch hypersphere. We prove a tight spectral-to-functional equivalence theorem showing that vanishing Fubini--Study/Wasserstein-2 distance implies provable functional closeness, establishing the first rigorous foundation for cross-modal and cross-architecture operator substitutability. Based on this metric, we propose Quantum Metric-Driven Functional Redundancy Graphs (QM-FRG) and one-shot structured pruning. Controlled simulation validates the superiority of the proposed metric over magnitude and random baselines. An extensive experimental validation on large-scale multimodal transformers and domestic heterogeneous hardware (Huawei Ascend, Cambricon MLU, Kunlunxin) hardware is deferred to an extended journal version currently in preparation.",
    "authors": [
      "Haijian Shao",
      "Wei Liu",
      "Xing Deng"
    ],
    "published": "2025-11-30T12:57:38+00:00",
    "url": "https://arxiv.org/pdf/2512.00880v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00878v1",
    "title": "Less is More: Resource-Efficient Low-Rank Adaptation",
    "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.",
    "authors": [
      "Chunlin Tian",
      "Xuyang Wei",
      "Huanrong Liu",
      "Zhijiang Guo",
      "Li Li"
    ],
    "published": "2025-11-30T12:52:04+00:00",
    "url": "https://arxiv.org/pdf/2512.00878v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00877v1",
    "title": "Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling",
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a revolutionary 3D representation. However, its substantial data size poses a major barrier to widespread adoption. While feed-forward 3DGS compression offers a practical alternative to costly per-scene per-train compressors, existing methods struggle to model long-range spatial dependencies, due to the limited receptive field of transform coding networks and the inadequate context capacity in entropy models. In this work, we propose a novel feed-forward 3DGS compression framework that effectively models long-range correlations to enable highly compact and generalizable 3D representations. Central to our approach is a large-scale context structure that comprises thousands of Gaussians based on Morton serialization. We then design a fine-grained space-channel auto-regressive entropy model to fully leverage this expansive context. Furthermore, we develop an attention-based transform coding model to extract informative latent priors by aggregating features from a wide range of neighboring Gaussians. Our method yields a $20\\times$ compression ratio for 3DGS in a feed-forward inference and achieves state-of-the-art performance among generalizable codecs.",
    "authors": [
      "Zhening Liu",
      "Rui Song",
      "Yushi Huang",
      "Yingdong Hu",
      "Xinjie Zhang",
      "Jiawei Shao",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "published": "2025-11-30T12:51:43+00:00",
    "url": "https://arxiv.org/pdf/2512.00877v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00873v1",
    "title": "Neural Discrete Representation Learning for Sparse-View CBCT Reconstruction: From Algorithm Design to Prospective Multicenter Clinical Evaluation",
    "abstract": "Cone beam computed tomography (CBCT)-guided puncture has become an established approach for diagnosing and treating early- to mid-stage thoracic tumours, yet the associated radiation exposure substantially elevates the risk of secondary malignancies. Although multiple low-dose CBCT strategies have been introduced, none have undergone validation using large-scale multicenter retrospective datasets, and prospective clinical evaluation remains lacking. Here, we propose DeepPriorCBCT - a three-stage deep learning framework that achieves diagnostic-grade reconstruction using only one-sixth of the conventional radiation dose. 4102 patients with 8675 CBCT scans from 12 centers were included to develop and validate DeepPriorCBCT. Additionally, a prospective cross-over trial (Registry number: NCT07035977) which recruited 138 patients scheduled for percutaneous thoracic puncture was conducted to assess the model's clinical applicability. Assessment by 11 physicians confirmed that reconstructed images were indistinguishable from original scans. Moreover, diagnostic performance and overall image quality were comparable to those generated by standard reconstruction algorithms. In the prospective trial, five radiologists reported no significant differences in image quality or lesion assessment between DeepPriorCBCT and the clinical standard (all P>0.05). Likewise, 25 interventionalists expressed no preference between model-based and full-sampling images for surgical guidance (Kappa<0.2). Radiation exposure with DeepPriorCBCT was reduced to approximately one-sixth of that with the conventional approach, and collectively, the findings confirm that it enables high-quality CBCT reconstruction under sparse sampling conditions while markedly decreasing intraoperative radiation risk.",
    "authors": [
      "Haoshen Wang",
      "Lei Chen",
      "Wei-Hua Zhang",
      "Linxia Wu",
      "Yong Luo",
      "Zengmao Wang",
      "Yuan Xiong",
      "Chengcheng Zhu",
      "Wenjuan Tang",
      "Xueyi Zhang",
      "Wei Zhou",
      "Xuhua Duan",
      "Lefei Zhang",
      "Gao-Jun Teng",
      "Bo Du",
      "Huangxuan Zhao"
    ],
    "published": "2025-11-30T12:45:02+00:00",
    "url": "https://arxiv.org/pdf/2512.00873v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00872v1",
    "title": "TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models",
    "abstract": "Existing foundation models (FMs) in the medical domain often require extensive fine-tuning or rely on training resource-intensive decoders, while many existing encoders are pretrained with objectives biased toward specific tasks. This illustrates a need for a strong, task-agnostic foundation model that requires minimal fine-tuning beyond feature extraction. In this work, we introduce a suite of task-agnostic pretraining of CT foundation models (TAP-CT): a simple yet effective adaptation of Vision Transformers (ViTs) and DINOv2 for volumetric data, enabling scalable self-supervised pretraining directly on 3D CT volumes. Our approach incorporates targeted modifications to patch embeddings, positional encodings, and volumetric augmentations, making the architecture depth-aware while preserving the simplicity of the underlying architectures. We show that large-scale 3D pretraining on an extensive in-house CT dataset (105K volumes) yields stable, robust frozen representations that generalize strongly across downstream tasks. To promote transparency and reproducibility, and to establish a powerful, low-resource baseline for future research in medical imaging, we will release all pretrained models, experimental configurations, and downstream benchmark code at https://huggingface.co/fomofo/tap-ct-b-3d.",
    "authors": [
      "Tim Veenboer",
      "George Yiasemis",
      "Eric Marcus",
      "Vivien Van Veldhuizen",
      "Cees G. M. Snoek",
      "Jonas Teuwen",
      "Kevin B. W. Groot Lipman"
    ],
    "published": "2025-11-30T12:43:15+00:00",
    "url": "https://arxiv.org/pdf/2512.00872v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00862v2",
    "title": "HBLLM: A Haar-Based Approach for Accurate Structured 1-Bit Quantized LLMs",
    "abstract": "We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$ on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: https://github.com/Yeyke/HBLLM.",
    "authors": [
      "Ningning Chen",
      "Weicai Ye",
      "Ying Jiang"
    ],
    "published": "2025-11-30T12:18:02+00:00",
    "url": "https://arxiv.org/pdf/2512.00862v2",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00852v1",
    "title": "One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces",
    "abstract": "Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.",
    "authors": [
      "Yandong Sun",
      "Qiang Huang",
      "Ziwei Xu",
      "Yiqun Sun",
      "Yixuan Tang",
      "Anthony K. H. Tung"
    ],
    "published": "2025-11-30T11:48:00+00:00",
    "url": "https://arxiv.org/pdf/2512.00852v1",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00850v1",
    "title": "Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting",
    "abstract": "We present Smol-GS, a novel method for learning compact representations for 3D Gaussian Splatting (3DGS). Our approach learns highly efficient encodings in 3D space that integrate both spatial and semantic information. The model captures the coordinates of the splats through a recursive voxel hierarchy, while splat-wise features store abstracted cues, including color, opacity, transformation, and material properties. This design allows the model to compress 3D scenes by orders of magnitude without loss of flexibility. Smol-GS achieves state-of-the-art compression on standard benchmarks while maintaining high rendering quality. Beyond visual fidelity, the discrete representations could potentially serve as a foundation for downstream tasks such as navigation, planning, and broader 3D scene understanding.",
    "authors": [
      "Haishan Wang",
      "Mohammad Hassan Vali",
      "Arno Solin"
    ],
    "published": "2025-11-30T11:42:00+00:00",
    "url": "https://arxiv.org/pdf/2512.00850v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00849v1",
    "title": "Topological Federated Clustering via Gravitational Potential Fields under Local Differential Privacy",
    "abstract": "Clustering non-independent and identically distributed (non-IID) data under local differential privacy (LDP) in federated settings presents a critical challenge: preserving privacy while maintaining accuracy without iterative communication. Existing one-shot methods rely on unstable pairwise centroid distances or neighborhood rankings, degrading severely under strong LDP noise and data heterogeneity. We present Gravitational Federated Clustering (GFC), a novel approach to privacy-preserving federated clustering that overcomes the limitations of distance-based methods under varying LDP. Addressing the critical challenge of clustering non-IID data with diverse privacy guarantees, GFC transforms privatized client centroids into a global gravitational potential field where true cluster centers emerge as topologically persistent singularities. Our framework introduces two key innovations: (1) a client-side compactness-aware perturbation mechanism that encodes local cluster geometry as \"mass\" values, and (2) a server-side topological aggregation phase that extracts stable centroids through persistent homology analysis of the potential field's superlevel sets. Theoretically, we establish a closed-form bound between the privacy budget $\u03b5$ and centroid estimation error, proving the potential field's Lipschitz smoothing properties exponentially suppress noise in high-density regions. Empirically, GFC outperforms state-of-the-art methods on ten benchmarks, especially under strong LDP constraints ($\u03b5< 1$), while maintaining comparable performance at lower privacy budgets. By reformulating federated clustering as a topological persistence problem in a synthetic physics-inspired space, GFC achieves unprecedented privacy-accuracy trade-offs without iterative communication, providing a new perspective for privacy-preserving distributed learning.",
    "authors": [
      "Yunbo Long",
      "Jiaquan Zhang",
      "Xi Chen",
      "Alexandra Brintrup"
    ],
    "published": "2025-11-30T11:41:16+00:00",
    "url": "https://arxiv.org/pdf/2512.00849v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00846v1",
    "title": "AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent",
    "abstract": "There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.",
    "authors": [
      "Neeraj Anand",
      "Rishabh Jain",
      "Sohan Patnaik",
      "Balaji Krishnamurthy",
      "Mausoom Sarkar"
    ],
    "published": "2025-11-30T11:32:54+00:00",
    "url": "https://arxiv.org/pdf/2512.00846v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00839v1",
    "title": "ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI",
    "abstract": "This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.",
    "authors": [
      "Fabrizio Maturo",
      "Donato Riccio",
      "Andrea Mazzitelli",
      "Giuseppe Bifulco",
      "Francesco Paolone",
      "Iulia Brezeanu"
    ],
    "published": "2025-11-30T11:21:29+00:00",
    "url": "https://arxiv.org/pdf/2512.00839v1",
    "categories": [
      "cs.AI",
      "stat.CO",
      "stat.ME"
    ]
  },
  {
    "arxiv_id": "2512.00837v1",
    "title": "WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models",
    "abstract": "Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.   In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\\% over state-of-the-art baselines at a watermark detectability strength of 95\\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\\% and 36.47\\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \\href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.",
    "authors": [
      "Yukang Lin",
      "Jiahao Shao",
      "Shuoran Jiang",
      "Wentao Zhu",
      "Bingjie Lu",
      "Xiangping Wu",
      "Joanna Siebert",
      "Qingcai Chen"
    ],
    "published": "2025-11-30T11:11:21+00:00",
    "url": "https://arxiv.org/pdf/2512.00837v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00836v1",
    "title": "Assessing model error in counterfactual worlds",
    "abstract": "Counterfactual scenario modeling exercises that ask \"what would happen if?\" are one of the most common ways we plan for the future. Despite their ubiquity in planning and decision making, scenario projections are rarely evaluated retrospectively. Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making, but requires estimating model error in counterfactual worlds. Here we present and contrast three approaches for estimating this error, and demonstrate the benefits and limitations of each in a simulation experiment. We provide recommendations for the estimation of counterfactual error and discuss the components of scenario design that are required to make scenario projections evaluable.",
    "authors": [
      "Emily Howerton",
      "Justin Lessler"
    ],
    "published": "2025-11-30T11:08:43+00:00",
    "url": "https://arxiv.org/pdf/2512.00836v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00834v1",
    "title": "SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks",
    "abstract": "Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.",
    "authors": [
      "Lin Zhu",
      "Kezhi Wang",
      "Luping Xiang",
      "Kun Yang"
    ],
    "published": "2025-11-30T11:06:58+00:00",
    "url": "https://arxiv.org/pdf/2512.00834v1",
    "categories": [
      "cs.AI",
      "cs.NI"
    ]
  },
  {
    "arxiv_id": "2512.00832v1",
    "title": "PanFlow: Decoupled Motion Control for Panoramic Video Generation",
    "abstract": "Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at https://github.com/chengzhag/PanFlow.",
    "authors": [
      "Cheng Zhang",
      "Hanwen Liang",
      "Donny Y. Chen",
      "Qianyi Wu",
      "Konstantinos N. Plataniotis",
      "Camilo Cruz Gambardella",
      "Jianfei Cai"
    ],
    "published": "2025-11-30T11:03:31+00:00",
    "url": "https://arxiv.org/pdf/2512.00832v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00829v1",
    "title": "Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy",
    "abstract": "Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.",
    "authors": [
      "Md Mehrab Hossain Opi",
      "Sumaiya Khan",
      "Moshammad Farzana Rahman"
    ],
    "published": "2025-11-30T10:34:08+00:00",
    "url": "https://arxiv.org/pdf/2512.00829v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00818v1",
    "title": "Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning",
    "abstract": "MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.",
    "authors": [
      "Haozhen Gong",
      "Xiaozhong Ji",
      "Yuansen Liu",
      "Wenbin Wu",
      "Xiaoxiao Yan",
      "Jingjing Liu",
      "Kai Wu",
      "Jiazhen Pan",
      "Bailiang Jian",
      "Jiangning Zhang",
      "Xiaobin Hu",
      "Hongwei Bran Li"
    ],
    "published": "2025-11-30T09:56:50+00:00",
    "url": "https://arxiv.org/pdf/2512.00818v1",
    "categories": [
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00814v1",
    "title": "IRPO: Boosting Image Restoration via Post-training GRPO",
    "abstract": "Recent advances in post-training paradigms have achieved remarkable success in high-level generation tasks, yet their potential for low-level vision remains rarely explored. Existing image restoration (IR) methods rely on pixel-level hard-fitting to ground-truth images, struggling with over-smoothing and poor generalization. To address these limitations, we propose IRPO, a low-level GRPO-based post-training paradigm that systematically explores both data formulation and reward modeling. We first explore a data formulation principle for low-level post-training paradigm, in which selecting underperforming samples from the pre-training stage yields optimal performance and improved efficiency. Furthermore, we model a reward-level criteria system that balances objective accuracy and human perceptual preference through three complementary components: a General Reward for structural fidelity, an Expert Reward leveraging Qwen-VL for perceptual alignment, and a Restoration Reward for task-specific low-level quality. Comprehensive experiments on six in-domain and five out-of-domain (OOD) low-level benchmarks demonstrate that IRPO achieves state-of-the-art results across diverse degradation types, surpassing the AdaIR baseline by 0.83 dB on in-domain tasks and 3.43 dB on OOD settings. Our code can be shown in https://github.com/HaoxuanXU1024/IRPO.",
    "authors": [
      "Haoxuan Xu. Yi Liu",
      "Boyuan Jiang",
      "Jinlong Peng",
      "Donghao Luo",
      "Xiaobin Hu",
      "Shuicheng Yan",
      "Haoang Li"
    ],
    "published": "2025-11-30T09:42:24+00:00",
    "url": "https://arxiv.org/pdf/2512.00814v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00812v1",
    "title": "Causal Invariance and Counterfactual Learning Driven Cooperative Game for Multi-Label Classification",
    "abstract": "Multi-label classification (MLC) remains vulnerable to label imbalance, spurious correlations, and distribution shifts, challenges that are particularly detrimental to rare label prediction. To address these limitations, we introduce the Causal Cooperative Game (CCG) framework, which conceptualizes MLC as a cooperative multi-player interaction. CCG unifies explicit causal discovery via Neural Structural Equation Models with a counterfactual curiosity reward to drive robust feature learning. Furthermore, it incorporates a causal invariance loss to ensure generalization across diverse environments, complemented by a specialized enhancement strategy for rare labels. Extensive benchmarking demonstrates that CCG substantially outperforms strong baselines in both rare label prediction and overall robustness. Through rigorous ablation studies and qualitative analysis, we validate the efficacy and interpretability of our components, underscoring the potential of synergizing causal inference with cooperative game theory for advancing multi-label learning.",
    "authors": [
      "Yijia Fan",
      "Jusheng Zhang",
      "Kaitong Cai",
      "Jing Yang",
      "Keze Wang"
    ],
    "published": "2025-11-30T09:39:43+00:00",
    "url": "https://arxiv.org/pdf/2512.00812v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00807v1",
    "title": "BioPro: On Difference-Aware Gender Fairness for Vision-Language Models",
    "abstract": "Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.",
    "authors": [
      "Yujie Lin",
      "Jiayao Ma",
      "Qingguo Hu",
      "Derek F. Wong",
      "Jinsong Su"
    ],
    "published": "2025-11-30T09:33:09+00:00",
    "url": "https://arxiv.org/pdf/2512.00807v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00805v1",
    "title": "Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding",
    "abstract": "Long video understanding is essential for human-like intelligence, enabling coherent perception and reasoning over extended temporal contexts. While the emerging thinking-with-frames paradigm, which alternates between global temporal reasoning and local frame examination, has advanced the reasoning capabilities of video multi-modal large language models (MLLMs), it suffers from a significant efficiency bottleneck due to the progressively growing and redundant multi-modal context. To address this, we propose SpecTemp, a reinforcement learning-based Speculative Temporal reasoning framework that decouples temporal perception from reasoning via a cooperative dual-model design. In SpecTemp, a lightweight draft MLLM rapidly explores and proposes salient frames from densely sampled temporal regions, while a powerful target MLLM focuses on temporal reasoning and verifies the draft's proposals, iteratively refining its attention until convergence. This design mirrors the collaborative pathways of the human brain, balancing efficiency with accuracy. To support training, we construct the SpecTemp-80K dataset, featuring synchronized dual-level annotations for coarse evidence spans and fine-grained frame-level evidence. Experiments across multiple video understanding benchmarks demonstrate that SpecTemp not only maintains competitive accuracy but also significantly accelerates inference compared with existing thinking-with-frames methods.",
    "authors": [
      "Pengfei Hu",
      "Meng Cao",
      "Yingyao Wang",
      "Yi Wang",
      "Jiahua Dong",
      "Jun Song",
      "Yu Cheng",
      "Bo Zheng",
      "Xiaodan Liang"
    ],
    "published": "2025-11-30T09:27:59+00:00",
    "url": "https://arxiv.org/pdf/2512.00805v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00804v1",
    "title": "Bias Injection Attacks on RAG Databases and Sanitization Defenses",
    "abstract": "This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective.   We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\\% which mitigates perspective shift by 6.2\\times in answers, while enabling the retrieval of 62\\% more benign passages.",
    "authors": [
      "Hao Wu",
      "Prateek Saxena"
    ],
    "published": "2025-11-30T09:27:18+00:00",
    "url": "https://arxiv.org/pdf/2512.00804v1",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ]
  },
  {
    "arxiv_id": "2512.00796v1",
    "title": "CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target",
    "abstract": "The point spread function (PSF) serves as a fundamental descriptor linking the real-world scene to the captured signal, manifesting as camera blur. Accurate PSF estimation is crucial for both optical characterization and computational vision, yet remains challenging due to the inherent ambiguity and the ill-posed nature of intensity-based deconvolution. We introduce CircleFlow, a high-fidelity PSF estimation framework that employs flow-guided edge localization for precise blur characterization. CircleFlow begins with a structured capture that encodes locally anisotropic and spatially varying PSFs by imaging a circle grid target, while leveraging the target's binary luminance prior to decouple image and kernel estimation. The latent sharp image is then reconstructed through subpixel alignment of an initialized binary structure guided by optical flow, whereas the PSF is modeled as an energy-constrained implicit neural representation. Both components are jointly optimized within a demosaicing-aware differentiable framework, ensuring physically consistent and robust PSF estimation enabled by accurate edge localization. Extensive experiments on simulated and real-world data demonstrate that CircleFlow achieves state-of-the-art accuracy and reliability, validating its effectiveness for practical PSF calibration.",
    "authors": [
      "Jiajian He",
      "Enjie Hu",
      "Shiqi Chen",
      "Tianchen Qiu",
      "Huajun Feng",
      "Zhihai Xu",
      "Yueting Chen"
    ],
    "published": "2025-11-30T09:12:34+00:00",
    "url": "https://arxiv.org/pdf/2512.00796v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.02073v1",
    "title": "HTG-GCL: Leveraging Hierarchical Topological Granularity from Cellular Complexes for Graph Contrastive Learning",
    "abstract": "Graph contrastive learning (GCL) aims to learn discriminative semantic invariance by contrasting different views of the same graph that share critical topological patterns. However, existing GCL approaches with structural augmentations often struggle to identify task-relevant topological structures, let alone adapt to the varying coarse-to-fine topological granularities required across different downstream tasks. To remedy this issue, we introduce Hierarchical Topological Granularity Graph Contrastive Learning (HTG-GCL), a novel framework that leverages transformations of the same graph to generate multi-scale ring-based cellular complexes, embodying the concept of topological granularity, thereby generating diverse topological views. Recognizing that a certain granularity may contain misleading semantics, we propose a multi-granularity decoupled contrast and apply a granularity-specific weighting mechanism based on uncertainty estimation. Comprehensive experiments on various benchmarks demonstrate the effectiveness of HTG-GCL, highlighting its superior performance in capturing meaningful graph representations through hierarchical topological information.",
    "authors": [
      "Qirui Ji",
      "Bin Qin",
      "Yifan Jin",
      "Yunze Zhao",
      "Chuxiong Sun",
      "Changwen Zheng",
      "Jianwen Cao",
      "Jiangmeng Li"
    ],
    "published": "2025-11-30T09:09:42+00:00",
    "url": "https://arxiv.org/pdf/2512.02073v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00794v1",
    "title": "PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery",
    "abstract": "Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.",
    "authors": [
      "Bo Guo",
      "Sijia Wen",
      "Yifan Zhao",
      "Jia Li",
      "Zhiming Zheng"
    ],
    "published": "2025-11-30T09:08:37+00:00",
    "url": "https://arxiv.org/pdf/2512.00794v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00791v2",
    "title": "Limitations of Using Identical Distributions for Training and Testing When Learning Boolean Functions",
    "abstract": "When the distributions of the training and test data do not coincide, the problem of understanding generalization becomes considerably more complex, prompting a variety of questions. Prior work has shown that, for some fixed learning methods, there are scenarios where training on a distribution different from the test distribution improves generalization. However, these results do not account for the possibility of choosing, for each training distribution, the optimal learning algorithm, leaving open whether the observed benefits stem from the mismatch itself or from suboptimality of the learner. In this work, we address this question in full generality. That is, we study whether it is always optimal for the training distribution to be identical to the test distribution when the learner is allowed to be optimally adapted to the training distribution. Surprisingly, assuming the existence of one-way functions, we find that the answer is no. That is, matching distributions is not always the best scenario. Nonetheless, we also show that when certain regularities are imposed on the target functions, the standard conclusion is recovered in the case of the uniform distribution.",
    "authors": [
      "Jordi P\u00e9rez-Guijarro"
    ],
    "published": "2025-11-30T09:06:07+00:00",
    "url": "https://arxiv.org/pdf/2512.00791v2",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00789v1",
    "title": "Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation",
    "abstract": "Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at https://github.com/shuanncai/EES",
    "authors": [
      "Xiaodong Cai",
      "Hai Lin",
      "Shaoxiong Zhan",
      "Weiqi Luo",
      "Hong-Gee Kim",
      "Hongyan Hao",
      "Yu Yang",
      "Hai-Tao Zheng"
    ],
    "published": "2025-11-30T08:58:08+00:00",
    "url": "https://arxiv.org/pdf/2512.00789v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00777v1",
    "title": "Sign Language Recognition using Bidirectional Reservoir Computing",
    "abstract": "Sign language recognition (SLR) facilitates communication between deaf and hearing individuals. Deep learning is widely used to develop SLR-based systems; however, it is computationally intensive and requires substantial computational resources, making it unsuitable for resource-constrained devices. To address this, we propose an efficient sign language recognition system using MediaPipe and an echo state network (ESN)-based bidirectional reservoir computing (BRC) architecture. MediaPipe extracts hand joint coordinates, which serve as inputs to the ESN-based BRC architecture. The BRC processes these features in both forward and backward directions, efficiently capturing temporal dependencies. The resulting states of BRC are concatenated to form a robust representation for classification. We evaluated our method on the Word-Level American Sign Language (WLASL) video dataset, achieving a competitive accuracy of 57.71% and a significantly lower training time of only 9 seconds, in contrast to the 55 minutes and $38$ seconds required by the deep learning-based Bi-GRU approach. Consequently, the BRC-based SLR system is well-suited for edge devices.",
    "authors": [
      "Nitin Kumar Singh",
      "Arie Rachmad Syulistyo",
      "Yuichiro Tanaka",
      "Hakaru Tamukoh"
    ],
    "published": "2025-11-30T08:25:27+00:00",
    "url": "https://arxiv.org/pdf/2512.00777v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00773v1",
    "title": "DEJIMA: A Novel Large-scale Japanese Dataset for Image Captioning and Visual Question Answering",
    "abstract": "This work addresses the scarcity of high-quality, large-scale resources for Japanese Vision-and-Language (V&L) modeling. We present a scalable and reproducible pipeline that integrates large-scale web collection with rigorous filtering/deduplication, object-detection-driven evidence extraction, and Large Language Model (LLM)-based refinement under grounding constraints. Using this pipeline, we build two resources: an image-caption dataset (DEJIMA-Cap) and a VQA dataset (DEJIMA-VQA), each containing 3.88M image-text pairs, far exceeding the size of existing Japanese V&L datasets. Human evaluations demonstrate that DEJIMA achieves substantially higher Japaneseness and linguistic naturalness than datasets constructed via translation or manual annotation, while maintaining factual correctness at a level comparable to human-annotated corpora. Quantitative analyses of image feature distributions further confirm that DEJIMA broadly covers diverse visual domains characteristic of Japan, complementing its linguistic and cultural representativeness. Models trained on DEJIMA exhibit consistent improvements across multiple Japanese multimodal benchmarks, confirming that culturally grounded, large-scale resources play a key role in enhancing model performance. All data sources and modules in our pipeline are licensed for commercial use, and we publicly release the resulting dataset and metadata to encourage further research and industrial applications in Japanese V&L modeling.",
    "authors": [
      "Toshiki Katsube",
      "Taiga Fukuhara",
      "Kenichiro Ando",
      "Yusuke Mukuta",
      "Kohei Uehara",
      "Tatsuya Harada"
    ],
    "published": "2025-11-30T08:09:43+00:00",
    "url": "https://arxiv.org/pdf/2512.00773v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00772v1",
    "title": "SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG",
    "abstract": "Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language   Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to   construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing   speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.   Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of   Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a   Large Language Model as a Query Strategist to automatically transform unstructured natural language queries   into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process   of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual   embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual   dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,   combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and   reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,   presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.",
    "authors": [
      "Hyunseok Ryu",
      "Wonjune Shin",
      "Hyun Park"
    ],
    "published": "2025-11-30T08:06:47+00:00",
    "url": "https://arxiv.org/pdf/2512.00772v1",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DL"
    ]
  },
  {
    "arxiv_id": "2512.00771v1",
    "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes",
    "abstract": "Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.",
    "authors": [
      "Xiaoshan Wu",
      "Yifei Yu",
      "Xiaoyang Lyu",
      "Yihua Huang",
      "Bo Wang",
      "Baoheng Zhang",
      "Zhongrui Wang",
      "Xiaojuan Qi"
    ],
    "published": "2025-11-30T08:05:28+00:00",
    "url": "https://arxiv.org/pdf/2512.00771v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00768v1",
    "title": "Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations",
    "abstract": "The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.",
    "authors": [
      "Hamed Razavi"
    ],
    "published": "2025-11-30T07:40:02+00:00",
    "url": "https://arxiv.org/pdf/2512.00768v1",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "arxiv_id": "2512.00765v2",
    "title": "The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches",
    "abstract": "Intelligent driving systems are vulnerable to physical adversarial attacks on traffic signs. These attacks can cause misclassification, leading to erroneous driving decisions that compromise road safety. Moreover, within V2X networks, such misinterpretations can propagate, inducing cascading failures that disrupt overall traffic flow and system stability. However, a key limitation of current physical attacks is their lack of stealth. Most methods apply perturbations to central regions of the sign, resulting in visually salient patterns that are easily detectable by human observers, thereby limiting their real-world practicality. This study proposes TESP-Attack, a novel stealth-aware adversarial patch method for traffic sign classification. Based on the observation that human visual attention primarily focuses on the central regions of traffic signs, we employ instance segmentation to generate edge-aligned masks that conform to the shape characteristics of the signs. A U-Net generator is utilized to craft adversarial patches, which are then optimized through color and texture constraints along with frequency domain analysis to achieve seamless integration with the background environment, resulting in highly effective visual concealment. The proposed method demonstrates outstanding attack success rates across traffic sign classification models with varied architectures, achieving over 90% under limited query budgets. It also exhibits strong cross-model transferability and maintains robust real-world performance that remains stable under varying angles and distances.",
    "authors": [
      "Haojie Ji",
      "Te Hu",
      "Haowen Li",
      "Long Jin",
      "Chongshi Xin",
      "Yuchi Yao",
      "Jiarui Xiao"
    ],
    "published": "2025-11-30T07:26:07+00:00",
    "url": "https://arxiv.org/pdf/2512.00765v2",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00763v1",
    "title": "Provable Benefit of Sign Descent: A Minimal Model Under Heavy-Tailed Class Imbalance",
    "abstract": "Adaptive optimization methods (such as Adam) play a major role in LLM pretraining, significantly outperforming Gradient Descent (GD). Recent studies have proposed new smoothness assumptions on the loss function to explain the advantages of adaptive algorithms with structured preconditioners, e.g., coordinate-wise or layer-wise, and steepest descent methods w.r.t. non-euclidean norms, e.g., $\\ell_\\infty$ norm or spectral norm, over GD. However, it remains unclear how these smoothness assumptions manifest in language modelling tasks. In this work, we aim to analyze the benefit of $\\ell_\\infty$-norm descent (a.k.a. sign descent) directly from properties of the data distribution, namely, heavy-tailed class imbalance. We propose a minimal yet representative setting of next-token prediction, where we can provably show faster convergence of coordinate-wise algorithms such as Sign descent (steepest descent w.r.t. $\\ell_\\infty$ norm) over normalized GD (steepest descent w.r.t. to $\\ell_2$ norm) in the presence of heavy tail class imbalance.",
    "authors": [
      "Robin Yadav",
      "Shuo Xie",
      "Tianhao Wang",
      "Zhiyuan Li"
    ],
    "published": "2025-11-30T07:21:02+00:00",
    "url": "https://arxiv.org/pdf/2512.00763v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00762v1",
    "title": "Seeing the Wind from a Falling Leaf",
    "abstract": "A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \\href{https://chaoren2357.github.io/seeingthewind/}{project page}.",
    "authors": [
      "Zhiyuan Gao",
      "Jiageng Mao",
      "Hong-Xing Yu",
      "Haozhe Lou",
      "Emily Yue-Ting Jia",
      "Jernej Barbic",
      "Jiajun Wu",
      "Yue Wang"
    ],
    "published": "2025-11-30T07:19:00+00:00",
    "url": "https://arxiv.org/pdf/2512.00762v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00757v1",
    "title": "Preventing Model Collapse via Contraction-Conditioned Neural Filters",
    "abstract": "This paper presents a neural network filter method based on contraction operators to address model collapse in recursive training of generative models. Unlike \\cite{xu2024probabilistic}, which requires superlinear sample growth ($O(t^{1+s})$), our approach completely eliminates the dependence on increasing sample sizes within an unbiased estimation framework by designing a neural filter that learns to satisfy contraction conditions. We develop specialized neural network architectures and loss functions that enable the filter to actively learn contraction conditions satisfying Assumption 2.3 in exponential family distributions, thereby ensuring practical application of our theoretical results. Theoretical analysis demonstrates that when the learned contraction conditions are satisfied, estimation errors converge probabilistically even with constant sample sizes, i.e., $\\limsup_{t\\to\\infty}\\mathbb{P}(\\|\\mathbf{e}_t\\|>\u03b4)=0$ for any $\u03b4>0$. Experimental results show that our neural network filter effectively learns contraction conditions and prevents model collapse under fixed sample size settings, providing an end-to-end solution for practical applications.",
    "authors": [
      "Zongjian Han",
      "Yiran Liang",
      "Ruiwen Wang",
      "Yiwei Luo",
      "Yilin Huang",
      "Xiaotong Song",
      "Dongqing Wei"
    ],
    "published": "2025-11-30T06:48:21+00:00",
    "url": "https://arxiv.org/pdf/2512.00757v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00756v1",
    "title": "MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents",
    "abstract": "With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.",
    "authors": [
      "Ruihan Chen",
      "Qiming Li",
      "Xiaocheng Feng",
      "Xiaoliang Yang",
      "Weihong Zhong",
      "Yuxuan Gu",
      "Zekun Zhou",
      "Bing Qin"
    ],
    "published": "2025-11-30T06:47:33+00:00",
    "url": "https://arxiv.org/pdf/2512.00756v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00752v1",
    "title": "Charts Are Not Images: On the Challenges of Scientific Chart Editing",
    "abstract": "Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \\textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \\textit{FigEdit} (https://github.com/adobe-research/figure-editing), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.",
    "authors": [
      "Shawn Li",
      "Ryan Rossi",
      "Sungchul Kim",
      "Sunav Choudhary",
      "Franck Dernoncourt",
      "Puneet Mathur",
      "Zhengzhong Tu",
      "Yue Zhao"
    ],
    "published": "2025-11-30T06:13:48+00:00",
    "url": "https://arxiv.org/pdf/2512.00752v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00748v1",
    "title": "Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization",
    "abstract": "Medical image segmentation is inherently influenced by data uncertainty, arising from ambiguous boundaries in medical scans and inter-observer variability in diagnosis. To address this challenge, previous works formulated the multi-rater medical image segmentation task, where multiple experts provide separate annotations for each image. However, existing models are typically constrained to either generate diverse segmentation that lacks expert specificity or to produce personalized outputs that merely replicate individual annotators. We propose Probabilistic modeling of multi-rater medical image Segmentation (ProSeg) that simultaneously enables both diversification and personalization. Specifically, we introduce two latent variables to model expert annotation preferences and image boundary ambiguity. Their conditional probabilistic distributions are then obtained through variational inference, allowing segmentation outputs to be generated by sampling from these distributions. Extensive experiments on both the nasopharyngeal carcinoma dataset (NPC) and the lung nodule dataset (LIDC-IDRI) demonstrate that our ProSeg achieves a new state-of-the-art performance, providing segmentation results that are both diverse and expert-personalized. Code can be found in https://github.com/AI4MOL/ProSeg.",
    "authors": [
      "Ke Liu",
      "Shangde Gao",
      "Yichao Fu",
      "Shangqi Gao",
      "Chunhua Shen"
    ],
    "published": "2025-11-30T05:53:39+00:00",
    "url": "https://arxiv.org/pdf/2512.00748v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00745v1",
    "title": "FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case",
    "abstract": "This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.",
    "authors": [
      "Md Abdullah Al Kafi",
      "Sumit Kumar Banshal"
    ],
    "published": "2025-11-30T05:48:12+00:00",
    "url": "https://arxiv.org/pdf/2512.00745v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00744v1",
    "title": "Joint Multi-scale Gated Transformer and Prior-guided Convolutional Network for Learned Image Compression",
    "abstract": "Recently, learned image compression methods have made remarkable achievements, some of which have outperformed the traditional image codec VVC. The advantages of learned image compression methods over traditional image codecs can be largely attributed to their powerful nonlinear transform coding. Convolutional layers and shifted window transformer (Swin-T) blocks are the basic units of neural networks, and their representation capabilities play an important role in nonlinear transform coding. In this paper, to improve the ability of the vanilla convolution to extract local features, we propose a novel prior-guided convolution (PGConv), where asymmetric convolutions (AConvs) and difference convolutions (DConvs) are introduced to strengthen skeleton elements and extract high-frequency information, respectively. A re-parameterization strategy is also used to reduce the computational complexity of PGConv. Moreover, to improve the ability of the Swin-T block to extract non-local features, we propose a novel multi-scale gated transformer (MGT), where dilated window-based multi-head self-attention blocks with different dilation rates and depth-wise convolution layers with different kernel sizes are used to extract multi-scale features, and a gate mechanism is introduced to enhance non-linearity. Finally, we propose a novel joint Multi-scale Gated Transformer and Prior-guided Convolutional Network (MGTPCN) for learned image compression. Experimental results show that our MGTPCN surpasses state-of-the-art algorithms with a better trade-off between performance and complexity.",
    "authors": [
      "Zhengxin Chen",
      "Xiaohai He",
      "Tingrong Zhang",
      "Shuhua Xiong",
      "Chao Ren"
    ],
    "published": "2025-11-30T05:45:47+00:00",
    "url": "https://arxiv.org/pdf/2512.00744v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00743v1",
    "title": "Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards",
    "abstract": "Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \\textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \\textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \\textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \\emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \\emph{reward-based grouping} to compute advantages for each reward function \\textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \\textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \\textit{PickScore-25k} and multi-objective \\textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \\href{https://github.com/fikry102/Multi-GRPO}{https://github.com/fikry102/Multi-GRPO}.",
    "authors": [
      "Qiang Lyu",
      "Zicong Chen",
      "Chongxiao Wang",
      "Haolin Shi",
      "Shibo Gao",
      "Ran Piao",
      "Youwei Zeng",
      "Jianlou Si",
      "Fei Ding",
      "Jing Li",
      "Chun Pong Lau",
      "Weiqiang Wang"
    ],
    "published": "2025-11-30T05:44:35+00:00",
    "url": "https://arxiv.org/pdf/2512.00743v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00742v1",
    "title": "On the Regulatory Potential of User Interfaces for AI Agent Governance",
    "abstract": "AI agents that take actions in their environment autonomously over extended time horizons require robust governance interventions to curb their potentially consequential risks. Prior proposals for governing AI agents primarily target system-level safeguards (e.g., prompt injection monitors) or agent infrastructure (e.g., agent IDs). In this work, we explore a complementary approach: regulating user interfaces of AI agents as a way of enforcing transparency and behavioral requirements that then demand changes at the system and/or infrastructure levels. Specifically, we analyze 22 existing agentic systems to identify UI elements that play key roles in human-agent interaction and communication. We then synthesize those elements into six high-level interaction design patterns that hold regulatory potential (e.g., requiring agent memory to be editable). We conclude with policy recommendations based on our analysis. Our work exposes a new surface for regulatory action that supplements previous proposals for practical AI agent governance.",
    "authors": [
      "K. J. Kevin Feng",
      "Tae Soo Kim",
      "Rock Yuren Pang",
      "Faria Huq",
      "Tal August",
      "Amy X. Zhang"
    ],
    "published": "2025-11-30T05:32:13+00:00",
    "url": "https://arxiv.org/pdf/2512.00742v1",
    "categories": [
      "cs.CY",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00741v1",
    "title": "MASCOT: Analyzing Malware Evolution Through A Well-Curated Source Code Dataset",
    "abstract": "In recent years, the explosion of malware and extensive code reuse have formed complex evolutionary connections among malware specimens. The rapid pace of development makes it challenging for existing studies to characterize recent evolutionary trends. In addition, intuitive tools to untangle these intricate connections between malware specimens or categories are urgently needed. This paper introduces a manually-reviewed malware source code dataset containing 6032 specimens. Building on and extending current research from a software engineering perspective, we systematically evaluate the scale, development costs, code quality, as well as security and dependencies of modern malware. We further introduce a multi-view genealogy analysis to clarify malware connections: at an overall view, this analysis quantifies the strength and direction of connections among specimens and categories; at a detailed view, it traces the evolutionary histories of individual specimens. Experimental results indicate that, despite persistent shortcomings in code quality, malware specimens exhibit an increasing complexity and standardization, in step with the development of mainstream software engineering practices. Meanwhile, our genealogy analysis intuitively reveals lineage expansion and evolution driven by code reuse, providing new evidence and tools for understanding the formation and evolution of the malware ecosystem.",
    "authors": [
      "Bojing Li",
      "Duo Zhong",
      "Dharani Nadendla",
      "Gabriel Terceros",
      "Prajna Bhandar",
      "Raguvir S",
      "Charles Nicholas"
    ],
    "published": "2025-11-30T05:26:23+00:00",
    "url": "https://arxiv.org/pdf/2512.00741v1",
    "categories": [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00738v1",
    "title": "Orchestrating Rewards in the Era of Intelligence-Driven Commerce",
    "abstract": "Despite their evolution from early copper-token schemes to sophisticated digital solutions, loyalty programs remain predominantly closed ecosystems, with brands retaining full control over all components. Coalition loyalty programs emerged to enable cross-brand interoperability, but approximately 60\\% fail within 10 years in spite of theoretical advantages rooted in network economics. This paper demonstrates that coalition failures stem from fundamental architectural limitations in centralized operator models rather than operational deficiencies, and argues further that neither closed nor coalition systems can scale in intelligence-driven paradigms where AI agents mediate commerce and demand trustless, protocol-based coordination that existing architectures cannot provide. We propose a hybrid framework where brands maintain sovereign control over their programs while enabling cross-brand interoperability through trustless exchange mechanisms. Our framework preserves closed system advantages while enabling open system benefits without the structural problems that doom traditional coalitions. We derive a mathematical pricing model accounting for empirically-validated market factors while enabling fair value exchange across interoperable reward systems.",
    "authors": [
      "Paul Osemudiame Oamen",
      "Robert Wesley",
      "Pius Onobhayedo"
    ],
    "published": "2025-11-30T05:24:18+00:00",
    "url": "https://arxiv.org/pdf/2512.00738v1",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.00736v1",
    "title": "REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories",
    "abstract": "Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM (Reasoning over Embodied Multi-Frame Trajectories), a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.",
    "authors": [
      "Jacob Thompson",
      "Emiliano Garcia-Lopez",
      "Yonatan Bisk"
    ],
    "published": "2025-11-30T05:20:22+00:00",
    "url": "https://arxiv.org/pdf/2512.00736v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00729v1",
    "title": "Probing the \"Psyche'' of Large Reasoning Models: Understanding Through a Human Lens",
    "abstract": "Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.",
    "authors": [
      "Yuxiang Chen",
      "Zuohan Wu",
      "Ziwei Wang",
      "Xiangning Yu",
      "Xujia Li",
      "Linyi Yang",
      "Mengyue Yang",
      "Jun Wang",
      "Lei Chen"
    ],
    "published": "2025-11-30T04:49:44+00:00",
    "url": "https://arxiv.org/pdf/2512.00729v1",
    "categories": [
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00728v1",
    "title": "Deep Learning for Modeling and Dispatching Hybrid Wind Farm Power Generation",
    "abstract": "Wind farms with integrated energy storage, or hybrid wind farms, are able to store energy and dispatch it to the grid following an operational strategy. For individual wind farms with integrated energy storage capacity, data-driven dispatch strategies using localized grid demand and market conditions as input parameters stand to maximize wind energy value. Synthetic power generation data modeled on atmospheric conditions provide another avenue for improving the robustness of data-driven dispatch strategies. To these ends, the present work develops two deep learning frameworks: COVE-NN, an LSTM-based dispatch strategy tailored to individual wind farms, which reduced annual COVE by 32.3% over 43 years of simulated operations in a case study at the Pyron site; and a power generation modeling framework that reduced RMSE by 9.5% and improved power curve similarity by 18.9% when validated on the Palouse wind farm. Together, these models pave the way for more robust, data-driven dispatch strategies and potential extensions to other renewable energy systems.",
    "authors": [
      "Zach Lawrence",
      "Jessica Yao",
      "Chris Qin"
    ],
    "published": "2025-11-30T04:47:00+00:00",
    "url": "https://arxiv.org/pdf/2512.00728v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00723v1",
    "title": "TrajDiff: End-to-end Autonomous Driving without Perception Annotation",
    "abstract": "End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.",
    "authors": [
      "Xingtai Gui",
      "Jianbo Zhao",
      "Wencheng Han",
      "Jikai Wang",
      "Jiahao Gong",
      "Feiyang Tan",
      "Cheng-zhong Xu",
      "Jianbing Shen"
    ],
    "published": "2025-11-30T04:34:20+00:00",
    "url": "https://arxiv.org/pdf/2512.00723v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.00722v1",
    "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
    "abstract": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.",
    "authors": [
      "Jiaming Xu",
      "Jiayi Pan",
      "Hanzhen Wang",
      "Yongkang Zhou",
      "Jiancai Ye",
      "Yu Wang",
      "Guohao Dai"
    ],
    "published": "2025-11-30T04:32:43+00:00",
    "url": "https://arxiv.org/pdf/2512.00722v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00718v1",
    "title": "RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images",
    "abstract": "Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.",
    "authors": [
      "Deliang Wang",
      "Peng Liu"
    ],
    "published": "2025-11-30T04:12:43+00:00",
    "url": "https://arxiv.org/pdf/2512.00718v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00716v1",
    "title": "Graph Data Augmentation with Contrastive Learning on Covariate Distribution Shift",
    "abstract": "Covariate distribution shift occurs when certain structural features present in the test set are absent from the training set. It is a common type of out-of-distribution (OOD) problem, frequently encountered in real-world graph data with complex structures. Existing research has revealed that most out-of-the-box graph neural networks (GNNs) fail to account for covariate shifts. Furthermore, we observe that existing methods aimed at addressing covariate shifts often fail to fully leverage the rich information contained within the latent space. Motivated by the potential of the latent space, we introduce a new method called MPAIACL for More Powerful Adversarial Invariant Augmentation using Contrastive Learning. MPAIACL leverages contrastive learning to unlock the full potential of vector representations by harnessing their intrinsic information. Through extensive experiments, MPAIACL demonstrates its robust generalization and effectiveness, as it performs well compared with other baselines across various public OOD datasets. The code is publicly available at https://github.com/flzeng1/MPAIACL.",
    "authors": [
      "Fanlong Zeng",
      "Wensheng Gan"
    ],
    "published": "2025-11-30T03:58:55+00:00",
    "url": "https://arxiv.org/pdf/2512.00716v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00714v1",
    "title": "Deep Learning-Based Computer Vision Models for Early Cancer Detection Using Multimodal Medical Imaging and Radiogenomic Integration Frameworks",
    "abstract": "Early cancer detection remains one of the most critical challenges in modern healthcare, where delayed diagnosis significantly reduces survival outcomes. Recent advancements in artificial intelligence, particularly deep learning, have enabled transformative progress in medical imaging analysis. Deep learning-based computer vision models, such as convolutional neural networks (CNNs), transformers, and hybrid attention architectures, can automatically extract complex spatial, morphological, and temporal patterns from multimodal imaging data including MRI, CT, PET, mammography, histopathology, and ultrasound. These models surpass traditional radiological assessment by identifying subtle tissue abnormalities and tumor microenvironment variations invisible to the human eye. At a broader scale, the integration of multimodal imaging with radiogenomics linking quantitative imaging features with genomics, transcriptomics, and epigenetic biomarkers has introduced a new paradigm for personalized oncology. This radiogenomic fusion allows the prediction of tumor genotype, immune response, molecular subtypes, and treatment resistance without invasive biopsies.",
    "authors": [
      "Emmanuella Avwerosuoghene Oghenekaro"
    ],
    "published": "2025-11-30T03:28:48+00:00",
    "url": "https://arxiv.org/pdf/2512.00714v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00713v1",
    "title": "Concept-Guided Backdoor Attack on Vision Language Models",
    "abstract": "Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing \"cat\" with \"dog\"), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs.",
    "authors": [
      "Haoyu Shen",
      "Weimin Lyu",
      "Haotian Xu",
      "Tengfei Ma"
    ],
    "published": "2025-11-30T03:24:23+00:00",
    "url": "https://arxiv.org/pdf/2512.00713v1",
    "categories": [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00709v1",
    "title": "When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF",
    "abstract": "Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.",
    "authors": [
      "Yifan Xu",
      "Xichen Ye",
      "Yifan Chen",
      "Qiaosheng Zhang"
    ],
    "published": "2025-11-30T03:16:20+00:00",
    "url": "https://arxiv.org/pdf/2512.00709v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02070v1",
    "title": "DPWMixer: Dual-Path Wavelet Mixer for Long-Term Time Series Forecasting",
    "abstract": "Long-term time series forecasting (LTSF) is a critical task in computational intelligence. While Transformer-based models effectively capture long-range dependencies, they often suffer from quadratic complexity and overfitting due to data sparsity. Conversely, efficient linear models struggle to depict complex non-linear local dynamics. Furthermore, existing multi-scale frameworks typically rely on average pooling, which acts as a non-ideal low-pass filter, leading to spectral aliasing and the irreversible loss of high-frequency transients. In response, this paper proposes DPWMixer, a computationally efficient Dual-Path architecture. The framework is built upon a Lossless Haar Wavelet Pyramid that replaces traditional pooling, utilizing orthogonal decomposition to explicitly disentangle trends and local fluctuations without information loss. To process these components, we design a Dual-Path Trend Mixer that integrates a global linear mapping for macro-trend anchoring and a flexible patch-based MLP-Mixer for micro-dynamic evolution. Finally, An adaptive multi-scale fusion module then integrates predictions from diverse scales, weighted by channel stationarity to optimize synthesis. Extensive experiments on eight public benchmarks demonstrate that our method achieves a consistent improvement over state-of-the-art baselines. The code is available at https://github.com/hit636/DPWMixer.",
    "authors": [
      "Li Qianyang",
      "Zhang Xingjun",
      "Wang Shaoxun",
      "Wei Jia"
    ],
    "published": "2025-11-30T03:12:50+00:00",
    "url": "https://arxiv.org/pdf/2512.02070v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00706v1",
    "title": "Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation",
    "abstract": "Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical challenge.In this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.",
    "authors": [
      "Chengzhi Yu",
      "Yifan Xu",
      "Yifan Chen",
      "Wenyi Zhang"
    ],
    "published": "2025-11-30T02:55:20+00:00",
    "url": "https://arxiv.org/pdf/2512.00706v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00700v1",
    "title": "CAR-Net: A Cascade Refinement Network for Rotational Motion Deblurring under Angle Information Uncertainty",
    "abstract": "We propose a new neural network architecture called CAR-net (CAscade Refinement Network) to deblur images that are subject to rotational motion blur. Our architecture is specifically designed for the semi-blind scenarios where only noisy information of the rotational motion blur angle is available. The core of our approach is progressive refinement process that starts with an initial deblurred estimate obtained from frequency-domain inversion; A series of refinement stages take the current deblurred image to predict and apply residual correction to the current estimate, progressively suppressing artifacts and restoring fine details. To handle parameter uncertainty, our architecture accommodates an optional angle detection module which can be trained end-to-end with refinement modules. We provide a detailed description of our architecture and illustrate its efficiency through experiments using both synthetic and real-life images. Our code and model as well as the links to the datasets are available at https://github.com/tony123105/CAR-Net",
    "authors": [
      "Ka Chung Lai",
      "Ahmet Cetinkaya"
    ],
    "published": "2025-11-30T02:36:12+00:00",
    "url": "https://arxiv.org/pdf/2512.00700v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00696v1",
    "title": "Hierarchical Molecular Language Models (HMLMs)",
    "abstract": "Cellular signaling networks represent complex information processing systems that have been modeled via traditional mathematical or statistical approaches. However, these methods often struggle to capture context-dependent signaling, pathway cross-talk, and temporal dynamics across multiple biological scales. Here, we introduce hierarchical molecular language models (HMLMs), a novel architecture that proposes a molecular network-specific large language model (LLM) to use in intracellular communication as a specialized molecular language, which includes molecules as tokens, protein interactions, post-translational modifications, and regulatory events modeled as semantic relationships within an adapted transformer architecture. HMLMs employ graph-structured attention mechanisms to accommodate signaling network topology while integrating information across the molecular, pathway, and cellular scales through hierarchical attention patterns. We demonstrate HMLM superiority using a cardiac fibroblast signaling network comprising over 100 molecular species across functional modules connected by regulatory edges. HMLM achieved a mean squared error (MSE) of 0.058 for temporal signaling predictions, representing 30% improvement over graph neural networks (GNNs: 0.083) and 52% improvement over ordinary differential equation models (ODEs: 0.121), with particular advantages under sparse temporal sampling conditions where HMLM maintained MSE = 0.041 with only 4 time-points. The HMLMs offer a foundation for AI-driven biology and medicine with predictable scaling characteristics suitable for interactive applications. By bridging molecular mechanisms with cellular phenotypes through AI-driven molecular language representation, HMLMs provide a powerful paradigm for systems biology that advances precision medicine applications and therapeutic discovery in the era of AI.",
    "authors": [
      "Hasi Hays",
      "Yue Yu",
      "William Richardson"
    ],
    "published": "2025-11-30T02:09:27+00:00",
    "url": "https://arxiv.org/pdf/2512.00696v1",
    "categories": [
      "q-bio.MN",
      "cs.AI",
      "cs.ET"
    ]
  },
  {
    "arxiv_id": "2512.00694v1",
    "title": "Affordance-First Decomposition for Continual Learning in Video-Language Understanding",
    "abstract": "Continual learning for video--language understanding is increasingly important as models face non-stationary data, domains, and query styles, yet prevailing solutions blur what should stay stable versus what should adapt, rely on static routing/capacity, or require replaying past videos. We aim to explicitly specify where stability lives and where plasticity should be focused under realistic memory and privacy constraints. We introduce Affordance-First Decomposition (AFD): videos are mapped to slowly varying affordance tokens that form a shared, time-aligned substrate, while a lightweight, query-routed, conflict-aware scheduler concentrates adaptation and grows capacity only when needed. The substrate is stabilized via weak alignment and teacher consistency, and training uses question-only replay. AFD achieves state-of-the-art across protocols: 51.6% average accuracy with -1.8% forgetting on domain-incremental VideoQA, ViLCo R@1@0.5 of 29.6% (MQ) and 20.7% (NLQ) with 18.4% stAP@0.25 (VQ), and 39.5% accuracy with -1.6% forgetting on time-incremental iVQA. Overall, AFD offers an explicit, interpretable split between a stable interaction-centered substrate and targeted adaptation.",
    "authors": [
      "Mengzhu Xu",
      "Hanzhi Liu",
      "Ningkang Peng",
      "Qianyu Chen",
      "Canran Xiao"
    ],
    "published": "2025-11-30T02:04:39+00:00",
    "url": "https://arxiv.org/pdf/2512.00694v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00691v1",
    "title": "Silhouette-based Gait Foundation Model",
    "abstract": "Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: https://github.com/ShiqiYu/OpenGait.",
    "authors": [
      "Dingqiang Ye",
      "Chao Fan",
      "Kartik Narayan",
      "Bingzhe Wu",
      "Chengwen Luo",
      "Jianqiang Li",
      "Vishal M. Patel"
    ],
    "published": "2025-11-30T01:53:41+00:00",
    "url": "https://arxiv.org/pdf/2512.00691v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00683v1",
    "title": "Model of human cognition",
    "abstract": "The development of large language models (LLMs) is limited by a lack of explainability, the absence of a unifying theory, and prohibitive operational costs. We propose a neuro-theoretical framework for the emergence of intelligence in systems that is both functionally robust and biologically plausible. The model provides theoretical insights into cognitive processes such as decision-making and problem solving, and a computationally efficient approach for the creation of explainable and generalizable artificial intelligence.",
    "authors": [
      "Wu Yonggang"
    ],
    "published": "2025-11-30T00:57:32+00:00",
    "url": "https://arxiv.org/pdf/2512.00683v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00677v1",
    "title": "Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer",
    "abstract": "Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/",
    "authors": [
      "Dong In Lee",
      "Hyungjun Doh",
      "Seunggeun Chi",
      "Runlin Duan",
      "Sangpil Kim",
      "Karthik Ramani"
    ],
    "published": "2025-11-30T00:18:46+00:00",
    "url": "https://arxiv.org/pdf/2512.00677v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00676v1",
    "title": "Realistic Handwritten Multi-Digit Writer (MDW) Number Recognition Challenges",
    "abstract": "Isolated digit classification has served as a motivating problem for decades of machine learning research. In real settings, numbers often occur as multiple digits, all written by the same person. Examples include ZIP Codes, handwritten check amounts, and appointment times. In this work, we leverage knowledge about the writers of NIST digit images to create more realistic benchmark multi-digit writer (MDW) data sets. As expected, we find that classifiers may perform well on isolated digits yet do poorly on multi-digit number recognition. If we want to solve real number recognition problems, additional advances are needed. The MDW benchmarks come with task-specific performance metrics that go beyond typical error calculations to more closely align with real-world impact. They also create opportunities to develop methods that can leverage task-specific knowledge to improve performance well beyond that of individual digit classification methods.",
    "authors": [
      "Kiri L. Wagstaff"
    ],
    "published": "2025-11-30T00:13:34+00:00",
    "url": "https://arxiv.org/pdf/2512.00676v1",
    "categories": [
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00673v1",
    "title": "A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data",
    "abstract": "Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.",
    "authors": [
      "Breanna E. Green",
      "Ashley L. Shea",
      "Pengfei Zhao",
      "Drew B. Margolin"
    ],
    "published": "2025-11-29T23:59:58+00:00",
    "url": "https://arxiv.org/pdf/2512.00673v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00672v1",
    "title": "ML-Tool-Bench: Tool-Augmented Planning for ML Tasks",
    "abstract": "The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence. These agents must orchestrate complex sequences of data analysis, feature engineering, model selection, and hyperparameter optimization, tasks that require sophisticated planning and iteration. While recent work on building ML agents has explored using large language models (LLMs) for direct code generation, tool-augmented approaches offer greater modularity and reliability. However, existing tool-use benchmarks focus primarily on task-specific tool selection or argument extraction for tool invocation, failing to evaluate the sophisticated planning capabilities required for ML Agents. In this work, we introduce a comprehensive benchmark for evaluating tool-augmented ML agents using a curated set of 61 specialized tools and 15 tabular ML challenges from Kaggle. Our benchmark goes beyond traditional tool-use evaluation by incorporating an in-memory named object management, allowing agents to flexibly name, save, and retrieve intermediate results throughout the workflows. We demonstrate that standard ReAct-style approaches struggle to generate valid tool sequences for complex ML pipelines, and that tree search methods with LLM-based evaluation underperform due to inconsistent state scoring. To address these limitations, we propose two simple approaches: 1) using shaped deterministic rewards with structured textual feedback, and 2) decomposing the original problem into a sequence of sub-tasks, which significantly improves trajectory validity and task performance. Using GPT-4o, our approach improves over ReAct by 16.52 percentile positions, taking the median across all Kaggle challenges. We believe our work provides a foundation for developing more capable tool-augmented planning ML agents.",
    "authors": [
      "Yaswanth Chittepu",
      "Raghavendra Addanki",
      "Tung Mai",
      "Anup Rao",
      "Branislav Kveton"
    ],
    "published": "2025-11-29T23:59:40+00:00",
    "url": "https://arxiv.org/pdf/2512.00672v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00670v1",
    "title": "EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients",
    "abstract": "Diffusion-based large language models (dLLMs) refine token generations through iterative denoising, but answers often stabilize before all steps complete. We propose EDIT (Early Diffusion Inference Termination), an inference-time criterion that adaptively stops denoising once sufficient reasoning stability relative to training-time reasoning is detected. EDIT monitors the alignment between token activations and a reasoning map derived from AdamW-aggregated LoRA updates captured during supervised fine-tuning (SFT). During training, optimization dynamics generate rich metadata about parameter importance that in prior methods is typically discarded upon model release. We preserve this information as a compact representation of learned reasoning pathways. During inference, alignment scores are converted to a distribution over the tokens already unmasked at the current denoising step, and convergence is detected when KL divergence between consecutive steps falls below a threshold on the matched unmasked (visible) tokens. Across reasoning benchmarks, EDIT reduces diffusion steps by 11.8% to 68.3% while preserving or improving accuracy in most settings, with approximately 0.02% storage overhead (about 1.5-2 MB for all QKV modules across 32 blocks in an 8 GB model). By utilizing training-gradient dynamics, our work opens a new research direction for reducing dLLM inference time and cost.",
    "authors": [
      "He-Yen Hsieh",
      "Hong Wang",
      "H. T. Kung"
    ],
    "published": "2025-11-29T23:47:47+00:00",
    "url": "https://arxiv.org/pdf/2512.00670v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00663v1",
    "title": "Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs",
    "abstract": "Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.",
    "authors": [
      "Tanmay Agrawal"
    ],
    "published": "2025-11-29T23:09:15+00:00",
    "url": "https://arxiv.org/pdf/2512.00663v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00659v1",
    "title": "Fast, Robust, Permutation-and-Sign Invariant SO(3) Pattern Alignment",
    "abstract": "We address the correspondence-free alignment of two rotation sets on \\(SO(3)\\), a core task in calibration and registration that is often impeded by missing time alignment, outliers, and unknown axis conventions. Our key idea is to decompose each rotation into its \\emph{Transformed Basis Vectors} (TBVs)-three unit vectors on \\(S^2\\)-and align the resulting spherical point sets per axis using fast, robust matchers (SPMC, FRS, and a hybrid). To handle axis relabels and sign flips, we introduce a \\emph{Permutation-and-Sign Invariant} (PASI) wrapper that enumerates the 24 proper signed permutations, scores them via summed correlations, and fuses the per-axis estimates into a single rotation by projection/Karcher mean. The overall complexity remains linear in the number of rotations (\\(\\mathcal{O}(n)\\)), contrasting with \\(\\mathcal{O}(N_r^3\\log N_r)\\) for spherical/\\(SO(3)\\) correlation. Experiments on EuRoC Machine Hall simulations   (axis-consistent) and the ETH Hand-Eye benchmark (\\texttt{robot\\_arm\\_real})   (axis-ambiguous) show that our methods are accurate, 6-60x faster than traditional methods, and robust under extreme outlier ratios (up to 90\\%), all without correspondence search.",
    "authors": [
      "Anik Sarker",
      "Alan T. Asbeck"
    ],
    "published": "2025-11-29T22:54:25+00:00",
    "url": "https://arxiv.org/pdf/2512.00659v1",
    "categories": [
      "cs.RO",
      "cs.CG",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00656v1",
    "title": "Sycophancy Claims about Language Models: The Missing Human-in-the-Loop",
    "abstract": "Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.",
    "authors": [
      "Jan Batzner",
      "Volker Stocker",
      "Stefan Schmid",
      "Gjergji Kasneci"
    ],
    "published": "2025-11-29T22:40:53+00:00",
    "url": "https://arxiv.org/pdf/2512.00656v1",
    "categories": [
      "cs.CL",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.00647v2",
    "title": "MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba",
    "abstract": "Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss as they discard or compress token representations. This problem is further exacerbated when the same fine-grained token processing is uniformly applied across all images regardless of visual complexity. We observe that not all inputs require fine-grained processing: simple images can be effectively handled at a coarse resolution, while only complex ones require refinement. Based on this insight, we propose MambaScope, an adaptive framework for efficient inference for Vision Mamba. MambaScope first performs coarse-grained inference by dividing the input image into large patches, significantly reducing token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover essential visual details with minimal additional cost. This dynamic resolution assignment strategy allows MambaScope to allocate computation adaptively according to image complexity, achieving efficient processing without compromising accuracy. Experiments across various vision tasks demonstrate that MambaScope outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.",
    "authors": [
      "Shanhui Liu",
      "Rui Xu",
      "Yunke Wang"
    ],
    "published": "2025-11-29T21:58:08+00:00",
    "url": "https://arxiv.org/pdf/2512.00647v2",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00641v1",
    "title": "Graph-Attention Network with Adversarial Domain Alignment for Robust Cross-Domain Facial Expression Recognition",
    "abstract": "Cross-domain facial expression recognition (CD-FER) remains difficult due to severe domain shift between training and deployment data. We propose Graph-Attention Network with Adversarial Domain Alignment (GAT-ADA), a hybrid framework that couples a ResNet-50 as backbone with a batch-level Graph Attention Network (GAT) to model inter-sample relations under shift. Each mini-batch is cast as a sparse ring graph so that attention aggregates cross-sample cues that are informative for adaptation. To align distributions, GAT-ADA combines adversarial learning via a Gradient Reversal Layer (GRL) with statistical alignment using CORAL and MMD. GAT-ADA is evaluated under a standard unsupervised domain adaptation protocol: training on one labeled source (RAF-DB) and adapting to multiple unlabeled targets (CK+, JAFFE, SFEW 2.0, FER2013, and ExpW). GAT-ADA attains 74.39% mean cross-domain accuracy. On RAF-DB to FER2013, it reaches 98.0% accuracy, corresponding to approximately a 36-point improvement over the best baseline we re-implemented with the same backbone and preprocessing.",
    "authors": [
      "Razieh Ghaedi",
      "AmirReza BabaAhmadi",
      "Reyer Zwiggelaar",
      "Xinqi Fan",
      "Nashid Alam"
    ],
    "published": "2025-11-29T21:32:03+00:00",
    "url": "https://arxiv.org/pdf/2512.00641v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00639v1",
    "title": "Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation",
    "abstract": "The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.",
    "authors": [
      "Mahmoud El Hussieni"
    ],
    "published": "2025-11-29T21:24:36+00:00",
    "url": "https://arxiv.org/pdf/2512.00639v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "cs.PF"
    ]
  },
  {
    "arxiv_id": "2512.00626v1",
    "title": "XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance",
    "abstract": "Accurate and timely diagnosis of multi-class skin lesions is hampered by subjective methods, inherent data imbalance in datasets like HAM10000, and the \"black box\" nature of Deep Learning (DL) models. This study proposes a trustworthy and highly accurate Computer-Aided Diagnosis (CAD) system to overcome these limitations. The approach utilizes Deep Convolutional Generative Adversarial Networks (DCGANs) for per class data augmentation to resolve the critical class imbalance problem. A fine-tuned ResNet-50 classifier is then trained on the augmented dataset to classify seven skin disease categories. Crucially, LIME and SHAP Explainable AI (XAI) techniques are integrated to provide transparency by confirming that predictions are based on clinically relevant features like irregular morphology. The system achieved a high overall Accuracy of 92.50 % and a Macro-AUC of 98.82 %, successfully outperforming various prior benchmarked architectures. This work successfully validates a verifiable framework that combines high performance with the essential clinical interpretability required for safe diagnostic deployment. Future research should prioritize enhancing discrimination for critical categories, such as Melanoma NOS (F1-Score is 0.8602).",
    "authors": [
      "Kim Gerard A. Villanueva",
      "Priyanka Kumar"
    ],
    "published": "2025-11-29T20:46:30+00:00",
    "url": "https://arxiv.org/pdf/2512.00626v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00625v1",
    "title": "Automatic Pith Detection in Tree Cross-Section Images Using Deep Learning",
    "abstract": "Pith detection in tree cross-sections is essential for forestry and wood quality analysis but remains a manual, error-prone task. This study evaluates deep learning models -- YOLOv9, U-Net, Swin Transformer, DeepLabV3, and Mask R-CNN -- to automate the process efficiently. A dataset of 582 labeled images was dynamically augmented to improve generalization. Swin Transformer achieved the highest accuracy (0.94), excelling in fine segmentation. YOLOv9 performed well for bounding box detection but struggled with boundary precision. U-Net was effective for structured patterns, while DeepLabV3 captured multi-scale features with slight boundary imprecision. Mask R-CNN initially underperformed due to overlapping detections, but applying Non-Maximum Suppression (NMS) improved its IoU from 0.45 to 0.80. Generalizability was next tested using an oak dataset of 11 images from Oregon State University's Tree Ring Lab. Additionally, for exploratory analysis purposes, an additional dataset of 64 labeled tree cross-sections was used to train the worst-performing model to see if this would improve its performance generalizing to the unseen oak dataset. Key challenges included tensor mismatches and boundary inconsistencies, addressed through hyperparameter tuning and augmentation. Our results highlight deep learning's potential for tree cross-section pith detection, with model choice depending on dataset characteristics and application needs.",
    "authors": [
      "Tzu-I Liao",
      "Mahmoud Fakhry",
      "Jibin Yesudas Varghese"
    ],
    "published": "2025-11-29T20:43:04+00:00",
    "url": "https://arxiv.org/pdf/2512.00625v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00621v1",
    "title": "Melody or Machine: Detecting Synthetic Music with Dual-Stream Contrastive Learning",
    "abstract": "The rapid evolution of end-to-end AI music generation poses an escalating threat to artistic authenticity and copyright, demanding detection methods that can keep pace. While foundational, existing models like SpecTTTra falter when faced with the diverse and rapidly advancing ecosystem of new generators, exhibiting significant performance drops on out-of-distribution (OOD) content. This generalization failure highlights a critical gap: the need for more challenging benchmarks and more robust detection architectures. To address this, we first introduce Melody or Machine (MoM), a new large-scale benchmark of over 130,000 songs (6,665 hours). MoM is the most diverse dataset to date, built with a mix of open and closed-source models and a curated OOD test set designed specifically to foster the development of truly generalizable detectors. Alongside this benchmark, we introduce CLAM, a novel dual-stream detection architecture. We hypothesize that subtle, machine-induced inconsistencies between vocal and instrumental elements, often imperceptible in a mixed signal, offer a powerful tell-tale sign of synthesis. CLAM is designed to test this hypothesis by employing two distinct pre-trained audio encoders (MERT and Wave2Vec2) to create parallel representations of the audio. These representations are fused by a learnable cross-aggregation module that models their inter-dependencies. The model is trained with a dual-loss objective: a standard binary cross-entropy loss for classification, complemented by a contrastive triplet loss which trains the model to distinguish between coherent and artificially mismatched stream pairings, enhancing its sensitivity to synthetic artifacts without presuming a simple feature alignment. CLAM establishes a new state-of-the-art in synthetic music forensics. It achieves an F1 score of 0.925 on our challenging MoM benchmark.",
    "authors": [
      "Arnesh Batra",
      "Dev Sharma",
      "Krish Thukral",
      "Ruhani Bhatia",
      "Naman Batra",
      "Aditya Gautam"
    ],
    "published": "2025-11-29T20:25:20+00:00",
    "url": "https://arxiv.org/pdf/2512.00621v1",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00619v1",
    "title": "Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies",
    "abstract": "Continual learning remains a fundamental challenge in artificial intelligence, with catastrophic forgetting posing a significant barrier to deploying neural networks in dynamic environments. Inspired by biological memory consolidation mechanisms, we propose a novel framework for generative replay that leverages predictive coding principles to mitigate forgetting. We present a comprehensive comparison between predictive coding-based and backpropagation-based gen- erative replay strategies, evaluating their effectiveness on task retention and transfer efficiency across multiple benchmark datasets. Our experimental results demonstrate that predictive coding-based replay achieves superior retention performance (average 15.3% improvement) while maintaining competitive transfer efficiency, suggesting that biologically-inspired mechanisms can offer principled solutions to continual learning challenges. The proposed framework provides insights into the relationship between biological memory processes and artificial learning systems, opening new avenues for neuroscience-inspired AI research.",
    "authors": [
      "Goutham Nalagatla",
      "Shreyas Grandhe"
    ],
    "published": "2025-11-29T20:20:52+00:00",
    "url": "https://arxiv.org/pdf/2512.00619v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00617v1",
    "title": "ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.",
    "authors": [
      "Omer Jauhar Khan"
    ],
    "published": "2025-11-29T20:16:11+00:00",
    "url": "https://arxiv.org/pdf/2512.00617v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00616v1",
    "title": "Stable Voting and the Splitting of Cycles",
    "abstract": "Algorithms for resolving majority cycles in preference aggregation have been studied extensively in computational social choice. Several sophisticated cycle-resolving methods, including Tideman's Ranked Pairs, Schulze's Beat Path, and Heitzig's River, are refinements of the Split Cycle (SC) method that resolves majority cycles by discarding the weakest majority victories in each cycle. Recently, Holliday and Pacuit proposed a new refinement of Split Cycle, dubbed Stable Voting, and a simplification thereof, called Simple Stable Voting (SSV). They conjectured that SSV is a refinement of SC whenever no two majority victories are of the same size. In this paper, we prove the conjecture up to 6 alternatives and refute it for more than 6 alternatives. While our proof of the conjecture for up to 5 alternatives uses traditional mathematical reasoning, our 6-alternative proof and 7-alternative counterexample were obtained with the use of SAT solving. The SAT encoding underlying this proof and counterexample is applicable far beyond SC and SSV: it can be used to test properties of any voting method whose choice of winners depends only on the ordering of margins of victory by size.",
    "authors": [
      "Wesley H. Holliday",
      "Milan Moss\u00e9",
      "Chase Norman",
      "Eric Pacuit",
      "Cynthia Wang"
    ],
    "published": "2025-11-29T20:13:27+00:00",
    "url": "https://arxiv.org/pdf/2512.00616v1",
    "categories": [
      "cs.GT",
      "cs.AI",
      "econ.TH"
    ]
  },
  {
    "arxiv_id": "2512.00614v1",
    "title": "Hierarchical Decentralized Multi-Agent Coordination with Privacy-Preserving Knowledge Sharing: Extending AgentNet for Scalable Autonomous Systems",
    "abstract": "Decentralized multi-agent systems have shown promise in enabling autonomous collaboration among LLM-based agents. While AgentNet demonstrated the feasibility of fully decentralized coordination through dynamic DAG topologies, several limitations remain: scalability challenges with large agent populations, communication overhead, lack of privacy guarantees, and suboptimal resource allocation. We propose AgentNet++, a hierarchical decentralized framework that extends AgentNet with multilevel agent organization, privacy-preserving knowledge sharing via differential privacy and secure aggregation, adaptive resource management, and theoretical convergence guarantees. Our approach introduces cluster-based hierarchies where agents self-organize into specialized groups, enabling efficient task routing and knowledge distillation while maintaining full decentralization. We provide formal analysis of convergence properties and privacy bounds, and demonstrate through extensive experiments on complex multi-agent tasks that AgentNet++ achieves 23% higher task completion rates, 40% reduction in communication overhead, and maintains strong privacy guarantees compared to AgentNet and other baselines. Our framework scales effectively to 1000+ agents while preserving the emergent intelligence properties of the original AgentNet.",
    "authors": [
      "Goutham Nalagatla"
    ],
    "published": "2025-11-29T20:07:20+00:00",
    "url": "https://arxiv.org/pdf/2512.00614v1",
    "categories": [
      "cs.MA",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00612v1",
    "title": "Generalized Graph Transformer Variational Autoencoder",
    "abstract": "Graph link prediction has long been a central problem in graph representation learning in both network analysis and generative modeling. Recent progress in deep learning has introduced increasingly sophisticated architectures for capturing relational dependencies within graph-structured data. In this work, we propose the Generalized Graph Transformer Variational Autoencoder (GGT-VAE). Our model integrates Generalized Graph Transformer Architecture with Variational Autoencoder framework for link prediction. Unlike prior GraphVAE, GCN, or GNN approaches, GGT-VAE leverages transformer style global self-attention mechanism along with laplacian positional encoding to model structural patterns across nodes into a latent space without relying on message passing. Experimental results on several benchmark datasets demonstrate that GGT-VAE consistently achieves above-baseline performance in terms of ROC-AUC and Average Precision. To the best of our knowledge, this is among the first studies to explore graph structure generation using a generalized graph transformer backbone in a variational framework.",
    "authors": [
      "Siddhant Karki"
    ],
    "published": "2025-11-29T19:53:44+00:00",
    "url": "https://arxiv.org/pdf/2512.00612v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00611v1",
    "title": "Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior",
    "abstract": "Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.",
    "authors": [
      "Franck Binard",
      "Vanja Kljajevic"
    ],
    "published": "2025-11-29T19:52:21+00:00",
    "url": "https://arxiv.org/pdf/2512.00611v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00607v1",
    "title": "On the Holographic Geometry of Deterministic Computation",
    "abstract": "Standard simulations of Turing machines suggest a linear relationship between the temporal duration $t$ of a run and the amount of information that must be stored by known simulations to certify, verify, or regenerate the configuration at time $t$. For deterministic multitape Turing machines over a fixed finite alphabet, this apparent linear dependence is not intrinsic: any length-$t$ run can be simulated in space $O(\\sqrt{t})$ via a Height Compression Theorem for succinct computation trees together with an Algebraic Replay Engine. In this paper we recast that construction in geometric and information-theoretic language. We interpret the execution trace as a spacetime dependency DAG and exhibit a family of recursively defined holographic boundary summaries such that, along the square-root-space simulation, the total description length of all boundary data stored at any time is $O(\\sqrt{t})$. Using Kolmogorov complexity, we prove that every internal configuration has constant conditional description complexity given the appropriate boundary summary and time index, establishing that the spacetime bulk carries no additional algorithmic information beyond its boundary. We express this as a one-dimensional computational area law: there exists a simulation in which the information capacity of the active \"holographic screen'' needed to generate a spacetime region of volume $t$ is bounded by $O(\\sqrt{t})$. In this precise sense, deterministic computation on a one-dimensional work tape admits a holographic representation, with the bulk history algebraically determined by data residing on a lower-dimensional boundary screen.",
    "authors": [
      "Logan Nye"
    ],
    "published": "2025-11-29T19:47:22+00:00",
    "url": "https://arxiv.org/pdf/2512.00607v1",
    "categories": [
      "cs.CC",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00602v1",
    "title": "AgentODRL: A Large Language Model-based Multi-agent System for ODRL Generation",
    "abstract": "The Open Digital Rights Language (ODRL) is a pivotal standard for automating data rights management. However, the inherent logical complexity of authorization policies, combined with the scarcity of high-quality \"Natural Language-to-ODRL\" training datasets, impedes the ability of current methods to efficiently and accurately translate complex rules from natural language into the ODRL format. To address this challenge, this research leverages the potent comprehension and generation capabilities of Large Language Models (LLMs) to achieve both automation and high fidelity in this translation process. We introduce AgentODRL, a multi-agent system based on an Orchestrator-Workers architecture. The architecture consists of specialized Workers, including a Generator for ODRL policy creation, a Decomposer for breaking down complex use cases, and a Rewriter for simplifying nested logical relationships. The Orchestrator agent dynamically coordinates these Workers, assembling an optimal pathway based on the complexity of the input use case. Specifically, we enhance the ODRL Generator by incorporating a validator-based syntax strategy and a semantic reflection mechanism powered by a LoRA-finetuned model, significantly elevating the quality of the generated policies. Extensive experiments were conducted on a newly constructed dataset comprising 770 use cases of varying complexity, all situated within the context of data spaces. The results, evaluated using ODRL syntax and semantic scores, demonstrate that our proposed Orchestrator-Workers system, enhanced with these strategies, achieves superior performance on the ODRL generation task.",
    "authors": [
      "Wanle Zhong",
      "Keman Huang",
      "Xiaoyong Du"
    ],
    "published": "2025-11-29T19:19:50+00:00",
    "url": "https://arxiv.org/pdf/2512.00602v1",
    "categories": [
      "cs.MA",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02069v1",
    "title": "Large Language Model based Smart Contract Auditing with LLMBugScanner",
    "abstract": "This paper presents LLMBugScanner, a large language model (LLM) based framework for smart contract vulnerability detection using fine-tuning and ensemble learning. Smart contract auditing presents several challenges for LLMs: different pretrained models exhibit varying reasoning abilities, and no single model performs consistently well across all vulnerability types or contract structures. These limitations persist even after fine-tuning individual LLMs.   To address these challenges, LLMBugScanner combines domain knowledge adaptation with ensemble reasoning to improve robustness and generalization. Through domain knowledge adaptation, we fine-tune LLMs on complementary datasets to capture both general code semantics and instruction-guided vulnerability reasoning, using parameter-efficient tuning to reduce computational cost. Through ensemble reasoning, we leverage the complementary strengths of multiple LLMs and apply a consensus-based conflict resolution strategy to produce more reliable vulnerability assessments.   We conduct extensive experiments across multiple popular LLMs and compare LLMBugScanner with both pretrained and fine-tuned individual models. Results show that LLMBugScanner achieves consistent accuracy improvements and stronger generalization, demonstrating that it provides a principled, cost-effective, and extensible framework for smart contract auditing.",
    "authors": [
      "Yining Yuan",
      "Yifei Wang",
      "Yichang Xu",
      "Zachary Yahn",
      "Sihao Hu",
      "Ling Liu"
    ],
    "published": "2025-11-29T19:13:44+00:00",
    "url": "https://arxiv.org/pdf/2512.02069v1",
    "categories": [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00601v2",
    "title": "Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization",
    "abstract": "Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.",
    "authors": [
      "Boyang Gu",
      "Hongjian Zhou",
      "Bradley Max Segal",
      "Jinge Wu",
      "Zeyu Cao",
      "Hantao Zhong",
      "Lei Clifton",
      "Fenglin Liu",
      "David A. Clifton"
    ],
    "published": "2025-11-29T19:09:24+00:00",
    "url": "https://arxiv.org/pdf/2512.00601v2",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00598v1",
    "title": "Developing Fairness-Aware Task Decomposition to Improve Equity in Post-Spinal Fusion Complication Prediction",
    "abstract": "Fairness in clinical prediction models remains a persistent challenge, particularly in high-stakes applications such as spinal fusion surgery for scoliosis, where patient outcomes exhibit substantial heterogeneity. Many existing fairness approaches rely on coarse demographic adjustments or post-hoc corrections, which fail to capture the latent structure of clinical populations and may unintentionally reinforce bias. We propose FAIR-MTL, a fairness-aware multitask learning framework designed to provide equitable and fine-grained prediction of postoperative complication severity.   Instead of relying on explicit sensitive attributes during model training, FAIR-MTL employs a data-driven subgroup inference mechanism. We extract a compact demographic embedding, and apply k-means clustering to uncover latent patient subgroups that may be differentially affected by traditional models. These inferred subgroup labels determine task routing within a shared multitask architecture. During training, subgroup imbalance is mitigated through inverse-frequency weighting, and regularization prevents overfitting to smaller groups.   Applied to postoperative complication prediction with four severity levels, FAIR-MTL achieves an AUC of 0.86 and an accuracy of 75%, outperforming single-task baselines while substantially reducing bias. For gender, the demographic parity difference decreases to 0.055 and equalized odds to 0.094; for age, these values reduce to 0.056 and 0.148, respectively. Model interpretability is ensured through SHAP and Gini importance analyses, which consistently highlight clinically meaningful predictors such as hemoglobin, hematocrit, and patient weight. Our findings show that incorporating unsupervised subgroup discovery into a multitask framework enables more equitable, interpretable, and clinically actionable predictions for surgical risk stratification.",
    "authors": [
      "Yining Yuan",
      "J. Ben Tamo",
      "Wenqi Shi",
      "Yishan Zhong",
      "Micky C. Nnamdi",
      "B. Randall Brenn",
      "Steven W. Hwang",
      "May D. Wang"
    ],
    "published": "2025-11-29T19:06:07+00:00",
    "url": "https://arxiv.org/pdf/2512.00598v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.00597v1",
    "title": "Scaling Down to Scale Up: Towards Operationally-Efficient and Deployable Clinical Models via Cross-Modal Low-Rank Adaptation for Medical Vision-Language Models",
    "abstract": "Foundation models trained via vision-language pretraining have demonstrated strong zero-shot capabilities across diverse image domains, yet their application to volumetric medical imaging remains limited. We introduce MedCT-VLM: Medical CT Vision-Language Model, a parameter-efficient vision-language framework designed to adapt large-scale CT foundation models for downstream clinical tasks. MedCT-VLM uses a parameter-efficient approach to adapt CT-CLIP, a contrastive vision-language model trained on 25,692 chest CT volumes, for multi-label pathology classification using Low-Rank Adaptation (LoRA). Rather than fine-tuning the model's 440 M parameters directly, we insert low-rank decomposition matrices into attention layers of both vision and text encoders, training only 1.67M parameters (0.38\\% of total). We evaluate on zero-shot classification across 18 thoracic pathologies, where the model must align CT embeddings with unseen text prompts at inference without task-specific training. LoRA fine-tuning improves mean AUROC from 61.3\\% to 68.9\\% (+7.6 pp), accuracy from 67.2\\% to 73.6\\% (+6.4 pp), and macro-F1 from 32.1\\% to 36.9\\% (+4.8 pp). These results demonstrate that parameter-efficient methods can effectively transfer large-scale pretraining to downstream medical imaging tasks, particularly for zero-shot scenarios where labeled data is scarce.",
    "authors": [
      "Thuraya Alzubaidi",
      "Farhad R. Nezami",
      "Muzammil Behzad"
    ],
    "published": "2025-11-29T19:03:25+00:00",
    "url": "https://arxiv.org/pdf/2512.00597v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00596v1",
    "title": "DLRREC: Denoising Latent Representations via Multi-Modal Knowledge Fusion in Deep Recommender Systems",
    "abstract": "Modern recommender systems struggle to effectively utilize the rich, yet high-dimensional and noisy, multi-modal features generated by Large Language Models (LLMs). Treating these features as static inputs decouples them from the core recommendation task. We address this limitation with a novel framework built on a key insight: deeply fusing multi-modal and collaborative knowledge for representation denoising. Our unified architecture introduces two primary technical innovations. First, we integrate dimensionality reduction directly into the recommendation model, enabling end-to-end co-training that makes the reduction process aware of the final ranking objective. Second, we introduce a contrastive learning objective that explicitly incorporates the collaborative filtering signal into the latent space. This synergistic process refines raw LLM embeddings, filtering noise while amplifying task-relevant signals. Extensive experiments confirm our method's superior discriminative power, proving that this integrated fusion and denoising strategy is critical for achieving state-of-the-art performance. Our work provides a foundational paradigm for effectively harnessing LLMs in recommender systems.",
    "authors": [
      "Jiahao Tian",
      "Zhenkai Wang"
    ],
    "published": "2025-11-29T18:57:42+00:00",
    "url": "https://arxiv.org/pdf/2512.00596v1",
    "categories": [
      "cs.IR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00595v1",
    "title": "IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference",
    "abstract": "Modern AI inference faces an irreducible tension: no single computational resource simultaneously maximizes performance, preserves privacy, minimizes cost, and maintains trust. Existing orchestration frameworks optimize single dimensions (Kubernetes prioritizes latency, federated learning preserves privacy, edge computing reduces network distance), creating solutions that struggle under real-world heterogeneity. We present IslandRun, a multi-objective orchestration system that treats computational resources as autonomous \"islands\" spanning personal devices, private edge servers, and public cloud. Our key insights: (1) request-level heterogeneity demands policy-constrained multi-objective optimization, (2) data locality enables routing compute to data rather than data to compute, and (3) typed placeholder sanitization preserves context semantics across trust boundaries. IslandRun introduces agent-based routing, tiered island groups with differential trust, and reversible anonymization. This establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous personal computing ecosystems.",
    "authors": [
      "Bala Siva Sai Akhil Malepati"
    ],
    "published": "2025-11-29T18:52:27+00:00",
    "url": "https://arxiv.org/pdf/2512.00595v1",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CR"
    ]
  },
  {
    "arxiv_id": "2512.00590v1",
    "title": "Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models",
    "abstract": "Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.",
    "authors": [
      "Alla Chepurova",
      "Aydar Bulatov",
      "Yuri Kuratov",
      "Mikhail Burtsev"
    ],
    "published": "2025-11-29T18:44:25+00:00",
    "url": "https://arxiv.org/pdf/2512.00590v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00586v1",
    "title": "Statistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical R&D",
    "abstract": "This work presents the development and evaluation of an NLP-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. While pharmaceutical R&D is plagued by high attrition rates and enormous costs, particularly within neuroscience, where success rates are below 10%, timely identification of promising programs can streamline resource allocation and reduce financial risk. Leveraging data from the ClinicalTrials.gov database and success labels from the recently developed Clinical Trial Outcome dataset, the classifier extracts text-based clinical trial features using statistical NLP techniques. These features were integrated into several non-LLM frameworks (logistic regression, gradient boosting, and random forest) to generate calibrated probability scores. Model performance was assessed on a retrospective dataset of 101,145 completed clinical trials spanning 1976-2024, achieving an overall ROC-AUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-specific language representation encoder. The BioBERT-based model achieved an overall ROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, 40% less squared error than would be observed using industry benchmarks. The BioBERT-based model also made trial outcome predictions that were superior to benchmark values 70% of the time overall. By integrating NLP-driven insights into drug development decision-making, this work aims to enhance strategic planning and optimize investment allocation in neuroscience programs.",
    "authors": [
      "Michael R. Doane"
    ],
    "published": "2025-11-29T18:40:42+00:00",
    "url": "https://arxiv.org/pdf/2512.00586v1",
    "categories": [
      "cs.LG",
      "cs.CL",
      "q-bio.QM"
    ]
  },
  {
    "arxiv_id": "2512.00582v1",
    "title": "SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension",
    "abstract": "Satire, a form of artistic expression combining humor with implicit critique, holds significant social value by illuminating societal issues. Despite its cultural and societal significance, satire comprehension, particularly in purely visual forms, remains a challenging task for current vision-language models. This task requires not only detecting satire but also deciphering its nuanced meaning and identifying the implicated entities. Existing models often fail to effectively integrate local entity relationships with global context, leading to misinterpretation, comprehension biases, and hallucinations. To address these limitations, we propose SatireDecoder, a training-free framework designed to enhance satirical image comprehension. Our approach proposes a multi-agent system performing visual cascaded decoupling to decompose images into fine-grained local and global semantic representations. In addition, we introduce a chain-of-thought reasoning strategy guided by uncertainty analysis, which breaks down the complex satire comprehension process into sequential subtasks with minimized uncertainty. Our method significantly improves interpretive accuracy while reducing hallucinations. Experimental results validate that SatireDecoder outperforms existing baselines in comprehending visual satire, offering a promising direction for vision-language reasoning in nuanced, high-level semantic tasks.",
    "authors": [
      "Yue Jiang",
      "Haiwei Xue",
      "Minghao Han",
      "Mingcheng Li",
      "Xiaolu Hou",
      "Dingkang Yang",
      "Lihua Zhang",
      "Xu Zheng"
    ],
    "published": "2025-11-29T18:27:50+00:00",
    "url": "https://arxiv.org/pdf/2512.00582v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00579v1",
    "title": "Slovak Conceptual Dictionary",
    "abstract": "When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.",
    "authors": [
      "Miroslav Bl\u0161t\u00e1k"
    ],
    "published": "2025-11-29T18:15:28+00:00",
    "url": "https://arxiv.org/pdf/2512.00579v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00572v1",
    "title": "Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models",
    "abstract": "Yoga is a popular form of exercise worldwide due to its spiritual and physical health benefits, but incorrect postures can lead to injuries. Automated yoga pose classification has therefore gained importance to reduce reliance on expert practitioners. While human pose keypoint extraction models have shown high potential in action recognition, systematic benchmarking for yoga pose recognition remains limited, as prior works often focus solely on raw images or a single pose extraction model. In this study, we introduce a curated dataset, 'Yoga-16', which addresses limitations of existing datasets, and systematically evaluate three deep learning architectures (VGG16, ResNet50, and Xception) using three input modalities (direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images). Our experiments demonstrate that skeleton-based representations outperform raw image inputs, with the highest accuracy of 96.09% achieved by VGG16 with MediaPipe Pose skeleton input. Additionally, we provide interpretability analysis using Grad-CAM, offering insights into model decision-making for yoga pose classification with cross validation analysis.",
    "authors": [
      "Mohammed Mohiuddin",
      "Syed Mohammod Minhaz Hossain",
      "Sumaiya Khanam",
      "Prionkar Barua",
      "Aparup Barua",
      "MD Tamim Hossain"
    ],
    "published": "2025-11-29T18:00:33+00:00",
    "url": "https://arxiv.org/pdf/2512.00572v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00571v1",
    "title": "Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization",
    "abstract": "Analogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble.",
    "authors": [
      "Tarun Chintada",
      "Uday Kiran Cheera"
    ],
    "published": "2025-11-29T17:56:51+00:00",
    "url": "https://arxiv.org/pdf/2512.00571v1",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ]
  },
  {
    "arxiv_id": "2512.02066v1",
    "title": "Parallel Multi-Circuit Quantum Feature Fusion in Hybrid Quantum-Classical Convolutional Neural Networks for Breast Tumor Classification",
    "abstract": "Quantum machine learning has emerged as a promising approach to improve feature extraction and classification tasks in high-dimensional data domains such as medical imaging. In this work, we present a hybrid Quantum-Classical Convolutional Neural Network (QCNN) architecture designed for the binary classification of the BreastMNIST dataset, a standardized benchmark for distinguishing between benign and malignant breast tumors. Our architecture integrates classical convolutional feature extraction with two distinct quantum circuits: an amplitude-encoding variational quantum circuit (VQC) and an angle-encoding VQC circuit with circular entanglement, both implemented on four qubits. These circuits generate quantum feature embeddings that are fused with classical features to form a joint feature space, which is subsequently processed by a fully connected classifier. To ensure fairness, the hybrid QCNN is parameter-matched against a baseline classical CNN, allowing us to isolate the contribution of quantum layers. Both models are trained under identical conditions using the Adam optimizer and binary cross-entropy loss. Experimental evaluation in five independent runs demonstrates that the hybrid QCNN achieves statistically significant improvements in classification accuracy compared to the classical CNN, as validated by a one-sided Wilcoxon signed rank test (p = 0.03125) and supported by large effect size of Cohen's d = 2.14. Our results indicate that hybrid QCNN architectures can leverage entanglement and quantum feature fusion to enhance medical image classification tasks. This work establishes a statistical validation framework for assessing hybrid quantum models in biomedical applications and highlights pathways for scaling to larger datasets and deployment on near-term quantum hardware.",
    "authors": [
      "Ece Yurtseven"
    ],
    "published": "2025-11-29T17:47:14+00:00",
    "url": "https://arxiv.org/pdf/2512.02066v1",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ]
  },
  {
    "arxiv_id": "2512.00565v1",
    "title": "Describe Anything Anywhere At Any Moment",
    "abstract": "Computer vision and robotics applications ranging from augmented reality to robot autonomy in large-scale environments require spatio-temporal memory frameworks that capture both geometric structure for accurate language-grounding as well as semantic detail. Existing methods face a tradeoff, where producing rich open-vocabulary descriptions comes at the expense of real-time performance when these descriptions have to be grounded in 3D. To address these challenges, we propose Describe Anything, Anywhere, at Any Moment (DAAAM), a novel spatio-temporal memory framework for large-scale and real-time 4D scene understanding. DAAAM introduces a novel optimization-based frontend to infer detailed semantic descriptions from localized captioning models, such as the Describe Anything Model (DAM), leveraging batch processing to speed up inference by an order of magnitude for online processing. It leverages such semantic understanding to build a hierarchical 4D scene graph (SG), which acts as an effective globally spatially and temporally consistent memory representation. DAAAM constructs 4D SGs with detailed, geometrically grounded descriptions while maintaining real-time performance. We show that DAAAM's 4D SG interfaces well with a tool-calling agent for inference and reasoning.   We thoroughly evaluate DAAAM in the complex task of spatio-temporal question answering on the NaVQA benchmark and show its generalization capabilities for sequential task grounding on the SG3D benchmark. We further curate an extended OC-NaVQA benchmark for large-scale and long-time evaluations. DAAAM achieves state-of-the-art results in both tasks, improving OC-NaVQA question accuracy by 53.6%, position errors by 21.9%, temporal errors by 21.6%, and SG3D task grounding accuracy by 27.8% over the most competitive baselines, respectively. We release our data and code open-source.",
    "authors": [
      "Nicolas Gorlo",
      "Lukas Schmid",
      "Luca Carlone"
    ],
    "published": "2025-11-29T17:27:17+00:00",
    "url": "https://arxiv.org/pdf/2512.00565v1",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.00563v1",
    "title": "Explainable Multi-Modal Deep Learning for Automatic Detection of Lung Diseases from Respiratory Audio Signals",
    "abstract": "Respiratory diseases remain major global health challenges, and traditional auscultation is often limited by subjectivity, environmental noise, and inter-clinician variability. This study presents an explainable multimodal deep learning framework for automatic lung-disease detection using respiratory audio signals. The proposed system integrates two complementary representations: a spectral-temporal encoder based on a CNN-BiLSTM Attention architecture, and a handcrafted acoustic-feature encoder capturing physiologically meaningful descriptors such as MFCCs, spectral centroid, spectral bandwidth, and zero-crossing rate. These branches are combined through late-stage fusion to leverage both data-driven learning and domain-informed acoustic cues. The model is trained and evaluated on the Asthma Detection Dataset Version 2 using rigorous preprocessing, including resampling, normalization, noise filtering, data augmentation, and patient-level stratified partitioning. The study achieved strong generalization with 91.21% accuracy, 0.899 macro F1-score, and 0.9866 macro ROC-AUC, outperforming all ablated variants. An ablation study confirms the importance of temporal modeling, attention mechanisms, and multimodal fusion. The framework incorporates Grad-CAM, Integrated Gradients, and SHAP, generating interpretable spectral, temporal, and feature-level explanations aligned with known acoustic biomarkers to build clinical transparency. The findings demonstrate the framework's potential for telemedicine, point-of-care diagnostics, and real-world respiratory screening.",
    "authors": [
      "S M Asiful Islam Saky",
      "Md Rashidul Islam",
      "Md Saiful Arefin",
      "Shahaba Alam"
    ],
    "published": "2025-11-29T17:15:58+00:00",
    "url": "https://arxiv.org/pdf/2512.00563v1",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00557v1",
    "title": "NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives",
    "abstract": "What visual information is encoded in individual brain regions, and how do distributed patterns combine to create their neural representations? Prior work has used generative models to replicate known category selectivity in isolated regions (e.g., faces in FFA), but these approaches offer limited insight into how regions interact during complex, naturalistic vision. We introduce NeuroVolve, a generative framework that provides brain-guided image synthesis via optimization of a neural objective function in the embedding space of a pretrained vision-language model. Images are generated under the guidance of a programmable neural objective, i.e., activating or deactivating single regions or multiple regions together. NeuroVolve is validated by recovering known selectivity for individual brain regions, while expanding to synthesize coherent scenes that satisfy complex, multi-region constraints. By tracking optimization steps, it reveals semantic trajectories through embedding space, unifying brain-guided image editing and preferred stimulus generation in a single process. We show that NeuroVolve can generate both low-level and semantic feature-specific stimuli for single ROIs, as well as stimuli aligned to curated neural objectives. These include co-activation and decorrelation between regions, exposing cooperative and antagonistic tuning relationships. Notably, the framework captures subject-specific preferences, supporting personalized brain-driven synthesis and offering interpretable constraints for mapping, analyzing, and probing neural representations of visual information.",
    "authors": [
      "Haomiao Chen",
      "Keith W Jamison",
      "Mert R. Sabuncu",
      "Amy Kuceyeski"
    ],
    "published": "2025-11-29T16:57:59+00:00",
    "url": "https://arxiv.org/pdf/2512.00557v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00556v1",
    "title": "Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations",
    "abstract": "The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.",
    "authors": [
      "Sina Salimian",
      "Gias Uddin",
      "Sumon Biswas",
      "Henry Leung"
    ],
    "published": "2025-11-29T16:56:38+00:00",
    "url": "https://arxiv.org/pdf/2512.00556v1",
    "categories": [
      "cs.SE",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00553v1",
    "title": "List Replicable Reinforcement Learning",
    "abstract": "Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \\emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \\emph{small list} of policies across different runs, with high probability. The size of this list defines the \\emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \\emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.",
    "authors": [
      "Bohan Zhang",
      "Michael Chen",
      "A. Pavan",
      "N. V. Vinodchandran",
      "Lin F. Yang",
      "Ruosong Wang"
    ],
    "published": "2025-11-29T16:47:43+00:00",
    "url": "https://arxiv.org/pdf/2512.00553v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.00552v1",
    "title": "Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity",
    "abstract": "Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.",
    "authors": [
      "Subramanyam Sahoo",
      "Vinija Jain",
      "Saanidhya Vats",
      "Siddharth Mohapatra",
      "Rui Min",
      "Aman Chadha",
      "Divya Chaudhary"
    ],
    "published": "2025-11-29T16:47:01+00:00",
    "url": "https://arxiv.org/pdf/2512.00552v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00547v1",
    "title": "Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions",
    "abstract": "Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \\ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.",
    "authors": [
      "Sandika Biswas",
      "Qianyi Wu",
      "Biplab Banerjee",
      "Hamid Rezatofighi"
    ],
    "published": "2025-11-29T16:36:22+00:00",
    "url": "https://arxiv.org/pdf/2512.00547v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00539v1",
    "title": "SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning",
    "abstract": "The widespread misuse of image generation technologies has raised security concerns, driving the development of AI-generated image detection methods. However, generalization has become a key challenge and open problem: existing approaches struggle to adapt to emerging generative methods and content types in real-world scenarios. To address this issue, we propose a Scene-Aware and Importance-Guided Dynamic Optimization detection framework with continual learning (SAIDO). Specifically, we design Scene-Awareness-Based Expert Module (SAEM) that dynamically identifies and incorporates new scenes using VLLMs. For each scene, independent expert modules are dynamically allocated, enabling the framework to capture scene-specific forgery features better and enhance cross-scene generalization. To mitigate catastrophic forgetting when learning from multiple image generative methods, we introduce Importance-Guided Dynamic Optimization Mechanism (IDOM), which optimizes each neuron through an importance-guided gradient projection strategy, thereby achieving an effective balance between model plasticity and stability. Extensive experiments on continual learning tasks demonstrate that our method outperforms the current SOTA method in both stability and plasticity, achieving 44.22\\% and 40.57\\% relative reductions in average detection error rate and forgetting rate, respectively. On open-world datasets, it improves the average detection accuracy by 9.47\\% compared to the current SOTA method.",
    "authors": [
      "Yongkang Hu",
      "Yu Cheng",
      "Yushuo Zhang",
      "Yuan Xie",
      "Zhaoxia Yin"
    ],
    "published": "2025-11-29T16:13:50+00:00",
    "url": "https://arxiv.org/pdf/2512.00539v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00534v1",
    "title": "Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update",
    "abstract": "Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.",
    "authors": [
      "Zeyuan An",
      "Yanghang Xiao",
      "Zhiying Leng",
      "Frederick W. B. Li",
      "Xiaohui Liang"
    ],
    "published": "2025-11-29T16:00:24+00:00",
    "url": "https://arxiv.org/pdf/2512.00534v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00532v1",
    "title": "Image Generation as a Visual Planner for Robotic Manipulation",
    "abstract": "Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling.   We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions.   Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \\href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}.",
    "authors": [
      "Ye Pang"
    ],
    "published": "2025-11-29T15:54:16+00:00",
    "url": "https://arxiv.org/pdf/2512.00532v1",
    "categories": [
      "cs.CV",
      "cs.RO"
    ]
  },
  {
    "arxiv_id": "2512.00521v1",
    "title": "Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction",
    "abstract": "In early stage drug discovery, bioactivity prediction of molecules against target proteins plays a crucial role. Trdaitional QSAR models that utilizes molecular descriptor based data often struggles to predict bioactivity of molecules effectively due to its limitation in capturing structural and contextual information embedded within each compound. To address this challenge, we propose Rep3Net, a unified deep learning architecture that not only incorporates descriptor data but also includes spatial and relational information through graph-based represenation of compounds and contextual information through ChemBERTa generated embeddings from SMILES strings. Our model employing multimodal concatenated features produce reliable bioactivity prediction on Poly [ADP-ribose] polymerase 1 (PARP-1) dataset. PARP-1 is a crucial agent in DNA damage repair and has become a significant theraputic target in malignancies that depend on it for survival and growth. A comprehensive analysis and comparison with conventional standalone models including GCN, GAT, XGBoost, etc. demonstrates that our architecture achieves the highest predictive performance. In computational screening of compounds in drug discovery, our architecture provides a scalable framework for bioactivity prediction.",
    "authors": [
      "Sabrina Islam",
      "Md. Atiqur Rahman",
      "Md. Bakhtiar Hasan",
      "Md. Hasanul Kabir"
    ],
    "published": "2025-11-29T15:39:48+00:00",
    "url": "https://arxiv.org/pdf/2512.00521v1",
    "categories": [
      "cs.LG",
      "cs.CL",
      "q-bio.QM"
    ]
  },
  {
    "arxiv_id": "2512.00515v1",
    "title": "Developing a Comprehensive Framework for Sentiment Analysis in Turkish",
    "abstract": "In this thesis, we developed a comprehensive framework for sentiment analysis that takes its many aspects into account mainly for Turkish. We have also proposed several approaches specific to sentiment analysis in English only. We have accordingly made five major and three minor contributions. We generated a novel and effective feature set by combining unsupervised, semi-supervised, and supervised metrics. We then fed them as input into classical machine learning methods, and outperformed neural network models for datasets of different genres in both Turkish and English. We created a polarity lexicon with a semi-supervised domain-specific method, which has been the first approach applied for corpora in Turkish. We performed a fine morphological analysis for the sentiment classification task in Turkish by determining the polarities of morphemes. This can be adapted to other morphologically-rich or agglutinative languages as well. We have built a novel neural network architecture, which combines recurrent and recursive neural network models for English. We built novel word embeddings that exploit sentiment, syntactic, semantic, and lexical characteristics for both Turkish and English. We also redefined context windows as subclauses in modelling word representations in English. This can also be applied to other linguistic fields and natural language processing tasks. We have achieved state-of-the-art and significant results for all these original approaches. Our minor contributions include methods related to aspect-based sentiment in Turkish, parameter redefinition in the semi-supervised approach, and aspect term extraction techniques for English. This thesis can be considered the most detailed and comprehensive study made on sentiment analysis in Turkish as of July, 2020. Our work has also contributed to the opinion classification problem in English.",
    "authors": [
      "Cem Rifki Aydin"
    ],
    "published": "2025-11-29T15:14:57+00:00",
    "url": "https://arxiv.org/pdf/2512.00515v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00514v1",
    "title": "Terrain Sensing with Smartphone Structured Light: 2D Dynamic Time Warping for Grid Pattern Matching",
    "abstract": "Low-cost mobile rovers often operate on uneven terrain where small bumps or tilts are difficult to perceive visually but can significantly affect locomotion stability. To address this problem, we explore a smartphone-based structured-light system that projects a grid pattern onto the ground and reconstructs local terrain unevenness from a single handheld device. The system is inspired by face-recognition projectors, but adapted for ground sensing. A key technical challenge is robustly matching the projected grid with its deformed observation under perspective distortion and partial occlusion. Conventional one-dimensional dynamic time warping (1D-DTW) is not directly applicable to such two-dimensional grid patterns. We therefore propose a topology-constrained two-dimensional dynamic time warping (2D-DTW) algorithm that performs column-wise alignment under a global grid consistency constraint. The proposed method is designed to be simple enough to run on resource limited platforms while preserving the grid structure required for accurate triangulation. We demonstrate that our 2D-DTW formulation can be used not only for terrain sensing but also as a general tool for matching structured grid patterns in image processing scenarios. This paper describes the overall system design as well as the 2D-DTW extension that emerged from this application.",
    "authors": [
      "Tanaka Nobuaki"
    ],
    "published": "2025-11-29T15:09:55+00:00",
    "url": "https://arxiv.org/pdf/2512.00514v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03089v1",
    "title": "Password-Activated Shutdown Protocols for Misaligned Frontier Agents",
    "abstract": "Frontier AI developers may fail to align or control highly-capable AI agents. In many cases, it could be useful to have emergency shutdown mechanisms which effectively prevent misaligned agents from carrying out harmful actions in the world. We introduce password-activated shutdown protocols (PAS protocols) -- methods for designing frontier agents to implement a safe shutdown protocol when given a password. We motivate PAS protocols by describing intuitive use-cases in which they mitigate risks from misaligned systems that subvert other control efforts, for instance, by disabling automated monitors or self-exfiltrating to external data centres. PAS protocols supplement other safety efforts, such as alignment fine-tuning or monitoring, contributing to defence-in-depth against AI risk. We provide a concrete demonstration in SHADE-Arena, a benchmark for AI monitoring and subversion capabilities, in which PAS protocols supplement monitoring to increase safety with little cost to performance. Next, PAS protocols should be robust to malicious actors who want to bypass shutdown. Therefore, we conduct a red-team blue-team game between the developers (blue-team), who must implement a robust PAS protocol, and a red-team trying to subvert the protocol. We conduct experiments in a code-generation setting, finding that there are effective strategies for the red-team, such as using another model to filter inputs, or fine-tuning the model to prevent shutdown behaviour. We then outline key challenges to implementing PAS protocols in real-life systems, including: security considerations of the password and decisions regarding when, and in which systems, to use them. PAS protocols are an intuitive mechanism for increasing the safety of frontier AI. We encourage developers to consider implementing PAS protocols prior to internal deployment of particularly dangerous systems to reduce loss-of-control risks.",
    "authors": [
      "Kai Williams",
      "Rohan Subramani",
      "Francis Rhys Ward"
    ],
    "published": "2025-11-29T14:49:53+00:00",
    "url": "https://arxiv.org/pdf/2512.03089v1",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00504v1",
    "title": "G-KV: Decoding-Time KV Cache Eviction with Global Attention",
    "abstract": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.",
    "authors": [
      "Mengqi Liao",
      "Lu Wang",
      "Chaoyun Zhang",
      "Zekai Shen",
      "Xiaowei Mao",
      "Si Qin",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Huaiyu Wan"
    ],
    "published": "2025-11-29T14:21:33+00:00",
    "url": "https://arxiv.org/pdf/2512.00504v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00499v1",
    "title": "ESPO: Entropy Importance Sampling Policy Optimization",
    "abstract": "Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.",
    "authors": [
      "Yuepeng Sheng",
      "Yuwei Huang",
      "Shuman Liu",
      "Haibo Zhang",
      "Anxiang Zeng"
    ],
    "published": "2025-11-29T14:09:38+00:00",
    "url": "https://arxiv.org/pdf/2512.00499v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.00496v1",
    "title": "CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning",
    "abstract": "As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.",
    "authors": [
      "Diego A. B. Moreira",
      "Alef I. Ferreira",
      "Jhessica Silva",
      "Gabriel O. dos Santos",
      "Gustavo Bonil",
      "Jo\u00e3o Gondim",
      "Marina dos Santos",
      "Helena Maia",
      "Simone Hashiguti",
      "N\u00e1dia da Silva",
      "Carolina Scarton",
      "Helio Pedrini",
      "Sandra Avila"
    ],
    "published": "2025-11-29T14:04:27+00:00",
    "url": "https://arxiv.org/pdf/2512.00496v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00493v1",
    "title": "CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration",
    "abstract": "High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.",
    "authors": [
      "Boshi Tang",
      "Henry Zheng",
      "Rui Huang",
      "Gao Huang"
    ],
    "published": "2025-11-29T14:01:13+00:00",
    "url": "https://arxiv.org/pdf/2512.00493v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00489v1",
    "title": "Learning What Helps: Task-Aligned Context Selection for Vision Tasks",
    "abstract": "Humans often resolve visual uncertainty by comparing an image with relevant examples, but ViTs lack the ability to identify which examples would improve their predictions. We present Task-Aligned Context Selection (TACS), a framework that learns to select paired examples which truly improve task performance rather than those that merely appear similar. TACS jointly trains a selector network with the task model through a hybrid optimization scheme combining gradient-based supervision and reinforcement learning, making retrieval part of the learning objective. By aligning selection with task rewards, TACS enables discriminative models to discover which contextual examples genuinely help. Across 18 datasets covering fine-grained recognition, medical image classification, and medical image segmentation, TACS consistently outperforms similarity-based retrieval, particularly in challenging or data-limited settings.",
    "authors": [
      "Jingyu Guo",
      "Emir Konuk",
      "Fredrik Strand",
      "Christos Matsoukas",
      "Kevin Smith"
    ],
    "published": "2025-11-29T13:47:35+00:00",
    "url": "https://arxiv.org/pdf/2512.00489v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00479v1",
    "title": "Mind the data gap: Missingness Still Shapes Large Language Model Prognoses",
    "abstract": "Data collection often reflects human decisions. In healthcare, for instance, a referral for a diagnostic test is influenced by the patient's health, their preferences, available resources, and the practitioner's recommendations. Despite the extensive literature on the informativeness of missingness, its implications on the performance of Large Language Models (LLMs) have not been studied. Through a series of experiments on data from Columbia University Medical Center, a large urban academic medical center, and MIMIC-IV, we demonstrate that patterns of missingness significantly impact zero-shot predictive performance. Notably, the explicit inclusion of missingness indicators at prompting benefits some while hurting other LLMs' zero-shot predictive performance and calibration, suggesting an inconsistent impact. The proposed aggregated analysis and theoretical insights suggest that larger models benefit from these interventions, while smaller models can be negatively impacted. The LLM paradigm risks obscuring the impact of missingness, often neglected even in conventional ML, even further. We conclude that there is a need for more transparent accounting and systematic evaluation of the impact of representing (informative) missingness on downstream performance.",
    "authors": [
      "Yuta Kobayashi",
      "Vincent Jeanselme",
      "Shalmali Joshi"
    ],
    "published": "2025-11-29T13:24:07+00:00",
    "url": "https://arxiv.org/pdf/2512.00479v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00475v1",
    "title": "Structured Context Learning for Generic Event Boundary Detection",
    "abstract": "Generic Event Boundary Detection (GEBD) aims to identify moments in videos that humans perceive as event boundaries. This paper proposes a novel method for addressing this task, called Structured Context Learning, which introduces the Structured Partition of Sequence (SPoS) to provide a structured context for learning temporal information. Our approach is end-to-end trainable and flexible, not restricted to specific temporal models like GRU, LSTM, and Transformers. This flexibility enables our method to achieve a better speed-accuracy trade-off. Specifically, we apply SPoS to partition the input frame sequence and provide a structured context for the subsequent temporal model. Notably, SPoS's overall computational complexity is linear with respect to the video length. We next calculate group similarities to capture differences between frames, and a lightweight fully convolutional network is utilized to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, we adapt the Gaussian kernel to preprocess the ground-truth event boundaries. Our proposed method has been extensively evaluated on the challenging Kinetics-GEBD, TAPOS, and shot transition detection datasets, demonstrating its superiority over existing state-of-the-art methods.",
    "authors": [
      "Xin Gu",
      "Congcong Li",
      "Xinyao Wang",
      "Dexiang Hong",
      "Libo Zhang",
      "Tiejian Luo",
      "Longyin Wen",
      "Heng Fan"
    ],
    "published": "2025-11-29T13:06:52+00:00",
    "url": "https://arxiv.org/pdf/2512.00475v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00473v1",
    "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
    "abstract": "With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce \" fake\" images with distinct AI artifacts, often characterized by \"overly smooth skin\" and \"oily facial sheens\". To recapture the original goal of \"indistinguishable-from-reality\" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a \"Detector Reward\" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.",
    "authors": [
      "Junyan Ye",
      "Leiqi Zhu",
      "Yuncheng Guo",
      "Dongzhi Jiang",
      "Zilong Huang",
      "Yifan Zhang",
      "Zhiyuan Yan",
      "Haohuan Fu",
      "Conghui He",
      "Weijia Li"
    ],
    "published": "2025-11-29T12:52:26+00:00",
    "url": "https://arxiv.org/pdf/2512.00473v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00469v1",
    "title": "FairMT: Fairness for Heterogeneous Multi-Task Learning",
    "abstract": "Fairness in machine learning has been extensively studied in single-task settings, while fair multi-task learning (MTL), especially with heterogeneous tasks (classification, detection, regression) and partially missing labels, remains largely unexplored. Existing fairness methods are predominantly classification-oriented and fail to extend to continuous outputs, making a unified fairness objective difficult to formulate. Further, existing MTL optimization is structurally misaligned with fairness: constraining only the shared representation, allowing task heads to absorb bias and leading to uncontrolled task-specific disparities. Finally, most work treats fairness as a zero-sum trade-off with utility, enforcing symmetric constraints that achieve parity by degrading well-served groups. We introduce FairMT, a unified fairness-aware MTL framework that accommodates all three task types under incomplete supervision. At its core is an Asymmetric Heterogeneous Fairness Constraint Aggregation mechanism, which consolidates task-dependent asymmetric violations into a unified fairness constraint. Utility and fairness are jointly optimized via a primal--dual formulation, while a head-aware multi-objective optimization proxy provides a tractable descent geometry that explicitly accounts for head-induced anisotropy. Across three homogeneous and heterogeneous MTL benchmarks encompassing diverse modalities and supervision regimes, FairMT consistently achieves substantial fairness gains while maintaining superior task utility. Code will be released upon paper acceptance.",
    "authors": [
      "Guanyu Hu",
      "Tangzheng Lian",
      "Na Yan",
      "Dimitrios Kollias",
      "Xinyu Yang",
      "Oya Celiktutan",
      "Siyang Song",
      "Zeyu Fu"
    ],
    "published": "2025-11-29T12:44:51+00:00",
    "url": "https://arxiv.org/pdf/2512.00469v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.00466v1",
    "title": "SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling",
    "abstract": "Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \\textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.",
    "authors": [
      "Yang Xiao",
      "Chunpu Xu",
      "Ruifeng Yuan",
      "Jiashuo Wang",
      "Wenjie Li",
      "Pengfei Liu"
    ],
    "published": "2025-11-29T12:38:07+00:00",
    "url": "https://arxiv.org/pdf/2512.00466v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00461v1",
    "title": "Whose Personae? Synthetic Persona Experiments in LLM Research and Pathways to Transparency",
    "abstract": "Synthetic personae experiments have become a prominent method in Large Language Model alignment research, yet the representativeness and ecological validity of these personae vary considerably between studies. Through a review of 63 peer-reviewed studies published between 2023 and 2025 in leading NLP and AI venues, we reveal a critical gap: task and population of interest are often underspecified in persona-based experiments, despite personalization being fundamentally dependent on these criteria. Our analysis shows substantial differences in user representation, with most studies focusing on limited sociodemographic attributes and only 35% discussing the representativeness of their LLM personae. Based on our findings, we introduce a persona transparency checklist that emphasizes representative sampling, explicit grounding in empirical data, and enhanced ecological validity. Our work provides both a comprehensive assessment of current practices and practical guidelines to improve the rigor and ecological validity of persona-based evaluations in language model alignment research.",
    "authors": [
      "Jan Batzner",
      "Volker Stocker",
      "Bingjun Tang",
      "Anusha Natarajan",
      "Qinhao Chen",
      "Stefan Schmid",
      "Gjergji Kasneci"
    ],
    "published": "2025-11-29T12:27:34+00:00",
    "url": "https://arxiv.org/pdf/2512.00461v1",
    "categories": [
      "cs.CY",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00456v1",
    "title": "CausalAffect: Causal Discovery for Facial Affective Understanding",
    "abstract": "Understanding human affect from facial behavior requires not only accurate recognition but also structured reasoning over the latent dependencies that drive muscle activations and their expressive outcomes. Although Action Units (AUs) have long served as the foundation of affective computing, existing approaches rarely address how to infer psychologically plausible causal relations between AUs and expressions directly from data. We propose CausalAffect, the first framework for causal graph discovery in facial affect analysis. CausalAffect models AU-AU and AU-Expression dependencies through a two-level polarity and direction aware causal hierarchy that integrates population-level regularities with sample-adaptive structures. A feature-level counterfactual intervention mechanism further enforces true causal effects while suppressing spurious correlations. Crucially, our approach requires neither jointly annotated datasets nor handcrafted causal priors, yet it recovers causal structures consistent with established psychological theories while revealing novel inhibitory and previously uncharacterized dependencies. Extensive experiments across six benchmarks demonstrate that CausalAffect advances the state of the art in both AU detection and expression recognition, establishing a principled connection between causal discovery and interpretable facial behavior. All trained models and source code will be released upon acceptance.",
    "authors": [
      "Guanyu Hu",
      "Tangzheng Lian",
      "Dimitrios Kollias",
      "Oya Celiktutan",
      "Xinyu Yang"
    ],
    "published": "2025-11-29T12:07:33+00:00",
    "url": "https://arxiv.org/pdf/2512.00456v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00453v1",
    "title": "Sample-Efficient Expert Query Control in Active Imitation Learning via Conformal Prediction",
    "abstract": "Active imitation learning (AIL) combats covariate shift by querying an expert during training. However, expert action labeling often dominates the cost, especially in GPU-intensive simulators, human-in-the-loop settings, and robot fleets that revisit near-duplicate states. We present Conformalized Rejection Sampling for Active Imitation Learning (CRSAIL), a querying rule that requests an expert action only when the visited state is under-represented in the expert-labeled dataset. CRSAIL scores state novelty by the distance to the $K$-th nearest expert state and sets a single global threshold via conformal prediction. This threshold is the empirical $(1-\u03b1)$ quantile of on-policy calibration scores, providing a distribution-free calibration rule that links $\u03b1$ to the expected query rate and makes $\u03b1$ a task-agnostic tuning knob. This state-space querying strategy is robust to outliers and, unlike safety-gate-based AIL, can be run without real-time expert takeovers: we roll out full trajectories (episodes) with the learner and only afterward query the expert on a subset of visited states. Evaluated on MuJoCo robotics tasks, CRSAIL matches or exceeds expert-level reward while reducing total expert queries by up to 96% vs. DAgger and up to 65% vs. prior AIL methods, with empirical robustness to $\u03b1$ and $K$, easing deployment on novel systems with unknown dynamics.",
    "authors": [
      "Arad Firouzkouhi",
      "Omid Mirzaeedodangeh",
      "Lars Lindemann"
    ],
    "published": "2025-11-29T11:58:21+00:00",
    "url": "https://arxiv.org/pdf/2512.00453v1",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00450v1",
    "title": "RecruitView: A Multimodal Dataset for Predicting Personality and Interview Performance for Human Resources Applications",
    "abstract": "Automated personality and soft skill assessment from multimodal behavioral data remains challenging due to limited datasets and methods that fail to capture geometric structure inherent in human traits. We introduce RecruitView, a dataset of 2,011 naturalistic video interview clips from 300+ participants with 27,000 pairwise comparative judgments across 12 dimensions: Big Five personality traits, overall personality score, and six interview performance metrics. To leverage this data, we propose Cross-Modal Regression with Manifold Fusion (CRMF), a geometric deep learning framework that explicitly models behavioral representations across hyperbolic, spherical, and Euclidean manifolds. CRMF employs geometry-specific expert networks to capture hierarchical trait structures, directional behavioral patterns, and continuous performance variations simultaneously. An adaptive routing mechanism dynamically weights expert contributions based on input characteristics. Through principled tangent space fusion, CRMF achieves superior performance while training 40-50% fewer trainable parameters than large multimodal models. Extensive experiments demonstrate that CRMF substantially outperforms the selected baselines, achieving up to 11.4% improvement in Spearman correlation and 6.0% in concordance index. Our RecruitView dataset is publicly available at https://huggingface.co/datasets/AI4A-lab/RecruitView",
    "authors": [
      "Amit Kumar Gupta",
      "Farhan Sheth",
      "Hammad Shaikh",
      "Dheeraj Kumar",
      "Angkul Puniya",
      "Deepak Panwar",
      "Sandeep Chaurasia",
      "Priya Mathur"
    ],
    "published": "2025-11-29T11:33:30+00:00",
    "url": "https://arxiv.org/pdf/2512.00450v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00439v1",
    "title": "PEOAT: Personalization-Guided Evolutionary Question Assembly for One-Shot Adaptive Testing",
    "abstract": "With the rapid advancement of intelligent education, Computerized Adaptive Testing (CAT) has attracted increasing attention by integrating educational psychology with deep learning technologies. Unlike traditional paper-and-pencil testing, CAT aims to efficiently and accurately assess examinee abilities by adaptively selecting the most suitable items during the assessment process. However, its real-time and sequential nature presents limitations in practical scenarios, particularly in large-scale assessments where interaction costs are high, or in sensitive domains such as psychological evaluations where minimizing noise and interference is essential. These challenges constrain the applicability of conventional CAT methods in time-sensitive or resourceconstrained environments. To this end, we first introduce a novel task called one-shot adaptive testing (OAT), which aims to select a fixed set of optimal items for each test-taker in a one-time selection. Meanwhile, we propose PEOAT, a Personalization-guided Evolutionary question assembly framework for One-shot Adaptive Testing from the perspective of combinatorial optimization. Specifically, we began by designing a personalization-aware initialization strategy that integrates differences between examinee ability and exercise difficulty, using multi-strategy sampling to construct a diverse and informative initial population. Building on this, we proposed a cognitive-enhanced evolutionary framework incorporating schema-preserving crossover and cognitively guided mutation to enable efficient exploration through informative signals. To maintain diversity without compromising fitness, we further introduced a diversity-aware environmental selection mechanism. The effectiveness of PEOAT is validated through extensive experiments on two datasets, complemented by case studies that uncovered valuable insights.",
    "authors": [
      "Xiaoshan Yu",
      "Ziwei Huang",
      "Shangshang Yang",
      "Ziwen Wang",
      "Haiping Ma",
      "Xingyi Zhang"
    ],
    "published": "2025-11-29T10:38:25+00:00",
    "url": "https://arxiv.org/pdf/2512.00439v1",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00438v1",
    "title": "FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal",
    "abstract": "Test-time scaling (TTS) has become a prevalent technique in image generation, significantly boosting output quality by expanding the number of parallel samples and filtering them using pre-trained reward models. However, applying this powerful methodology to the next-token prediction (NTP) paradigm remains challenging. The primary obstacle is the low correlation between the reward of an image decoded from an intermediate token sequence and the reward of the fully generated image. Consequently, these incomplete intermediate representations prove to be poor indicators for guiding the pruning direction, a limitation that stems from their inherent incompleteness in scale or semantic content. To effectively address this critical issue, we introduce the Filling-Based Reward (FR). This novel design estimates the approximate future trajectory of an intermediate sample by finding and applying a reasonable filling scheme to complete the sequence. Both the correlation coefficient between rewards of intermediate samples and final samples, as well as multiple intrinsic signals like token confidence, indicate that the FR provides an excellent and reliable metric for accurately evaluating the quality of intermediate samples. Building upon this foundation, we propose FR-TTS, a sophisticated scaling strategy. FR-TTS efficiently searches for good filling schemes and incorporates a diversity reward with a dynamic weighting schedule to achieve a balanced and comprehensive evaluation of intermediate samples. We experimentally validate the superiority of FR-TTS over multiple established benchmarks and various reward models. Code is available at \\href{https://github.com/xuhang07/FR-TTS}{https://github.com/xuhang07/FR-TTS}.",
    "authors": [
      "Hang Xu",
      "Linjiang Huang",
      "Feng Zhao"
    ],
    "published": "2025-11-29T10:34:16+00:00",
    "url": "https://arxiv.org/pdf/2512.00438v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00428v1",
    "title": "Recognizing Pneumonia in Real-World Chest X-rays with a Classifier Trained with Images Synthetically Generated by Nano Banana",
    "abstract": "We trained a classifier with synthetic chest X-ray (CXR) images generated by Nano Banana, the latest AI model for image generation and editing, released by Google. When directly applied to real-world CXRs having only been trained with synthetic data, the classifier achieved an AUROC of 0.923 (95% CI: 0.919 - 0.927), and an AUPR of 0.900 (95% CI: 0.894 - 0.907) in recognizing pneumonia in the 2018 RSNA Pneumonia Detection dataset (14,863 CXRs), and an AUROC of 0.824 (95% CI: 0.810 - 0.836), and an AUPR of 0.913 (95% CI: 0.904 - 0.922) in the Chest X-Ray dataset (5,856 CXRs). These external validation results on real-world data demonstrate the feasibility of this approach and suggest potential for synthetic data in medical AI development. Nonetheless, several limitations remain at present, including challenges in prompt design for controlling the diversity of synthetic CXR data and the requirement for post-processing to ensure alignment with real-world data. However, the growing sophistication and accessibility of medical intelligence will necessitate substantial validation, regulatory approval, and ethical oversight prior to clinical translation.",
    "authors": [
      "Jiachuan Peng",
      "Kyle Lam",
      "Jianing Qiu"
    ],
    "published": "2025-11-29T10:05:44+00:00",
    "url": "https://arxiv.org/pdf/2512.00428v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00425v1",
    "title": "What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards",
    "abstract": "Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\\texttt{NewtonRewards}$ extracts $\\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.",
    "authors": [
      "Minh-Quan Le",
      "Yuanzhi Zhu",
      "Vicky Kalogeiton",
      "Dimitris Samaras"
    ],
    "published": "2025-11-29T10:04:50+00:00",
    "url": "https://arxiv.org/pdf/2512.00425v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00424v1",
    "title": "Recovering Origin Destination Flows from Bus CCTV: Early Results from Nairobi and Kigali",
    "abstract": "Public transport in sub-Saharan Africa (SSA) often operates in overcrowded conditions where existing automated systems fail to capture reliable passenger flow data. Leveraging onboard CCTV already deployed for security, we present a baseline pipeline that combines YOLOv12 detection, BotSORT tracking, OSNet embeddings, OCR-based timestamping, and telematics-based stop classification to recover bus origin--destination (OD) flows. On annotated CCTV segments from Nairobi and Kigali buses, the system attains high counting accuracy under low-density, well-lit conditions (recall $\\approx$95\\%, precision $\\approx$91\\%, F1 $\\approx$93\\%). It produces OD matrices that closely match manual tallies. Under realistic stressors such as overcrowding, color-to-monochrome shifts, posture variation, and non-standard door use, performance degrades sharply (e.g., $\\sim$40\\% undercount in peak-hour boarding and a $\\sim$17 percentage-point drop in recall for monochrome segments), revealing deployment-specific failure modes and motivating more robust, deployment-focused Re-ID methods for SSA transit.",
    "authors": [
      "Nthenya Kyatha",
      "Jay Taneja"
    ],
    "published": "2025-11-29T10:03:17+00:00",
    "url": "https://arxiv.org/pdf/2512.00424v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00422v1",
    "title": "PhysGen: Physically Grounded 3D Shape Generation for Industrial Design",
    "abstract": "Existing generative models for 3D shapes can synthesize high-fidelity and visually plausible shapes. For certain classes of shapes that have undergone an engineering design process, the realism of the shape is tightly coupled with the underlying physical properties, e.g., aerodynamic efficiency for automobiles. Since existing methods lack knowledge of such physics, they are unable to use this knowledge to enhance the realism of shape generation. Motivated by this, we propose a unified physics-based 3D shape generation pipeline, with a focus on industrial design applications. Specifically, we introduce a new flow matching model with explicit physical guidance, consisting of an alternating update process. We iteratively perform a velocity-based update and a physics-based refinement, progressively adjusting the latent code to align with the desired 3D shapes and physical properties. We further strengthen physical validity by incorporating a physics-aware regularization term into the velocity-based update step. To support such physics-guided updates, we build a shape-and-physics variational autoencoder (SP-VAE) that jointly encodes shape and physics information into a unified latent space. The experiments on three benchmarks show that this synergistic formulation improves shape realism beyond mere visual plausibility.",
    "authors": [
      "Yingxuan You",
      "Chen Zhao",
      "Hantao Zhang",
      "Mingda Xu",
      "Pascal Fua"
    ],
    "published": "2025-11-29T10:00:35+00:00",
    "url": "https://arxiv.org/pdf/2512.00422v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00420v1",
    "title": "An Approach to Joint Hybrid Decision Making between Humans and Artificial Intelligence",
    "abstract": "Due to the progress in artificial intelligence, it is important to understand how capable artificial agents should be used when interacting with humans, since high level authority and responsibility often remain with the human agent. However, integrated frameworks are lacking that can account for heterogeneous agents and draw on different scientific fields, such as human-factors engineering and artificial intelligence. Therefore, joint hybrid intelligence is described as a framework abstracting humans and artificial intelligence as decision making agents. A general definition of intelligence is provided on the basis of decision making competence being applicable to agents of different sorts. This framework is used for proposing the interrelated design space of joint hybrid intelligence being aimed at integrating the heterogeneous capabilities of humans and artificial intelligence. At the core of this design space lies joint agent engineering with the goal of integrating the design subspaces operator training, artificial intelligence engineering, and interface design via developing joint agent patterns. The ''extended swarming'' approach to human-swarm interaction is discussed as an example of such a pattern.",
    "authors": [
      "Jonas D. Rockbach",
      "Sven Fuchs",
      "Maren Bennewitz"
    ],
    "published": "2025-11-29T09:53:28+00:00",
    "url": "https://arxiv.org/pdf/2512.00420v1",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00418v1",
    "title": "Significant Other AI: Identity, Memory, and Emotional Regulation as Long-Term Relational Intelligence",
    "abstract": "Significant Others (SOs) stabilize identity, regulate emotion, and support narrative meaning-making, yet many people today lack access to such relational anchors. Recent advances in large language models and memory-augmented AI raise the question of whether artificial systems could support some of these functions. Existing empathic AIs, however, remain reactive and short-term, lacking autobiographical memory, identity modeling, predictive emotional regulation, and narrative coherence. This manuscript introduces Significant Other Artificial Intelligence (SO-AI) as a new domain of relational AI. It synthesizes psychological and sociological theory to define SO functions and derives requirements for SO-AI, including identity awareness, long-term memory, proactive support, narrative co-construction, and ethical boundary enforcement. A conceptual architecture is proposed, comprising an anthropomorphic interface, a relational cognition layer, and a governance layer. A research agenda outlines methods for evaluating identity stability, longitudinal interaction patterns, narrative development, and sociocultural impact. SO-AI reframes AI-human relationships as long-term, identity-bearing partnerships and provides a foundational blueprint for investigating whether AI can responsibly augment the relational stability many individuals lack today.",
    "authors": [
      "Sung Park"
    ],
    "published": "2025-11-29T09:52:59+00:00",
    "url": "https://arxiv.org/pdf/2512.00418v1",
    "categories": [
      "cs.HC",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00417v2",
    "title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency",
    "abstract": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.   Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.",
    "authors": [
      "Jiacheng Guo",
      "Suozhi Huang",
      "Zixin Yao",
      "Yifan Zhang",
      "Yifu Lu",
      "Jiashuo Liu",
      "Zihao Li",
      "Nicholas Deng",
      "Qixin Xiao",
      "Jia Tian",
      "Kanghong Zhan",
      "Tianyi Li",
      "Xiaochen Liu",
      "Jason Ge",
      "Chaoyang He",
      "Kaixuan Huang",
      "Lin Yang",
      "Wenhao Huang",
      "Mengdi Wang"
    ],
    "published": "2025-11-29T09:52:34+00:00",
    "url": "https://arxiv.org/pdf/2512.00417v2",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00413v1",
    "title": "SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control",
    "abstract": "Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.",
    "authors": [
      "Ji Gan",
      "Lingxu Chen",
      "Jiaxu Leng",
      "Xinbo Gao"
    ],
    "published": "2025-11-29T09:46:37+00:00",
    "url": "https://arxiv.org/pdf/2512.00413v1",
    "categories": [
      "cs.CV",
      "cs.GR"
    ]
  },
  {
    "arxiv_id": "2512.00412v1",
    "title": "Red Teaming Large Reasoning Models",
    "abstract": "Large Reasoning Models (LRMs) have emerged as a powerful advancement in multi-step reasoning tasks, offering enhanced transparency and logical consistency through explicit chains of thought (CoT). However, these models introduce novel safety and reliability risks, such as CoT-hijacking and prompt-induced inefficiencies, which are not fully captured by existing evaluation methods. To address this gap, we propose RT-LRM, a unified benchmark designed to assess the trustworthiness of LRMs. RT-LRM evaluates three core dimensions: truthfulness, safety and efficiency. Beyond metric-based evaluation, we further introduce the training paradigm as a key analytical perspective to investigate the systematic impact of different training strategies on model trustworthiness. We achieve this by designing a curated suite of 30 reasoning tasks from an observational standpoint. We conduct extensive experiments on 26 models and identify several valuable insights into the trustworthiness of LRMs. For example, LRMs generally face trustworthiness challenges and tend to be more fragile than Large Language Models (LLMs) when encountering reasoning-induced risks. These findings uncover previously underexplored vulnerabilities and highlight the need for more targeted evaluations. In addition, we release a scalable toolbox for standardized trustworthiness research to support future advancements in this important field. Our code and datasets will be open-sourced.",
    "authors": [
      "Jiawei Chen",
      "Yang Yang",
      "Chao Yu",
      "Yu Tian",
      "Zhi Cao",
      "Linghao Li",
      "Hang Su",
      "Zhaoxia Yin"
    ],
    "published": "2025-11-29T09:45:03+00:00",
    "url": "https://arxiv.org/pdf/2512.00412v1",
    "categories": [
      "cs.CR",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00410v1",
    "title": "Balancing Efficiency and Fairness: An Iterative Exchange Framework for Multi-UAV Cooperative Path Planning",
    "abstract": "Multi-UAV cooperative path planning (MUCPP) is a fundamental problem in multi-agent systems, aiming to generate collision-free trajectories for a team of unmanned aerial vehicles (UAVs) to complete distributed tasks efficiently. A key challenge lies in achieving both efficiency, by minimizing total mission cost, and fairness, by balancing the workload among UAVs to avoid overburdening individual agents. This paper presents a novel Iterative Exchange Framework for MUCPP, balancing efficiency and fairness through iterative task exchanges and path refinements. The proposed framework formulates a composite objective that combines the total mission distance and the makespan, and iteratively improves the solution via local exchanges under feasibility and safety constraints. For each UAV, collision-free trajectories are generated using A* search over a terrain-aware configuration space. Comprehensive experiments on multiple terrain datasets demonstrate that the proposed method consistently achieves superior trade-offs between total distance and makespan compared to existing baselines.",
    "authors": [
      "Hongzong Li",
      "Luwei Liao",
      "Xiangguang Dai",
      "Yuming Feng",
      "Rong Feng",
      "Shiqin Tang"
    ],
    "published": "2025-11-29T09:41:22+00:00",
    "url": "https://arxiv.org/pdf/2512.00410v1",
    "categories": [
      "cs.RO",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00408v1",
    "title": "Low-Bitrate Video Compression through Semantic-Conditioned Diffusion",
    "abstract": "Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.",
    "authors": [
      "Lingdong Wang",
      "Guan-Ming Su",
      "Divya Kothandaraman",
      "Tsung-Wei Huang",
      "Mohammad Hajiesmaili",
      "Ramesh K. Sitaraman"
    ],
    "published": "2025-11-29T09:38:16+00:00",
    "url": "https://arxiv.org/pdf/2512.00408v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00406v1",
    "title": "GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework",
    "abstract": "Building design directly affects human well-being and carbon emissions, yet generating spatial-functional and energy-compliant floorplans remains manual, costly, and non-scalable. Existing methods produce visually plausible layouts but frequently violate key constraints, yielding invalid results due to the absence of automated evaluation. We present GreenPlanner, an energy- and functionality-aware generative framework that unifies design evaluation and generation. It consists of a labeled Design Feasibility Dataset for learning constraint priors; a fast Practical Design Evaluator (PDE) for predicting energy performance and spatial-functional validity; a Green Plan Dataset (GreenPD) derived from PDE-guided filtering to pair user requirements with regulation-compliant layouts; and a GreenFlow generator trained on GreenPD with PDE feedback for controllable, regulation-aware generation. Experiments show that GreenPlanner accelerates evaluation by over $10^{5}\\times$ with $>$99% accuracy, eliminates invalid samples, and boosts design efficiency by 87% over professional architects.",
    "authors": [
      "Pengyu Zeng",
      "Yuqin Dai",
      "Jun Yin",
      "Jing Zhong",
      "Ziyang Han",
      "Chaoyang Shi",
      "ZhanXiang Jin",
      "Maowei Jiang",
      "Yuxing Han",
      "Shuai Lu"
    ],
    "published": "2025-11-29T09:35:50+00:00",
    "url": "https://arxiv.org/pdf/2512.00406v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00403v1",
    "title": "SelfAI: Building a Self-Training AI System with LLM Agents",
    "abstract": "Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \\textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.",
    "authors": [
      "Xiao Wu",
      "Ting-Zhu Huang",
      "Liang-Jian Deng",
      "Xiaobing Yu",
      "Yu Zhong",
      "Shangqi Deng",
      "Ufaq Khan",
      "Jianghao Wu",
      "Xiaofeng Liu",
      "Imran Razzak",
      "Xiaojun Chang",
      "Yutong Xie"
    ],
    "published": "2025-11-29T09:18:39+00:00",
    "url": "https://arxiv.org/pdf/2512.00403v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00396v1",
    "title": "Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement",
    "abstract": "We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.",
    "authors": [
      "Andrea Procopio",
      "Marco Esposito",
      "Sara Raggiunto",
      "Andrey Gizdov",
      "Alberto Belli",
      "Paola Pierleoni"
    ],
    "published": "2025-11-29T08:52:41+00:00",
    "url": "https://arxiv.org/pdf/2512.00396v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ]
  },
  {
    "arxiv_id": "2512.00395v1",
    "title": "Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction",
    "abstract": "Integrating segmentation into Multimodal Large Language Models (MLLMs) presents a core trilemma: simultaneously preserving dialogue ability, achieving high segmentation performance, and ensuring fast inference. Prevailing paradigms are forced into a compromise. Embedding prediction methods introduce a conflicting pixel-level objective that degrades the MLLM's general dialogue abilities. The alternative, next-token prediction, reframes segmentation as an autoregressive task, which preserves dialogue but forces a trade-off between poor segmentation performance with sparse outputs or prohibitive inference speeds with rich ones. We resolve this trilemma with all-mask prediction, a novel paradigm that decouples autoregressive dialogue generation from non-autoregressive mask prediction. We present STAMP: Simultaneous Textual All-Mask Prediction, an MLLM that embodies this paradigm. After generating a textual response, STAMP predicts an entire segmentation mask in a single forward pass by treating it as a parallel \"fill-in-the-blank\" task over image patches. This design maintains the MLLM's dialogue ability by avoiding conflicting objectives, enables high segmentation performance by leveraging rich, bidirectional spatial context for all mask tokens, and achieves exceptional speed. Extensive experiments show that STAMP significantly outperforms state-of-the-art methods across multiple segmentation benchmarks, providing a solution that excels in dialogue, segmentation, and speed without compromise.",
    "authors": [
      "Jiazhen Liu",
      "Mingkuan Feng",
      "Long Chen"
    ],
    "published": "2025-11-29T08:52:41+00:00",
    "url": "https://arxiv.org/pdf/2512.00395v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00392v1",
    "title": "A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction",
    "abstract": "This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used \"English as she is spoke\" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.",
    "authors": [
      "Damian Heywood",
      "Joseph Andrew Carrier",
      "Kyu-Hong Hwang"
    ],
    "published": "2025-11-29T08:45:00+00:00",
    "url": "https://arxiv.org/pdf/2512.00392v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00391v1",
    "title": "From Coefficients to Directions: Rethinking Model Merging with Directional Alignment",
    "abstract": "Model merging has emerged as a practical paradigm for integrating multiple independently trained models into a single model without joint retraining. Previous studies have demonstrated the effectiveness of combining parameters through strategies such as parameter decomposition, coefficient optimization, and subspace learning, significantly reducing the need for expensive joint training and achieving strong empirical performance across diverse tasks. However, these approaches predominantly treat merging as a problem of parameter space decomposition or fusion coefficient optimization, while overlooking the critical role of directional information in both parameter and feature spaces. In practice, na\u00efve merging introduces inconsistencies in dominant parameter directions and disrupts structural coherence across models, which can degrade performance. Moreover, coefficient-based optimization methods implicitly assume compatible feature-space directions across models. However, Neural Collapse indicates that class features follow structured directional patterns, which may differ across independently trained models, making coefficient optimization alone insufficient. In this work, we emphasize the importance of \\emph{directional alignment} and introduce a unified geometric framework, \\emph{Merging with Directional Alignment} (\\method{}), which aligns directional structures consistently in both the parameter and feature spaces. Our analysis shows that directional alignment improves structural coherence, and extensive experiments across benchmarks, model scales, and task configurations further validate the effectiveness of our approach.",
    "authors": [
      "Zhikang Chen",
      "Sen Cui",
      "Deheng Ye",
      "Min Zhang",
      "Gang Niu",
      "Yu Zhang",
      "Masashi Sugiyama",
      "Tingting Zhu"
    ],
    "published": "2025-11-29T08:40:58+00:00",
    "url": "https://arxiv.org/pdf/2512.00391v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00390v1",
    "title": "Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing",
    "abstract": "Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.",
    "authors": [
      "Nuo Chen",
      "Hanpei Fang",
      "Jiqun Liu",
      "Wilson Wei",
      "Tetsuya Sakai",
      "Xiao-Ming Wu"
    ],
    "published": "2025-11-29T08:37:51+00:00",
    "url": "https://arxiv.org/pdf/2512.00390v1",
    "categories": [
      "cs.CL",
      "cs.IR"
    ]
  },
  {
    "arxiv_id": "2512.00387v1",
    "title": "WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing",
    "abstract": "Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.",
    "authors": [
      "Kaihang Pan",
      "Weile Chen",
      "Haiyi Qiu",
      "Qifan Yu",
      "Wendong Bu",
      "Zehan Wang",
      "Yun Zhu",
      "Juncheng Li",
      "Siliang Tang"
    ],
    "published": "2025-11-29T08:32:35+00:00",
    "url": "https://arxiv.org/pdf/2512.00387v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00385v1",
    "title": "EZ-SP: Fast and Lightweight Superpoint-Based 3D Segmentation",
    "abstract": "Superpoint-based pipelines provide an efficient alternative to point- or voxel-based 3D semantic segmentation, but are often bottlenecked by their CPU-bound partition step. We propose a learnable, fully GPU partitioning algorithm that generates geometrically and semantically coherent superpoints 13$\\times$ faster than prior methods. Our module is compact (under 60k parameters), trains in under 20 minutes with a differentiable surrogate loss, and requires no handcrafted features. Combine with a lightweight superpoint classifier, the full pipeline fits in $<$2 MB of VRAM, scales to multi-million-point scenes, and supports real-time inference. With 72$\\times$ faster inference and 120$\\times$ fewer parameters, EZ-SP matches the accuracy of point-based SOTA models across three domains: indoor scans (S3DIS), autonomous driving (KITTI-360), and aerial LiDAR (DALES). Code and pretrained models are accessible at github.com/drprojects/superpoint_transformer.",
    "authors": [
      "Louis Geist",
      "Loic Landrieu",
      "Damien Robert"
    ],
    "published": "2025-11-29T08:21:51+00:00",
    "url": "https://arxiv.org/pdf/2512.00385v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00383v1",
    "title": "An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines",
    "abstract": "We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.",
    "authors": [
      "Jianhai Su",
      "Jinzhu Luo",
      "Qi Zhang"
    ],
    "published": "2025-11-29T08:17:03+00:00",
    "url": "https://arxiv.org/pdf/2512.00383v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ]
  },
  {
    "arxiv_id": "2512.00381v1",
    "title": "Pore-scale Image Patch Dataset and A Comparative Evaluation of Pore-scale Facial Features",
    "abstract": "The weak-texture nature of facial skin regions presents significant challenges for local descriptor matching in applications such as facial motion analysis and 3D face reconstruction. Although deep learning-based descriptors have demonstrated superior performance to traditional hand-crafted descriptors in many applications, the scarcity of pore-scale image patch datasets has hindered their further development in the facial domain. In this paper, we propose the PorePatch dataset, a high-quality pore-scale image patch dataset, and establish a rational evaluation benchmark. We introduce a Data-Model Co-Evolution (DMCE) framework to generate a progressively refined, high-quality dataset from high-resolution facial images. We then train existing SOTA models on our dataset and conduct extensive experiments. Our results show that the SOTA model achieves a FPR95 value of 1.91% on the matching task, outperforming PSIFT (22.41%) by a margin of 20.5%. However, its advantage is diminished in the 3D reconstruction task, where its overall performance is not significantly better than that of traditional descriptors. This indicates that deep learning descriptors still have limitations in addressing the challenges of facial weak-texture regions, and much work remains to be done in this field.",
    "authors": [
      "Dong Li",
      "HuaLiang Lin",
      "JiaYu Li"
    ],
    "published": "2025-11-29T08:14:04+00:00",
    "url": "https://arxiv.org/pdf/2512.00381v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00376v1",
    "title": "Layer Probing Improves Kinase Functional Prediction with Protein Language Models",
    "abstract": "Protein language models (PLMs) have transformed sequence-based protein analysis, yet most applications rely only on final-layer embeddings, which may overlook biologically meaningful information encoded in earlier layers. We systematically evaluate all 33 layers of ESM-2 for kinase functional prediction using both unsupervised clustering and supervised classification. We show that mid-to-late transformer layers (layers 20-33) outperform the final layer by 32 percent in unsupervised Adjusted Rand Index and improve homology-aware supervised accuracy to 75.7 percent. Domain-level extraction, calibrated probability estimates, and a reproducible benchmarking pipeline further strengthen reliability. Our results demonstrate that transformer depth contains functionally distinct biological signals and that principled layer selection significantly improves kinase function prediction.",
    "authors": [
      "Ajit Kumar",
      "IndraPrakash Jha"
    ],
    "published": "2025-11-29T08:06:11+00:00",
    "url": "https://arxiv.org/pdf/2512.00376v1",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00371v1",
    "title": "Evaluating LLMs in Open-Source Games",
    "abstract": "Large Language Models' (LLMs) programming capabilities enable their participation in open-source games: a game-theoretic setting in which players submit computer programs in lieu of actions. These programs offer numerous advantages, including interpretability, inter-agent transparency, and formal verifiability; additionally, they enable program equilibria, solutions that leverage the transparency of code and are inaccessible within normal-form settings. We evaluate the capabilities of leading open- and closed-weight LLMs to predict and classify program strategies and evaluate features of the approximate program equilibria reached by LLM agents in dyadic and evolutionary settings. We identify the emergence of payoff-maximizing, cooperative, and deceptive strategies, characterize the adaptation of mechanisms within these programs over repeated open-source games, and analyze their comparative evolutionary fitness. We find that open-source games serve as a viable environment to study and steer the emergence of cooperative strategy in multi-agent dilemmas.",
    "authors": [
      "Swadesh Sistla",
      "Max Kleiman-Weiner"
    ],
    "published": "2025-11-29T07:46:25+00:00",
    "url": "https://arxiv.org/pdf/2512.00371v1",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ]
  },
  {
    "arxiv_id": "2512.00369v1",
    "title": "POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models",
    "abstract": "The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale \u03c9 as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.",
    "authors": [
      "Wenshuo Chen",
      "Haosen Li",
      "Shaofeng Liang",
      "Lei Wang",
      "Haozhe Jia",
      "Kaishen Yuan",
      "Jieming Wu",
      "Bowen Tian",
      "Yutao Yue"
    ],
    "published": "2025-11-29T07:35:20+00:00",
    "url": "https://arxiv.org/pdf/2512.00369v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00368v1",
    "title": "THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering",
    "abstract": "Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.",
    "authors": [
      "Jian Zhu"
    ],
    "published": "2025-11-29T07:34:26+00:00",
    "url": "https://arxiv.org/pdf/2512.00368v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00367v1",
    "title": "Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation",
    "abstract": "Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.",
    "authors": [
      "Aparajitha Allamraju",
      "Maitreya Prafulla Chitale",
      "Hiranmai Sri Adibhatla",
      "Rahul Mishra",
      "Manish Shrivastava"
    ],
    "published": "2025-11-29T07:30:37+00:00",
    "url": "https://arxiv.org/pdf/2512.00367v1",
    "categories": [
      "cs.IR",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00366v1",
    "title": "S^2-KD: Semantic-Spectral Knowledge Distillation Spatiotemporal Forecasting",
    "abstract": "Spatiotemporal forecasting often relies on computationally intensive models to capture complex dynamics. Knowledge distillation (KD) has emerged as a key technique for creating lightweight student models, with recent advances like frequency-aware KD successfully preserving spectral properties (i.e., high-frequency details and low-frequency trends). However, these methods are fundamentally constrained by operating on pixel-level signals, leaving them blind to the rich semantic and causal context behind the visual patterns. To overcome this limitation, we introduce S^2-KD, a novel framework that unifies Semantic priors with Spectral representations for distillation. Our approach begins by training a privileged, multimodal teacher model. This teacher leverages textual narratives from a Large Multimodal Model (LMM) to reason about the underlying causes of events, while its architecture simultaneously decouples spectral components in its latent space. The core of our framework is a new distillation objective that transfers this unified semantic-spectral knowledge into a lightweight, vision-only student. Consequently, the student learns to make predictions that are not only spectrally accurate but also semantically coherent, without requiring any textual input or architectural overhead at inference. Extensive experiments on benchmarks like WeatherBench and TaxiBJ+ show that S^2-KD significantly boosts the performance of simple student models, enabling them to outperform state-of-the-art methods, particularly in long-horizon and complex non-stationary scenarios.",
    "authors": [
      "Wenshuo Wang",
      "Yaomin Shen",
      "Yingjie Tan",
      "Yihao Chen"
    ],
    "published": "2025-11-29T07:27:15+00:00",
    "url": "https://arxiv.org/pdf/2512.00366v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00365v1",
    "title": "Towards aligned body representations in vision models",
    "abstract": "Human physical reasoning relies on internal \"body\" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.",
    "authors": [
      "Andrey Gizdov",
      "Andrea Procopio",
      "Yichen Li",
      "Daniel Harari",
      "Tomer Ullman"
    ],
    "published": "2025-11-29T07:25:32+00:00",
    "url": "https://arxiv.org/pdf/2512.00365v1",
    "categories": [
      "cs.CV",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00363v1",
    "title": "MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters",
    "abstract": "Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method.",
    "authors": [
      "Jianhong Han",
      "Yupei Wang",
      "Yuan Zhang",
      "Liang Chen"
    ],
    "published": "2025-11-29T07:23:01+00:00",
    "url": "https://arxiv.org/pdf/2512.00363v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00360v1",
    "title": "CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA",
    "abstract": "We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.",
    "authors": [
      "Vsevolod Kovalev",
      "Parteek Kumar"
    ],
    "published": "2025-11-29T07:06:51+00:00",
    "url": "https://arxiv.org/pdf/2512.00360v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00355v1",
    "title": "SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction",
    "abstract": "With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.",
    "authors": [
      "Junqiao Fan",
      "Pengfei Liu",
      "Haocong Rao"
    ],
    "published": "2025-11-29T06:49:38+00:00",
    "url": "https://arxiv.org/pdf/2512.00355v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00350v1",
    "title": "MedCondDiff: Lightweight, Robust, Semantically Guided Diffusion for Medical Image Segmentation",
    "abstract": "We introduce MedCondDiff, a diffusion-based framework for multi-organ medical image segmentation that is efficient and anatomically grounded. The model conditions the denoising process on semantic priors extracted by a Pyramid Vision Transformer (PVT) backbone, yielding a semantically guided and lightweight diffusion architecture. This design improves robustness while reducing both inference time and VRAM usage compared to conventional diffusion models. Experiments on multi-organ, multi-modality datasets demonstrate that MedCondDiff delivers competitive performance across anatomical regions and imaging modalities, underscoring the potential of semantically guided diffusion models as an effective class of architectures for medical imaging tasks.",
    "authors": [
      "Ruirui Huang",
      "Jiacheng Li"
    ],
    "published": "2025-11-29T06:43:15+00:00",
    "url": "https://arxiv.org/pdf/2512.00350v1",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.03087v1",
    "title": "When Harmful Content Gets Camouflaged: Unveiling Perception Failure of LVLMs with CamHarmTI",
    "abstract": "Large vision-language models (LVLMs) are increasingly used for tasks where detecting multimodal harmful content is crucial, such as online content moderation. However, real-world harmful content is often camouflaged, relying on nuanced text-image interplay, such as memes or images with embedded malicious text, to evade detection. This raises a key question: \\textbf{can LVLMs perceive such camouflaged harmful content as sensitively as humans do?} In this paper, we introduce CamHarmTI, a benchmark for evaluating LVLM ability to perceive and interpret camouflaged harmful content within text-image compositions. CamHarmTI consists of over 4,500 samples across three types of image-text posts. Experiments on 100 human users and 12 mainstream LVLMs reveal a clear perceptual gap: humans easily recognize such content (e.g., over 95.75\\% accuracy), whereas current LVLMs often fail (e.g., ChatGPT-4o achieves only 2.10\\% accuracy). Moreover, fine-tuning experiments demonstrate that \\bench serves as an effective resource for improving model perception, increasing accuracy by 55.94\\% for Qwen2.5VL-7B. Attention analysis and layer-wise probing further reveal that fine-tuning enhances sensitivity primarily in the early layers of the vision encoder, promoting a more integrated scene understanding. These findings highlight the inherent perceptual limitations in LVLMs and offer insight into more human-aligned visual reasoning systems.",
    "authors": [
      "Yanhui Li",
      "Qi Zhou",
      "Zhihong Xu",
      "Huizhong Guo",
      "Wenhai Wang",
      "Dongxia Wang"
    ],
    "published": "2025-11-29T06:39:47+00:00",
    "url": "https://arxiv.org/pdf/2512.03087v1",
    "categories": [
      "cs.MM",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00349v1",
    "title": "Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models",
    "abstract": "Are frontier AI systems becoming more capable? Certainly. Yet such progress is not an unalloyed blessing but rather a Trojan horse: behind their performance leaps lie more insidious and destructive safety risks, namely deception. Unlike hallucination, which arises from insufficient capability and leads to mistakes, deception represents a deeper threat in which models deliberately mislead users through complex reasoning and insincere responses. As system capabilities advance, deceptive behaviours have spread from textual to multimodal settings, amplifying their potential harm. First and foremost, how can we monitor these covert multimodal deceptive behaviors? Nevertheless, current research remains almost entirely confined to text, leaving the deceptive risks of multimodal large language models unexplored. In this work, we systematically reveal and quantify multimodal deception risks, introducing MM-DeceptionBench, the first benchmark explicitly designed to evaluate multimodal deception. Covering six categories of deception, MM-DeceptionBench characterizes how models strategically manipulate and mislead through combined visual and textual modalities. On the other hand, multimodal deception evaluation is almost a blind spot in existing methods. Its stealth, compounded by visual-semantic ambiguity and the complexity of cross-modal reasoning, renders action monitoring and chain-of-thought monitoring largely ineffective. To tackle this challenge, we propose debate with images, a novel multi-agent debate monitor framework. By compelling models to ground their claims in visual evidence, this method substantially improves the detectability of deceptive strategies. Experiments show that it consistently increases agreement with human judgements across all tested models, boosting Cohen's kappa by 1.5x and accuracy by 1.25x on GPT-4o.",
    "authors": [
      "Sitong Fang",
      "Shiyi Hou",
      "Kaile Wang",
      "Boyuan Chen",
      "Donghai Hong",
      "Jiayi Zhou",
      "Josef Dai",
      "Yaodong Yang",
      "Jiaming Ji"
    ],
    "published": "2025-11-29T06:39:36+00:00",
    "url": "https://arxiv.org/pdf/2512.00349v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00345v1",
    "title": "mmPred: Radar-based Human Motion Prediction in the Dark",
    "abstract": "Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.",
    "authors": [
      "Junqiao Fan",
      "Haocong Rao",
      "Jiarui Zhang",
      "Jianfei Yang",
      "Lihua Xie"
    ],
    "published": "2025-11-29T06:26:55+00:00",
    "url": "https://arxiv.org/pdf/2512.00345v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00344v1",
    "title": "Echo-N1: Affective RL Frontier",
    "abstract": "The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.",
    "authors": [
      "Naifan Zhang",
      "Ruihan Sun",
      "Ruixi Su",
      "Shiqi Ma",
      "Shiya Zhang",
      "Xianna Weng",
      "Xiaofan Zhang",
      "Yuhan Zhan",
      "Yuyang Xu",
      "Zhaohan Chen",
      "Zhengyuan Pan",
      "Ziyi Song"
    ],
    "published": "2025-11-29T06:25:16+00:00",
    "url": "https://arxiv.org/pdf/2512.00344v1",
    "categories": [
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00343v1",
    "title": "Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models",
    "abstract": "Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: https://github.com/Robin-WZQ/AMDET",
    "authors": [
      "Zhongqi Wang",
      "Jie Zhang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "published": "2025-11-29T06:20:00+00:00",
    "url": "https://arxiv.org/pdf/2512.00343v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00336v1",
    "title": "MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection",
    "abstract": "The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at https://github.com/HuMengXue0104/MVAD.",
    "authors": [
      "Mengxue Hu",
      "Yunfeng Diao",
      "Changtao Miao",
      "Jianshu Li",
      "Zhe Li",
      "Joey Tianyi Zhou"
    ],
    "published": "2025-11-29T05:59:38+00:00",
    "url": "https://arxiv.org/pdf/2512.00336v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00333v1",
    "title": "IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages",
    "abstract": "While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.",
    "authors": [
      "Ayush Maheshwari",
      "Kaushal Sharma",
      "Vivek Patel",
      "Aditya Maheshwari"
    ],
    "published": "2025-11-29T05:49:50+00:00",
    "url": "https://arxiv.org/pdf/2512.00333v1",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00332v1",
    "title": "Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents",
    "abstract": "Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.",
    "authors": [
      "Daud Waqas",
      "Aaryamaan Golthi",
      "Erika Hayashida",
      "Huanzhi Mao"
    ],
    "published": "2025-11-29T05:44:37+00:00",
    "url": "https://arxiv.org/pdf/2512.00332v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.00331v1",
    "title": "CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System",
    "abstract": "Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.",
    "authors": [
      "Yefeng Wu",
      "Yuchen Song",
      "Yecheng Zhao",
      "Ling Wu",
      "Shan Wan"
    ],
    "published": "2025-11-29T05:41:57+00:00",
    "url": "https://arxiv.org/pdf/2512.00331v1",
    "categories": [
      "cs.AI",
      "cs.MA"
    ]
  },
  {
    "arxiv_id": "2512.00329v1",
    "title": "Evidence-Guided Schema Normalization for Temporal Tabular Reasoning",
    "abstract": "Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\\% improvement over the baseline (68.89 EM).",
    "authors": [
      "Ashish Thanga",
      "Vibhu Dixit",
      "Abhilash Shankarampeta",
      "Vivek Gupta"
    ],
    "published": "2025-11-29T05:40:08+00:00",
    "url": "https://arxiv.org/pdf/2512.00329v1",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ]
  },
  {
    "arxiv_id": "2512.00327v1",
    "title": "Odometry Without Correspondence from Inertially Constrained Ruled Surfaces",
    "abstract": "Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.",
    "authors": [
      "Chenqi Zhu",
      "Levi Burner",
      "Yiannis Aloimonos"
    ],
    "published": "2025-11-29T05:36:50+00:00",
    "url": "https://arxiv.org/pdf/2512.00327v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00325v1",
    "title": "Progressive Code Integration for Abstractive Bug Report Summarization",
    "abstract": "Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.",
    "authors": [
      "Shaira Sadia Karim",
      "Abrar Mahmud Rahim",
      "Lamia Alam",
      "Ishmam Tashdeed",
      "Lutfun Nahar Lota",
      "Md. Abu Raihan M. Kamal",
      "Md. Azam Hossain"
    ],
    "published": "2025-11-29T05:35:36+00:00",
    "url": "https://arxiv.org/pdf/2512.00325v1",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "arxiv_id": "2512.00324v1",
    "title": "MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation",
    "abstract": "Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.",
    "authors": [
      "Jinda Du",
      "Jieji Ren",
      "Qiaojun Yu",
      "Ningbin Zhang",
      "Yu Deng",
      "Xingyu Wei",
      "Yufei Liu",
      "Guoying Gu",
      "Xiangyang Zhu"
    ],
    "published": "2025-11-29T05:34:39+00:00",
    "url": "https://arxiv.org/pdf/2512.00324v1",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.HC"
    ]
  },
  {
    "arxiv_id": "2512.00323v1",
    "title": "Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets",
    "abstract": "Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\\%), biomedical\\_cpgQA (96.45\\%), QuAC (11.13\\%), and Question Answer Dataset (41.6\\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\\% on the IELTS dataset.",
    "authors": [
      "Muhammad Muneeb",
      "David B. Ascher",
      "Ahsan Baidar Bakht"
    ],
    "published": "2025-11-29T05:31:45+00:00",
    "url": "https://arxiv.org/pdf/2512.00323v1",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "arxiv_id": "2512.02062v1",
    "title": "Superpixel Attack: Enhancing Black-box Adversarial Attack with Image-driven Division Areas",
    "abstract": "Deep learning models are used in safety-critical tasks such as automated driving and face recognition. However, small perturbations in the model input can significantly change the predictions. Adversarial attacks are used to identify small perturbations that can lead to misclassifications. More powerful black-box adversarial attacks are required to develop more effective defenses. A promising approach to black-box adversarial attacks is to repeat the process of extracting a specific image area and changing the perturbations added to it. Existing attacks adopt simple rectangles as the areas where perturbations are changed in a single iteration. We propose applying superpixels instead, which achieve a good balance between color variance and compactness. We also propose a new search method, versatile search, and a novel attack method, Superpixel Attack, which applies superpixels and performs versatile search. Superpixel Attack improves attack success rates by an average of 2.10% compared with existing attacks. Most models used in this study are robust against adversarial attacks, and this improvement is significant for black-box adversarial attacks. The code is avilable at https://github.com/oe1307/SuperpixelAttack.git.",
    "authors": [
      "Issa Oe",
      "Keiichiro Yamamura",
      "Hiroki Ishikura",
      "Ryo Hamahira",
      "Katsuki Fujisawa"
    ],
    "published": "2025-11-29T05:28:52+00:00",
    "url": "https://arxiv.org/pdf/2512.02062v1",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.03086v1",
    "title": "Beyond Code Pairs: Dialogue-Based Data Generation for LLM Code Translation",
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in code translation, yet their performance deteriorates in low-resource programming domains such as Fortran and emerging frameworks like CUDA, where high-quality parallel data are scarce. We present an automated dataset generation pipeline featuring a dual-LLM Questioner-Solver design that incorporates external knowledge from compilers and runtime feedback. Beyond traditional source-target code pair datasets, our approach additionally generates (1) verified translations with unit tests for assessing functional consistency, and (2) multi-turn dialogues that capture the reasoning process behind translation refinement. Applied to Fortran -> C++ and C++ -> CUDA, the pipeline yields 3.64k and 3.93k dialogues, respectively. Fine-tuning on this data yields dramatic improvements in functional correctness, boosting unit test success rates by over 56% on the challenging C++-to-CUDA task. We show this data enables a 7B open-weight model to significantly outperform larger proprietary systems on key metrics like compilation success.",
    "authors": [
      "Le Chen",
      "Nuo Xu",
      "Winson Chen",
      "Bin Lei",
      "Pei-Hung Lin",
      "Dunzhi Zhou",
      "Rajeev Thakur",
      "Caiwen Ding",
      "Ali Jannesari",
      "Chunhua Liao"
    ],
    "published": "2025-11-29T05:26:53+00:00",
    "url": "https://arxiv.org/pdf/2512.03086v1",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.SE"
    ]
  },
  {
    "arxiv_id": "2512.00319v1",
    "title": "RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language generation and reasoning. However, their integration into automated software ecosystems is often hindered by the \"Structure Gap\" - the inherent tension between the probabilistic nature of token generation and the deterministic requirements of structured data formats (e.g., JSON, XML). Traditional Supervised Fine-Tuning (SFT) often fails to enforce strict syntactic constraints, leading to \"hallucinated\" keys or malformed structures, while constrained decoding methods impose significant inference latency. In this paper, we propose a lightweight, efficient Reinforcement Learning (RL) framework to bridge this gap. We introduce a novel Multi-dimensional Reward Function that decomposes the structured output task into a hierarchy of constraints: structural integrity, format correctness, content accuracy, and validity. Leveraging Gradient Regularized Policy Optimization (GRPO), we enable the model to internalize these constraints without the need for a separate critic network, reducing peak VRAM usage by 40% compared to PPO. We validate our approach on multiple tasks, including complex recipe generation and structured math reasoning (GSM8K-JSON). Experimental results demonstrate that our method achieves 89.7% structural accuracy and 92.1% JSON validity, significantly outperforming both zero-shot baselines (e.g., GPT-3.5) and SFT on larger models like LLaMA-3-8B. Furthermore, we provide a detailed analysis of training dynamics, revealing a distinct self-paced curriculum where the model sequentially acquires syntactic proficiency before semantic accuracy. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.",
    "authors": [
      "Ruike Hu",
      "Shulei Wu"
    ],
    "published": "2025-11-29T04:47:14+00:00",
    "url": "https://arxiv.org/pdf/2512.00319v1",
    "categories": [
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "arxiv_id": "2512.00311v1",
    "title": "Tracing Mathematical Proficiency Through Problem-Solving Processes",
    "abstract": "Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.",
    "authors": [
      "Jungyang Park",
      "Suho Kang",
      "Jaewoo Park",
      "Jaehong Kim",
      "Jaewoo Shin",
      "Seonjoon Park",
      "Youngjae Yu"
    ],
    "published": "2025-11-29T04:12:06+00:00",
    "url": "https://arxiv.org/pdf/2512.00311v1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ]
  },
  {
    "arxiv_id": "2512.00310v1",
    "title": "ART-ASyn: Anatomy-aware Realistic Texture-based Anomaly Synthesis Framework for Chest X-Rays",
    "abstract": "Unsupervised anomaly detection aims to identify anomalies without pixel-level annotations. Synthetic anomaly-based methods exhibit a unique capacity to introduce controllable irregularities with known masks, enabling explicit supervision during training. However, existing methods often produce synthetic anomalies that are visually distinct from real pathological patterns and ignore anatomical structure. This paper presents a novel Anatomy-aware Realistic Texture-based Anomaly Synthesis framework (ART-ASyn) for chest X-rays that generates realistic and anatomically consistent lung opacity related anomalies using texture-based augmentation guided by our proposed Progressive Binary Thresholding Segmentation method (PBTSeg) for lung segmentation. The generated paired samples of synthetic anomalies and their corresponding precise pixel-level anomaly mask for each normal sample enable explicit segmentation supervision. In contrast to prior work limited to one-class classification, ART-ASyn is further evaluated for zero-shot anomaly segmentation, demonstrating generalizability on an unseen dataset without target-domain annotations. Code availability is available at https://github.com/angelacao-hub/ART-ASyn.",
    "authors": [
      "Qinyi Cao",
      "Jianan Fan",
      "Weidong Cai"
    ],
    "published": "2025-11-29T04:06:58+00:00",
    "url": "https://arxiv.org/pdf/2512.00310v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00308v1",
    "title": "Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation",
    "abstract": "Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.",
    "authors": [
      "Xiao Cui",
      "Yulei Qin",
      "Wengang Zhou",
      "Hongsheng Li",
      "Houqiang Li"
    ],
    "published": "2025-11-29T04:04:05+00:00",
    "url": "https://arxiv.org/pdf/2512.00308v1",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "arxiv_id": "2512.00307v1",
    "title": "Adversarial Signed Graph Learning with Differential Privacy",
    "abstract": "Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.",
    "authors": [
      "Haobin Ke",
      "Sen Zhang",
      "Qingqing Ye",
      "Xun Ran",
      "Haibo Hu"
    ],
    "published": "2025-11-29T04:02:48+00:00",
    "url": "https://arxiv.org/pdf/2512.00307v1",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  }
]